{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gdown\n!pip install flask-ngrok\n!pip install pyngrok==4.1.1\n!ngrok authtoken\n!pip install tensorflow-gan\n!pip install Ninja\n!pip install ml_collections","metadata":{"execution":{"iopub.status.busy":"2023-12-17T20:01:43.630631Z","iopub.execute_input":"2023-12-17T20:01:43.631379Z","iopub.status.idle":"2023-12-17T20:03:14.217957Z","shell.execute_reply.started":"2023-12-17T20:01:43.631347Z","shell.execute_reply":"2023-12-17T20:03:14.216957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# info\n\n# test for V126 (80%)\n# test_batch_size 1","metadata":{"execution":{"iopub.status.busy":"2023-12-17T20:03:14.219965Z","iopub.execute_input":"2023-12-17T20:03:14.220263Z","iopub.status.idle":"2023-12-17T20:03:14.224771Z","shell.execute_reply.started":"2023-12-17T20:03:14.220238Z","shell.execute_reply":"2023-12-17T20:03:14.223737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/yang-song/score_sde_pytorch.git","metadata":{"execution":{"iopub.status.busy":"2023-12-17T20:03:14.225768Z","iopub.execute_input":"2023-12-17T20:03:14.226001Z","iopub.status.idle":"2023-12-17T20:03:16.021037Z","shell.execute_reply.started":"2023-12-17T20:03:14.225980Z","shell.execute_reply":"2023-12-17T20:03:16.019871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport tensorflow_gan\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.autograd import Function\nfrom torch.utils.cpp_extension import load\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nimport logging\nimport tensorflow as tf\nimport matplotlib.pyplot as plt","metadata":{"id":"Vl3UaqDDLzL0","outputId":"823e5b85-63e5-40f2-8252-7d9d5514f07e","execution":{"iopub.status.busy":"2023-12-17T20:03:16.024216Z","iopub.execute_input":"2023-12-17T20:03:16.024611Z","iopub.status.idle":"2023-12-17T20:03:32.344042Z","shell.execute_reply.started":"2023-12-17T20:03:16.024575Z","shell.execute_reply":"2023-12-17T20:03:32.343191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"directory=\"/kaggle/working/score_sde_pytorch/op/Untitled Folder\"\ndirectory1=\"/kaggle/working/score_sde_pytorch/op/Untitled Folder 1\"\nif not os.path.exists(directory):\n    os.makedirs(directory)\nif not os.path.exists(directory1):\n    os.makedirs(directory1)\n\n\n\nshutil.move(\"/kaggle/working/score_sde_pytorch/op/upfirdn2d.cpp\", \"/kaggle/working/score_sde_pytorch/op/Untitled Folder 1\")\nshutil.move(\"/kaggle/working/score_sde_pytorch/op/upfirdn2d_kernel.cu\", \"/kaggle/working/score_sde_pytorch/op/Untitled Folder 1\")\n\nshutil.move(\"/kaggle/working/score_sde_pytorch/op/fused_bias_act.cpp\", \"/kaggle/working/score_sde_pytorch/op/Untitled Folder\")\nshutil.move(\"/kaggle/working/score_sde_pytorch/op/fused_bias_act_kernel.cu\", \"/kaggle/working/score_sde_pytorch/op/Untitled Folder\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T20:03:32.345223Z","iopub.execute_input":"2023-12-17T20:03:32.345777Z","iopub.status.idle":"2023-12-17T20:03:32.356116Z","shell.execute_reply.started":"2023-12-17T20:03:32.345748Z","shell.execute_reply":"2023-12-17T20:03:32.355070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @title\nfrom torch.utils.cpp_extension import load\nimport os\n\nsource_directory = \"/kaggle/working/score_sde_pytorch/op/Untitled Folder 1/\"\nsources = [\"upfirdn2d.cpp\", \"upfirdn2d_kernel.cu\"]\n\nupfirdn2d_op = load(\n    name='upfirdn2d',\n    sources=[os.path.join(source_directory, src) for src in sources]\n)\n\nclass UpFirDn2dBackward(Function):\n    staticmethod\n    def forward(\n        ctx, grad_output, kernel, grad_kernel, up, down, pad, g_pad, in_size, out_size\n    ):\n\n        up_x, up_y = up\n        down_x, down_y = down\n        g_pad_x0, g_pad_x1, g_pad_y0, g_pad_y1 = g_pad\n\n        grad_output = grad_output.reshape(-1, out_size[0], out_size[1], 1)\n\n        grad_input = upfirdn2d_op.upfirdn2d(\n            grad_output,\n            grad_kernel,\n            down_x,\n            down_y,\n            up_x,\n            up_y,\n            g_pad_x0,\n            g_pad_x1,\n            g_pad_y0,\n            g_pad_y1,\n        )\n        grad_input = grad_input.view(in_size[0], in_size[1], in_size[2], in_size[3])\n\n        ctx.save_for_backward(kernel)\n\n        pad_x0, pad_x1, pad_y0, pad_y1 = pad\n\n        ctx.up_x = up_x\n        ctx.up_y = up_y\n        ctx.down_x = down_x\n        ctx.down_y = down_y\n        ctx.pad_x0 = pad_x0\n        ctx.pad_x1 = pad_x1\n        ctx.pad_y0 = pad_y0\n        ctx.pad_y1 = pad_y1\n        ctx.in_size = in_size\n        ctx.out_size = out_size\n\n        return grad_input\n\n    staticmethod\n    def backward(ctx, gradgrad_input):\n        kernel, = ctx.saved_tensors\n\n        gradgrad_input = gradgrad_input.reshape(-1, ctx.in_size[2], ctx.in_size[3], 1)\n\n        gradgrad_out = upfirdn2d_op.upfirdn2d(\n            gradgrad_input,\n            kernel,\n            ctx.up_x,\n            ctx.up_y,\n            ctx.down_x,\n            ctx.down_y,\n            ctx.pad_x0,\n            ctx.pad_x1,\n            ctx.pad_y0,\n            ctx.pad_y1,\n        )\n        # gradgrad_out = gradgrad_out.view(ctx.in_size[0], ctx.out_size[0], ctx.out_size[1], ctx.in_size[3])\n        gradgrad_out = gradgrad_out.view(\n            ctx.in_size[0], ctx.in_size[1], ctx.out_size[0], ctx.out_size[1]\n        )\n\n        return gradgrad_out, None, None, None, None, None, None, None, None\n\n\nclass UpFirDn2d(Function):\n    staticmethod\n    def forward(ctx, input, kernel, up, down, pad):\n        up_x, up_y = up\n        down_x, down_y = down\n        pad_x0, pad_x1, pad_y0, pad_y1 = pad\n\n        kernel_h, kernel_w = kernel.shape\n        batch, channel, in_h, in_w = input.shape\n        ctx.in_size = input.shape\n\n        input = input.reshape(-1, in_h, in_w, 1)\n\n        ctx.save_for_backward(kernel, torch.flip(kernel, [0, 1]))\n\n        out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h) // down_y + 1\n        out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w) // down_x + 1\n        ctx.out_size = (out_h, out_w)\n\n        ctx.up = (up_x, up_y)\n        ctx.down = (down_x, down_y)\n        ctx.pad = (pad_x0, pad_x1, pad_y0, pad_y1)\n\n        g_pad_x0 = kernel_w - pad_x0 - 1\n        g_pad_y0 = kernel_h - pad_y0 - 1\n        g_pad_x1 = in_w * up_x - out_w * down_x + pad_x0 - up_x + 1\n        g_pad_y1 = in_h * up_y - out_h * down_y + pad_y0 - up_y + 1\n\n        ctx.g_pad = (g_pad_x0, g_pad_x1, g_pad_y0, g_pad_y1)\n\n        out = upfirdn2d_op.upfirdn2d(\n            input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1\n        )\n        # out = out.view(major, out_h, out_w, minor)\n        out = out.view(-1, channel, out_h, out_w)\n\n        return out\n\n    staticmethod\n    def backward(ctx, grad_output):\n        kernel, grad_kernel = ctx.saved_tensors\n\n        grad_input = UpFirDn2dBackward.apply(\n            grad_output,\n            kernel,\n            grad_kernel,\n            ctx.up,\n            ctx.down,\n            ctx.pad,\n            ctx.g_pad,\n            ctx.in_size,\n            ctx.out_size,\n        )\n\n        return grad_input, None, None, None, None\n\n\ndef upfirdn2d(input, kernel, up=1, down=1, pad=(0, 0)):\n    if input.device.type == \"cpu\":\n        out = upfirdn2d_native(\n            input, kernel, up, up, down, down, pad[0], pad[1], pad[0], pad[1]\n        )\n\n    else:\n        out = UpFirDn2d.apply(\n            input, kernel, (up, up), (down, down), (pad[0], pad[1], pad[0], pad[1])\n        )\n\n    return out\n\n\ndef upfirdn2d_native(\n    input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1\n):\n    _, channel, in_h, in_w = input.shape\n    input = input.reshape(-1, in_h, in_w, 1)\n\n    _, in_h, in_w, minor = input.shape\n    kernel_h, kernel_w = kernel.shape\n\n    out = input.view(-1, in_h, 1, in_w, 1, minor)\n    out = F.pad(out, [0, 0, 0, up_x - 1, 0, 0, 0, up_y - 1])\n    out = out.view(-1, in_h * up_y, in_w * up_x, minor)\n\n    out = F.pad(\n        out, [0, 0, max(pad_x0, 0), max(pad_x1, 0), max(pad_y0, 0), max(pad_y1, 0)]\n    )\n    out = out[\n        :,\n        max(-pad_y0, 0) : out.shape[1] - max(-pad_y1, 0),\n        max(-pad_x0, 0) : out.shape[2] - max(-pad_x1, 0),\n        :,\n    ]\n\n    out = out.permute(0, 3, 1, 2)\n    out = out.reshape(\n        [-1, 1, in_h * up_y + pad_y0 + pad_y1, in_w * up_x + pad_x0 + pad_x1]\n    )\n    w = torch.flip(kernel, [0, 1]).view(1, 1, kernel_h, kernel_w)\n    out = F.conv2d(out, w)\n    out = out.reshape(\n        -1,\n        minor,\n        in_h * up_y + pad_y0 + pad_y1 - kernel_h + 1,\n        in_w * up_x + pad_x0 + pad_x1 - kernel_w + 1,\n    )\n    out = out.permute(0, 2, 3, 1)\n    out = out[:, ::down_y, ::down_x, :]\n\n    out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h) // down_y + 1\n    out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w) // down_x + 1\n\n    return out.view(-1, channel, out_h, out_w)","metadata":{"id":"xgMg0-CGFLA7","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:03:32.357494Z","iopub.execute_input":"2023-12-17T20:03:32.357753Z","iopub.status.idle":"2023-12-17T20:04:03.846043Z","shell.execute_reply.started":"2023-12-17T20:03:32.357731Z","shell.execute_reply":"2023-12-17T20:04:03.845183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @title\nimport os\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.autograd import Function\nfrom torch.utils.cpp_extension import load\n\n# !pip install Ninja\nmodule_path = os.path.dirname( \"/kaggle/working/score_sde_pytorch/op/Untitled Folder\")\nfused = load(\n    \"fused\",\n    sources=[\n        os.path.join( \"/kaggle/working/score_sde_pytorch/op/Untitled Folder\", \"fused_bias_act.cpp\"),\n        os.path.join( \"/kaggle/working/score_sde_pytorch/op/Untitled Folder\", \"fused_bias_act_kernel.cu\"),\n    ],\n)\n\n\nclass FusedLeakyReLUFunctionBackward(Function):\n    staticmethod\n    def forward(ctx, grad_output, out, negative_slope, scale):\n        ctx.save_for_backward(out)\n        ctx.negative_slope = negative_slope\n        ctx.scale = scale\n\n        empty = grad_output.new_empty(0)\n\n        grad_input = fused.fused_bias_act(\n            grad_output, empty, out, 3, 1, negative_slope, scale\n        )\n\n        dim = [0]\n\n        if grad_input.ndim > 2:\n            dim += list(range(2, grad_input.ndim))\n\n        grad_bias = grad_input.sum(dim).detach()\n\n        return grad_input, grad_bias\n\n    staticmethod\n    def backward(ctx, gradgrad_input, gradgrad_bias):\n        out, = ctx.saved_tensors\n        gradgrad_out = fused.fused_bias_act(\n            gradgrad_input, gradgrad_bias, out, 3, 1, ctx.negative_slope, ctx.scale\n        )\n\n        return gradgrad_out, None, None, None\n\n\nclass FusedLeakyReLUFunction(Function):\n    staticmethod\n    def forward(ctx, input, bias, negative_slope, scale):\n        empty = input.new_empty(0)\n        out = fused.fused_bias_act(input, bias, empty, 3, 0, negative_slope, scale)\n        ctx.save_for_backward(out)\n        ctx.negative_slope = negative_slope\n        ctx.scale = scale\n\n        return out\n\n    staticmethod\n    def backward(ctx, grad_output):\n        out, = ctx.saved_tensors\n\n        grad_input, grad_bias = FusedLeakyReLUFunctionBackward.apply(\n            grad_output, out, ctx.negative_slope, ctx.scale\n        )\n\n        return grad_input, grad_bias, None, None\n\n\nclass FusedLeakyReLU(nn.Module):\n    def __init__(self, channel, negative_slope=0.2, scale=2 ** 0.5):\n        super().__init__()\n\n        self.bias = nn.Parameter(torch.zeros(channel))\n        self.negative_slope = negative_slope\n        self.scale = scale\n\n    def forward(self, input):\n        return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)\n\n\ndef fused_leaky_relu(input, bias, negative_slope=0.2, scale=2 ** 0.5):\n    if input.device.type == \"cpu\":\n        rest_dim = [1] * (input.ndim - bias.ndim - 1)\n        return (\n            F.leaky_relu(\n                input + bias.view(1, bias.shape[0], *rest_dim), negative_slope=0.2\n            )\n            * scale\n        )\n\n    else:\n        return FusedLeakyReLUFunction.apply(input, bias, negative_slope, scale)","metadata":{"id":"AKZWq1ptFOgj","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:03.847257Z","iopub.execute_input":"2023-12-17T20:04:03.847575Z","iopub.status.idle":"2023-12-17T20:04:31.957959Z","shell.execute_reply.started":"2023-12-17T20:04:03.847549Z","shell.execute_reply":"2023-12-17T20:04:31.956992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model Utils**","metadata":{"id":"3zEpT62UFVQn"}},{"cell_type":"code","source":"# @title\n# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"All functions and modules related to model definition.\n\"\"\"\n\nimport torch\n#import sde_lib\nimport numpy as np\n\n\n_MODELS = {}\n\n\ndef register_model(cls=None, *, name=None):\n  \"\"\"A decorator for registering model classes.\"\"\"\n\n  def _register(cls):\n    if name is None:\n        local_name = cls.__name__\n    else:\n        local_name = name\n    if local_name in _MODELS:\n        raise ValueError(f'Already registered model with name: {local_name}')\n    _MODELS[local_name] = cls\n\n    return cls\n\n  if cls is None:\n    return _register\n  else:\n    return _register(cls)\n\n\ndef get_model(name):\n    print(_MODELS)\n    return _MODELS[name]\n\n\ndef get_sigmas(config):\n  \"\"\"Get sigmas --- the set of noise levels for SMLD from config files.\n  Args:\n    config: A ConfigDict object parsed from the config file\n  Returns:\n    sigmas: a jax numpy arrary of noise levels\n  \"\"\"\n  sigmas = np.exp(\n    np.linspace(np.log(config.model.sigma_max), np.log(config.model.sigma_min), config.model.num_scales))\n\n  return sigmas\n\n\ndef get_ddpm_params(config):\n  \"\"\"Get betas and alphas --- parameters used in the original DDPM paper.\"\"\"\n  num_diffusion_timesteps = 1000\n  # parameters need to be adapted if number of time steps differs from 1000\n  beta_start = config.model.beta_min / config.model.num_scales\n  beta_end = config.model.beta_max / config.model.num_scales\n  betas = np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n\n  alphas = 1. - betas\n  alphas_cumprod = np.cumprod(alphas, axis=0)\n  sqrt_alphas_cumprod = np.sqrt(alphas_cumprod)\n  sqrt_1m_alphas_cumprod = np.sqrt(1. - alphas_cumprod)\n\n  return {\n    'betas': betas,\n    'alphas': alphas,\n    'alphas_cumprod': alphas_cumprod,\n    'sqrt_alphas_cumprod': sqrt_alphas_cumprod,\n    'sqrt_1m_alphas_cumprod': sqrt_1m_alphas_cumprod,\n    'beta_min': beta_start * (num_diffusion_timesteps - 1),\n    'beta_max': beta_end * (num_diffusion_timesteps - 1),\n    'num_diffusion_timesteps': num_diffusion_timesteps\n  }\n\n\ndef create_model(config):\n  \"\"\"Create the score model.\"\"\"\n  model_name = config.model.name\n  score_model = get_model(model_name)(config)\n  score_model = score_model.to(config.device)\n  score_model = torch.nn.DataParallel(score_model)\n  return score_model\n\n\ndef get_model_fn(model, train=False):\n  \"\"\"Create a function to give the output of the score-based model.\n  Args:\n    model: The score model.\n    train: `True` for training and `False` for evaluation.\n  Returns:\n    A model function.\n  \"\"\"\n\n  def model_fn(x, labels):\n    \"\"\"Compute the output of the score-based model.\n    Args:\n      x: A mini-batch of input data.\n      labels: A mini-batch of conditioning variables for time steps. Should be interpreted differently\n        for different models.\n    Returns:\n      A tuple of (model output, new mutable states)\n    \"\"\"\n    if not train:\n      model.eval()\n      return model(x, labels)\n    else:\n      model.train()\n      torch.cuda.empty_cache()\n      return model(x, labels)\n\n  return model_fn\n\n\ndef get_score_fn(sde, model, train=False, continuous=False):\n  \"\"\"Wraps `score_fn` so that the model output corresponds to a real time-dependent score function.\n  Args:\n    sde: An `sde_lib.SDE` object that represents the forward SDE.\n    model: A score model.\n    train: `True` for training and `False` for evaluation.\n    continuous: If `True`, the score-based model is expected to directly take continuous time steps.\n  Returns:\n    A score function.\n  \"\"\"\n  model_fn = get_model_fn(model, train=train)\n  # print(\"first\")\n\n  if isinstance(sde,  VPSDE) or isinstance(sde,  subVPSDE):\n    # print(\"second\")\n    def score_fn(x, t):\n      # Scale neural network output by standard deviation and flip sign\n      if continuous or isinstance(sde,  subVPSDE):\n        # print(\"third\")\n        # For VP-trained models, t=0 corresponds to the lowest noise level\n        # The maximum value of time embedding is assumed to 999 for\n        # continuously-trained models.\n        labels = t * 999\n        score = model_fn(x, labels)\n        std = sde.marginal_prob(torch.zeros_like(x), t)[1]\n      else:\n        # print(\"forth\")\n        # For VP-trained models, t=0 corresponds to the lowest noise level\n        labels = t * (sde.N - 1)\n        score = model_fn(x, labels)\n        std = sde.sqrt_1m_alphas_cumprod.to(labels.device)[labels.long()]\n\n      # print(\"fifth\")\n      score = -score / std[:, None, None, None]\n      return score\n\n  elif isinstance(sde,  VESDE):\n    # print(\"seventh\")\n    def score_fn(x, t):\n      if continuous:\n        #print(\"its  continues!\")\n        labels = sde.marginal_prob(torch.zeros_like(x), t)[1]\n        # print(\"eighth\")\n\n      else:\n\n        # For VE-trained models, t=0 corresponds to the highest noise level\n        labels = sde.T - t\n        labels *= sde.N - 1\n        labels = torch.round(labels).long()\n        # print(\"ninth\")\n\n      #labels=torch.ones(64, device=device) *0.01\n      torch.cuda.empty_cache()\n      score = model_fn(x, labels)\n      # print(\"tenth\")\n      return score\n\n  else:\n    raise NotImplementedError(f\"SDE class {sde.__class__.__name__} not yet supported.\")\n\n#   print(\"eleventh\")\n  return score_fn\n\n\ndef to_flattened_numpy(x):\n  \"\"\"Flatten a torch tensor `x` and convert it to numpy.\"\"\"\n  return x.detach().cpu().numpy().reshape((-1,))\n\n\ndef from_flattened_numpy(x, shape):\n  \"\"\"Form a torch tensor with the given `shape` from a flattened numpy array `x`.\"\"\"\n  return torch.from_numpy(x.reshape(shape))","metadata":{"id":"AVJVvgucFYfB","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:31.959274Z","iopub.execute_input":"2023-12-17T20:04:31.959603Z","iopub.status.idle":"2023-12-17T20:04:31.982787Z","shell.execute_reply.started":"2023-12-17T20:04:31.959576Z","shell.execute_reply":"2023-12-17T20:04:31.981840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Normalize**","metadata":{"id":"5TYbaJ00FdBw"}},{"cell_type":"code","source":"# @title\n\"\"\"Normalization layers.\"\"\"\nimport torch.nn as nn\nimport torch\nimport functools\n\n\ndef get_normalization_Normalize(config, conditional=False):\n  \"\"\"Obtain normalization modules from the config file.\"\"\"\n  norm = config.model.normalization\n  if conditional:\n    if norm == 'InstanceNorm++':\n      return functools.partial(ConditionalInstanceNorm2dPlus, num_classes=config.model.num_classes)\n    else:\n      raise NotImplementedError(f'{norm} not implemented yet.')\n  else:\n    if norm == 'InstanceNorm':\n      return nn.InstanceNorm2d\n    elif norm == 'InstanceNorm++':\n      return InstanceNorm2dPlus\n    elif norm == 'VarianceNorm':\n      return VarianceNorm2d\n    elif norm == 'GroupNorm':\n      return nn.GroupNorm\n    else:\n      raise ValueError('Unknown normalization: %s' % norm)\n\n\nclass ConditionalBatchNorm2d(nn.Module):\n  def __init__(self, num_features, num_classes, bias=True):\n    super().__init__()\n    self.num_features = num_features\n    self.bias = bias\n    self.bn = nn.BatchNorm2d(num_features, affine=False)\n    if self.bias:\n      self.embed = nn.Embedding(num_classes, num_features * 2)\n      self.embed.weight.data[:, :num_features].uniform_()  # Initialise scale at N(1, 0.02)\n      self.embed.weight.data[:, num_features:].zero_()  # Initialise bias at 0\n    else:\n      self.embed = nn.Embedding(num_classes, num_features)\n      self.embed.weight.data.uniform_()\n\n  def forward(self, x, y):\n    out = self.bn(x)\n    if self.bias:\n      gamma, beta = self.embed(y).chunk(2, dim=1)\n      out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\n    else:\n      gamma = self.embed(y)\n      out = gamma.view(-1, self.num_features, 1, 1) * out\n    return out\n\n\nclass ConditionalInstanceNorm2d(nn.Module):\n  def __init__(self, num_features, num_classes, bias=True):\n    super().__init__()\n    self.num_features = num_features\n    self.bias = bias\n    self.instance_norm = nn.InstanceNorm2d(num_features, affine=False, track_running_stats=False)\n    if bias:\n      self.embed = nn.Embedding(num_classes, num_features * 2)\n      self.embed.weight.data[:, :num_features].uniform_()  # Initialise scale at N(1, 0.02)\n      self.embed.weight.data[:, num_features:].zero_()  # Initialise bias at 0\n    else:\n      self.embed = nn.Embedding(num_classes, num_features)\n      self.embed.weight.data.uniform_()\n\n  def forward(self, x, y):\n    h = self.instance_norm(x)\n    if self.bias:\n      gamma, beta = self.embed(y).chunk(2, dim=-1)\n      out = gamma.view(-1, self.num_features, 1, 1) * h + beta.view(-1, self.num_features, 1, 1)\n    else:\n      gamma = self.embed(y)\n      out = gamma.view(-1, self.num_features, 1, 1) * h\n    return out\n\n\nclass ConditionalVarianceNorm2d(nn.Module):\n  def __init__(self, num_features, num_classes, bias=False):\n    super().__init__()\n    self.num_features = num_features\n    self.bias = bias\n    self.embed = nn.Embedding(num_classes, num_features)\n    self.embed.weight.data.normal_(1, 0.02)\n\n  def forward(self, x, y):\n    vars = torch.var(x, dim=(2, 3), keepdim=True)\n    h = x / torch.sqrt(vars + 1e-5)\n\n    gamma = self.embed(y)\n    out = gamma.view(-1, self.num_features, 1, 1) * h\n    return out\n\n\nclass VarianceNorm2d(nn.Module):\n  def __init__(self, num_features, bias=False):\n    super().__init__()\n    self.num_features = num_features\n    self.bias = bias\n    self.alpha = nn.Parameter(torch.zeros(num_features))\n    self.alpha.data.normal_(1, 0.02)\n\n  def forward(self, x):\n    vars = torch.var(x, dim=(2, 3), keepdim=True)\n    h = x / torch.sqrt(vars + 1e-5)\n\n    out = self.alpha.view(-1, self.num_features, 1, 1) * h\n    return out\n\n\nclass ConditionalNoneNorm2d(nn.Module):\n  def __init__(self, num_features, num_classes, bias=True):\n    super().__init__()\n    self.num_features = num_features\n    self.bias = bias\n    if bias:\n      self.embed = nn.Embedding(num_classes, num_features * 2)\n      self.embed.weight.data[:, :num_features].uniform_()  # Initialise scale at N(1, 0.02)\n      self.embed.weight.data[:, num_features:].zero_()  # Initialise bias at 0\n    else:\n      self.embed = nn.Embedding(num_classes, num_features)\n      self.embed.weight.data.uniform_()\n\n  def forward(self, x, y):\n    if self.bias:\n      gamma, beta = self.embed(y).chunk(2, dim=-1)\n      out = gamma.view(-1, self.num_features, 1, 1) * x + beta.view(-1, self.num_features, 1, 1)\n    else:\n      gamma = self.embed(y)\n      out = gamma.view(-1, self.num_features, 1, 1) * x\n    return out\n\n\nclass NoneNorm2d(nn.Module):\n  def __init__(self, num_features, bias=True):\n    super().__init__()\n\n  def forward(self, x):\n    return x\n\n\nclass InstanceNorm2dPlus(nn.Module):\n  def __init__(self, num_features, bias=True):\n    super().__init__()\n    self.num_features = num_features\n    self.bias = bias\n    self.instance_norm = nn.InstanceNorm2d(num_features, affine=False, track_running_stats=False)\n    self.alpha = nn.Parameter(torch.zeros(num_features))\n    self.gamma = nn.Parameter(torch.zeros(num_features))\n    self.alpha.data.normal_(1, 0.02)\n    self.gamma.data.normal_(1, 0.02)\n    if bias:\n      self.beta = nn.Parameter(torch.zeros(num_features))\n\n  def forward(self, x):\n    means = torch.mean(x, dim=(2, 3))\n    m = torch.mean(means, dim=-1, keepdim=True)\n    v = torch.var(means, dim=-1, keepdim=True)\n    means = (means - m) / (torch.sqrt(v + 1e-5))\n    h = self.instance_norm(x)\n\n    if self.bias:\n      h = h + means[..., None, None] * self.alpha[..., None, None]\n      out = self.gamma.view(-1, self.num_features, 1, 1) * h + self.beta.view(-1, self.num_features, 1, 1)\n    else:\n      h = h + means[..., None, None] * self.alpha[..., None, None]\n      out = self.gamma.view(-1, self.num_features, 1, 1) * h\n    return out\n\n\nclass ConditionalInstanceNorm2dPlus(nn.Module):\n  def __init__(self, num_features, num_classes, bias=True):\n    super().__init__()\n    self.num_features = num_features\n    self.bias = bias\n    self.instance_norm = nn.InstanceNorm2d(num_features, affine=False, track_running_stats=False)\n    if bias:\n      self.embed = nn.Embedding(num_classes, num_features * 3)\n      self.embed.weight.data[:, :2 * num_features].normal_(1, 0.02)  # Initialise scale at N(1, 0.02)\n      self.embed.weight.data[:, 2 * num_features:].zero_()  # Initialise bias at 0\n    else:\n      self.embed = nn.Embedding(num_classes, 2 * num_features)\n      self.embed.weight.data.normal_(1, 0.02)\n\n  def forward(self, x, y):\n    means = torch.mean(x, dim=(2, 3))\n    m = torch.mean(means, dim=-1, keepdim=True)\n    v = torch.var(means, dim=-1, keepdim=True)\n    means = (means - m) / (torch.sqrt(v + 1e-5))\n    h = self.instance_norm(x)\n\n    if self.bias:\n      gamma, alpha, beta = self.embed(y).chunk(3, dim=-1)\n      h = h + means[..., None, None] * alpha[..., None, None]\n      out = gamma.view(-1, self.num_features, 1, 1) * h + beta.view(-1, self.num_features, 1, 1)\n    else:\n      gamma, alpha = self.embed(y).chunk(2, dim=-1)\n      h = h + means[..., None, None] * alpha[..., None, None]\n      out = gamma.view(-1, self.num_features, 1, 1) * h\n    return out","metadata":{"id":"eIXZpHKkFflO","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:31.983987Z","iopub.execute_input":"2023-12-17T20:04:31.984341Z","iopub.status.idle":"2023-12-17T20:04:32.026852Z","shell.execute_reply.started":"2023-12-17T20:04:31.984306Z","shell.execute_reply":"2023-12-17T20:04:32.025743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Layers**","metadata":{"id":"iwFiw6IaFjDP"}},{"cell_type":"code","source":"# @title\n# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: skip-file\n\"\"\"Common layers for defining score networks.\n\"\"\"\nimport math\nimport string\nfrom functools import partial\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\n\n\n\ndef get_act_layers(config):\n  \"\"\"Get activation functions from the config file.\"\"\"\n\n  if config.model.nonlinearity.lower() == 'elu':\n    return nn.ELU()\n  elif config.model.nonlinearity.lower() == 'relu':\n    return nn.ReLU()\n  elif config.model.nonlinearity.lower() == 'lrelu':\n    return nn.LeakyReLU(negative_slope=0.2)\n  elif config.model.nonlinearity.lower() == 'swish':\n    return nn.SiLU()\n  else:\n    raise NotImplementedError('activation function does not exist!')\n\n\ndef ncsn_conv1x1(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1., padding=0):\n  \"\"\"1x1 convolution. Same as NCSNv1/v2.\"\"\"\n  conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=bias, dilation=dilation,\n                   padding=padding)\n  init_scale = 1e-10 if init_scale == 0 else init_scale\n  conv.weight.data *= init_scale\n  conv.bias.data *= init_scale\n  return conv\n\n\ndef variance_scaling(scale, mode, distribution,\n                     in_axis=1, out_axis=0,\n                     dtype=torch.float32,\n                     device='cpu'):\n  \"\"\"Ported from JAX. \"\"\"\n\n  def _compute_fans(shape, in_axis=1, out_axis=0):\n    receptive_field_size = np.prod(shape) / shape[in_axis] / shape[out_axis]\n    fan_in = shape[in_axis] * receptive_field_size\n    fan_out = shape[out_axis] * receptive_field_size\n    return fan_in, fan_out\n\n  def init(shape, dtype=dtype, device=device):\n    fan_in, fan_out = _compute_fans(shape, in_axis, out_axis)\n    if mode == \"fan_in\":\n      denominator = fan_in\n    elif mode == \"fan_out\":\n      denominator = fan_out\n    elif mode == \"fan_avg\":\n      denominator = (fan_in + fan_out) / 2\n    else:\n      raise ValueError(\n        \"invalid mode for variance scaling initializer: {}\".format(mode))\n    variance = scale / denominator\n    if distribution == \"normal\":\n      return torch.randn(*shape, dtype=dtype, device=device) * np.sqrt(variance)\n    elif distribution == \"uniform\":\n      return (torch.rand(*shape, dtype=dtype, device=device) * 2. - 1.) * np.sqrt(3 * variance)\n    else:\n      raise ValueError(\"invalid distribution for variance scaling initializer\")\n\n  return init\n\n\ndef default_init_layer(scale=1.):\n  \"\"\"The same initialization used in DDPM.\"\"\"\n  scale = 1e-10 if scale == 0 else scale\n  return variance_scaling(scale, 'fan_avg', 'uniform')\n\n\nclass Dense(nn.Module):\n  \"\"\"Linear layer with `default_init`.\"\"\"\n  def __init__(self):\n    super().__init__()\n\n\ndef ddpm_conv1x1_layer(in_planes, out_planes, stride=1, bias=True, init_scale=1., padding=0):\n  \"\"\"1x1 convolution with DDPM initialization.\"\"\"\n  conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, padding=padding, bias=bias)\n  conv.weight.data = default_init_layer(init_scale)(conv.weight.data.shape)\n  nn.init.zeros_(conv.bias)\n  return conv\n\n\ndef ncsn_conv3x3(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1., padding=1):\n  \"\"\"3x3 convolution with PyTorch initialization. Same as NCSNv1/NCSNv2.\"\"\"\n  init_scale = 1e-10 if init_scale == 0 else init_scale\n  conv = nn.Conv2d(in_planes, out_planes, stride=stride, bias=bias,\n                   dilation=dilation, padding=padding, kernel_size=3)\n  conv.weight.data *= init_scale\n  conv.bias.data *= init_scale\n  return conv\n\n\ndef ddpm_conv3x3_layer(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1., padding=1):\n  \"\"\"3x3 convolution with DDPM initialization.\"\"\"\n  conv = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=padding,\n                   dilation=dilation, bias=bias)\n  conv.weight.data = default_init_layer(init_scale)(conv.weight.data.shape)\n  nn.init.zeros_(conv.bias)\n  return conv\n\n  ###########################################################################\n  # Functions below are ported over from the NCSNv1/NCSNv2 codebase:\n  # https://github.com/ermongroup/ncsn\n  # https://github.com/ermongroup/ncsnv2\n  ###########################################################################\n\n\nclass CRPBlock(nn.Module):\n  def __init__(self, features, n_stages, act=nn.ReLU(), maxpool=True):\n    super().__init__()\n    self.convs = nn.ModuleList()\n    for i in range(n_stages):\n      self.convs.append(ncsn_conv3x3(features, features, stride=1, bias=False))\n    self.n_stages = n_stages\n    if maxpool:\n      self.pool = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)\n    else:\n      self.pool = nn.AvgPool2d(kernel_size=5, stride=1, padding=2)\n\n    self.act = act\n\n  def forward(self, x):\n    x = self.act(x)\n    path = x\n    for i in range(self.n_stages):\n      path = self.pool(path)\n      path = self.convs[i](path)\n      x = path + x\n    return x\n\n\nclass CondCRPBlock(nn.Module):\n  def __init__(self, features, n_stages, num_classes, normalizer, act=nn.ReLU()):\n    super().__init__()\n    self.convs = nn.ModuleList()\n    self.norms = nn.ModuleList()\n    self.normalizer = normalizer\n    for i in range(n_stages):\n      self.norms.append(normalizer(features, num_classes, bias=True))\n      self.convs.append(ncsn_conv3x3(features, features, stride=1, bias=False))\n\n    self.n_stages = n_stages\n    self.pool = nn.AvgPool2d(kernel_size=5, stride=1, padding=2)\n    self.act = act\n\n  def forward(self, x, y):\n    x = self.act(x)\n    path = x\n    for i in range(self.n_stages):\n      path = self.norms[i](path, y)\n      path = self.pool(path)\n      path = self.convs[i](path)\n\n      x = path + x\n    return x\n\n\nclass RCUBlock(nn.Module):\n  def __init__(self, features, n_blocks, n_stages, act=nn.ReLU()):\n    super().__init__()\n\n    for i in range(n_blocks):\n      for j in range(n_stages):\n        setattr(self, '{}_{}_conv'.format(i + 1, j + 1), ncsn_conv3x3(features, features, stride=1, bias=False))\n\n    self.stride = 1\n    self.n_blocks = n_blocks\n    self.n_stages = n_stages\n    self.act = act\n\n  def forward(self, x):\n    for i in range(self.n_blocks):\n      residual = x\n      for j in range(self.n_stages):\n        x = self.act(x)\n        x = getattr(self, '{}_{}_conv'.format(i + 1, j + 1))(x)\n\n      x += residual\n    return x\n\n\nclass CondRCUBlock(nn.Module):\n  def __init__(self, features, n_blocks, n_stages, num_classes, normalizer, act=nn.ReLU()):\n    super().__init__()\n\n    for i in range(n_blocks):\n      for j in range(n_stages):\n        setattr(self, '{}_{}_norm'.format(i + 1, j + 1), normalizer(features, num_classes, bias=True))\n        setattr(self, '{}_{}_conv'.format(i + 1, j + 1), ncsn_conv3x3(features, features, stride=1, bias=False))\n\n    self.stride = 1\n    self.n_blocks = n_blocks\n    self.n_stages = n_stages\n    self.act = act\n    self.normalizer = normalizer\n\n  def forward(self, x, y):\n    for i in range(self.n_blocks):\n      residual = x\n      for j in range(self.n_stages):\n        x = getattr(self, '{}_{}_norm'.format(i + 1, j + 1))(x, y)\n        x = self.act(x)\n        x = getattr(self, '{}_{}_conv'.format(i + 1, j + 1))(x)\n\n      x += residual\n    return x\n\n\nclass MSFBlock(nn.Module):\n  def __init__(self, in_planes, features):\n    super().__init__()\n    assert isinstance(in_planes, list) or isinstance(in_planes, tuple)\n    self.convs = nn.ModuleList()\n    self.features = features\n\n    for i in range(len(in_planes)):\n      self.convs.append(ncsn_conv3x3(in_planes[i], features, stride=1, bias=True))\n\n  def forward(self, xs, shape):\n    sums = torch.zeros(xs[0].shape[0], self.features, *shape, device=xs[0].device)\n    for i in range(len(self.convs)):\n      h = self.convs[i](xs[i])\n      h = F.interpolate(h, size=shape, mode='bilinear', align_corners=True)\n      sums += h\n    return sums\n\n\nclass CondMSFBlock(nn.Module):\n  def __init__(self, in_planes, features, num_classes, normalizer):\n    super().__init__()\n    assert isinstance(in_planes, list) or isinstance(in_planes, tuple)\n\n    self.convs = nn.ModuleList()\n    self.norms = nn.ModuleList()\n    self.features = features\n    self.normalizer = normalizer\n\n    for i in range(len(in_planes)):\n      self.convs.append(ncsn_conv3x3(in_planes[i], features, stride=1, bias=True))\n      self.norms.append(normalizer(in_planes[i], num_classes, bias=True))\n\n  def forward(self, xs, y, shape):\n    sums = torch.zeros(xs[0].shape[0], self.features, *shape, device=xs[0].device)\n    for i in range(len(self.convs)):\n      h = self.norms[i](xs[i], y)\n      h = self.convs[i](h)\n      h = F.interpolate(h, size=shape, mode='bilinear', align_corners=True)\n      sums += h\n    return sums\n\n\nclass RefineBlock(nn.Module):\n  def __init__(self, in_planes, features, act=nn.ReLU(), start=False, end=False, maxpool=True):\n    super().__init__()\n\n    assert isinstance(in_planes, tuple) or isinstance(in_planes, list)\n    self.n_blocks = n_blocks = len(in_planes)\n\n    self.adapt_convs = nn.ModuleList()\n    for i in range(n_blocks):\n      self.adapt_convs.append(RCUBlock(in_planes[i], 2, 2, act))\n\n    self.output_convs = RCUBlock(features, 3 if end else 1, 2, act)\n\n    if not start:\n      self.msf = MSFBlock(in_planes, features)\n\n    self.crp = CRPBlock(features, 2, act, maxpool=maxpool)\n\n  def forward(self, xs, output_shape):\n    assert isinstance(xs, tuple) or isinstance(xs, list)\n    hs = []\n    for i in range(len(xs)):\n      h = self.adapt_convs[i](xs[i])\n      hs.append(h)\n\n    if self.n_blocks > 1:\n      h = self.msf(hs, output_shape)\n    else:\n      h = hs[0]\n\n    h = self.crp(h)\n    h = self.output_convs(h)\n\n    return h\n\n\nclass CondRefineBlock(nn.Module):\n  def __init__(self, in_planes, features, num_classes, normalizer, act=nn.ReLU(), start=False, end=False):\n    super().__init__()\n\n    assert isinstance(in_planes, tuple) or isinstance(in_planes, list)\n    self.n_blocks = n_blocks = len(in_planes)\n\n    self.adapt_convs = nn.ModuleList()\n    for i in range(n_blocks):\n      self.adapt_convs.append(\n        CondRCUBlock(in_planes[i], 2, 2, num_classes, normalizer, act)\n      )\n\n    self.output_convs = CondRCUBlock(features, 3 if end else 1, 2, num_classes, normalizer, act)\n\n    if not start:\n      self.msf = CondMSFBlock(in_planes, features, num_classes, normalizer)\n\n    self.crp = CondCRPBlock(features, 2, num_classes, normalizer, act)\n\n  def forward(self, xs, y, output_shape):\n    assert isinstance(xs, tuple) or isinstance(xs, list)\n    hs = []\n    for i in range(len(xs)):\n      h = self.adapt_convs[i](xs[i], y)\n      hs.append(h)\n\n    if self.n_blocks > 1:\n      h = self.msf(hs, y, output_shape)\n    else:\n      h = hs[0]\n\n    h = self.crp(h, y)\n    h = self.output_convs(h, y)\n\n    return h\n\n\nclass ConvMeanPool(nn.Module):\n  def __init__(self, input_dim, output_dim, kernel_size=3, biases=True, adjust_padding=False):\n    super().__init__()\n    if not adjust_padding:\n      conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride=1, padding=kernel_size // 2, bias=biases)\n      self.conv = conv\n    else:\n      conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride=1, padding=kernel_size // 2, bias=biases)\n\n      self.conv = nn.Sequential(\n        nn.ZeroPad2d((1, 0, 1, 0)),\n        conv\n      )\n\n  def forward(self, inputs):\n    output = self.conv(inputs)\n    output = sum([output[:, :, ::2, ::2], output[:, :, 1::2, ::2],\n                  output[:, :, ::2, 1::2], output[:, :, 1::2, 1::2]]) / 4.\n    return output\n\n\nclass MeanPoolConv(nn.Module):\n  def __init__(self, input_dim, output_dim, kernel_size=3, biases=True):\n    super().__init__()\n    self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride=1, padding=kernel_size // 2, bias=biases)\n\n  def forward(self, inputs):\n    output = inputs\n    output = sum([output[:, :, ::2, ::2], output[:, :, 1::2, ::2],\n                  output[:, :, ::2, 1::2], output[:, :, 1::2, 1::2]]) / 4.\n    return self.conv(output)\n\n\nclass UpsampleConv(nn.Module):\n  def __init__(self, input_dim, output_dim, kernel_size=3, biases=True):\n    super().__init__()\n    self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride=1, padding=kernel_size // 2, bias=biases)\n    self.pixelshuffle = nn.PixelShuffle(upscale_factor=2)\n\n  def forward(self, inputs):\n    output = inputs\n    output = torch.cat([output, output, output, output], dim=1)\n    output = self.pixelshuffle(output)\n    return self.conv(output)\n\n\nclass ConditionalResidualBlock(nn.Module):\n  def __init__(self, input_dim, output_dim, num_classes, resample=1, act=nn.ELU(),\n               normalization=ConditionalInstanceNorm2dPlus, adjust_padding=False, dilation=None):\n    super().__init__()\n    self.non_linearity = act\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.resample = resample\n    self.normalization = normalization\n    if resample == 'down':\n      if dilation > 1:\n        self.conv1 = ncsn_conv3x3(input_dim, input_dim, dilation=dilation)\n        self.normalize2 = normalization(input_dim, num_classes)\n        self.conv2 = ncsn_conv3x3(input_dim, output_dim, dilation=dilation)\n        conv_shortcut = partial(ncsn_conv3x3, dilation=dilation)\n      else:\n        self.conv1 = ncsn_conv3x3(input_dim, input_dim)\n        self.normalize2 = normalization(input_dim, num_classes)\n        self.conv2 = ConvMeanPool(input_dim, output_dim, 3, adjust_padding=adjust_padding)\n        conv_shortcut = partial(ConvMeanPool, kernel_size=1, adjust_padding=adjust_padding)\n\n    elif resample is None:\n      if dilation > 1:\n        conv_shortcut = partial(ncsn_conv3x3, dilation=dilation)\n        self.conv1 = ncsn_conv3x3(input_dim, output_dim, dilation=dilation)\n        self.normalize2 = normalization(output_dim, num_classes)\n        self.conv2 = ncsn_conv3x3(output_dim, output_dim, dilation=dilation)\n      else:\n        conv_shortcut = nn.Conv2d\n        self.conv1 = ncsn_conv3x3(input_dim, output_dim)\n        self.normalize2 = normalization(output_dim, num_classes)\n        self.conv2 = ncsn_conv3x3(output_dim, output_dim)\n    else:\n      raise Exception('invalid resample value')\n\n    if output_dim != input_dim or resample is not None:\n      self.shortcut = conv_shortcut(input_dim, output_dim)\n\n    self.normalize1 = normalization(input_dim, num_classes)\n\n  def forward(self, x, y):\n    output = self.normalize1(x, y)\n    output = self.non_linearity(output)\n    output = self.conv1(output)\n    output = self.normalize2(output, y)\n    output = self.non_linearity(output)\n    output = self.conv2(output)\n\n    if self.output_dim == self.input_dim and self.resample is None:\n      shortcut = x\n    else:\n      shortcut = self.shortcut(x)\n\n    return shortcut + output\n\n\nclass ResidualBlock(nn.Module):\n  def __init__(self, input_dim, output_dim, resample=None, act=nn.ELU(),\n               normalization=nn.InstanceNorm2d, adjust_padding=False, dilation=1):\n    super().__init__()\n    self.non_linearity = act\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.resample = resample\n    self.normalization = normalization\n    if resample == 'down':\n      if dilation > 1:\n        self.conv1 = ncsn_conv3x3(input_dim, input_dim, dilation=dilation)\n        self.normalize2 = normalization(input_dim)\n        self.conv2 = ncsn_conv3x3(input_dim, output_dim, dilation=dilation)\n        conv_shortcut = partial(ncsn_conv3x3, dilation=dilation)\n      else:\n        self.conv1 = ncsn_conv3x3(input_dim, input_dim)\n        self.normalize2 = normalization(input_dim)\n        self.conv2 = ConvMeanPool(input_dim, output_dim, 3, adjust_padding=adjust_padding)\n        conv_shortcut = partial(ConvMeanPool, kernel_size=1, adjust_padding=adjust_padding)\n\n    elif resample is None:\n      if dilation > 1:\n        conv_shortcut = partial(ncsn_conv3x3, dilation=dilation)\n        self.conv1 = ncsn_conv3x3(input_dim, output_dim, dilation=dilation)\n        self.normalize2 = normalization(output_dim)\n        self.conv2 = ncsn_conv3x3(output_dim, output_dim, dilation=dilation)\n      else:\n        # conv_shortcut = nn.Conv2d ### Something wierd here.\n        conv_shortcut = partial(ncsn_conv1x1)\n        self.conv1 = ncsn_conv3x3(input_dim, output_dim)\n        self.normalize2 = normalization(output_dim)\n        self.conv2 = ncsn_conv3x3(output_dim, output_dim)\n    else:\n      raise Exception('invalid resample value')\n\n    if output_dim != input_dim or resample is not None:\n      self.shortcut = conv_shortcut(input_dim, output_dim)\n\n    self.normalize1 = normalization(input_dim)\n\n  def forward(self, x):\n    output = self.normalize1(x)\n    output = self.non_linearity(output)\n    output = self.conv1(output)\n    output = self.normalize2(output)\n    output = self.non_linearity(output)\n    output = self.conv2(output)\n\n    if self.output_dim == self.input_dim and self.resample is None:\n      shortcut = x\n    else:\n      shortcut = self.shortcut(x)\n\n    return shortcut + output\n\n\n###########################################################################\n# Functions below are ported over from the DDPM codebase:\n#  https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/nn.py\n###########################################################################\n\ndef get_timestep_embedding(timesteps, embedding_dim, max_positions=10000):\n  assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n  half_dim = embedding_dim // 2\n  # magic number 10000 is from transformers\n  emb = math.log(max_positions) / (half_dim - 1)\n  # emb = math.log(2.) / (half_dim - 1)\n  emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n  # emb = tf.range(num_embeddings, dtype=jnp.float32)[:, None] * emb[None, :]\n  # emb = tf.cast(timesteps, dtype=jnp.float32)[:, None] * emb[None, :]\n  emb = timesteps.float()[:, None] * emb[None, :]\n  emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n  if embedding_dim % 2 == 1:  # zero pad\n    emb = F.pad(emb, (0, 1), mode='constant')\n  assert emb.shape == (timesteps.shape[0], embedding_dim)\n  return emb\n\n\ndef _einsum(a, b, c, x, y):\n  einsum_str = '{},{}->{}'.format(''.join(a), ''.join(b), ''.join(c))\n  return torch.einsum(einsum_str, x, y)\n\n\ndef contract_inner(x, y):\n  \"\"\"tensordot(x, y, 1).\"\"\"\n  x_chars = list(string.ascii_lowercase[:len(x.shape)])\n  y_chars = list(string.ascii_lowercase[len(x.shape):len(y.shape) + len(x.shape)])\n  y_chars[0] = x_chars[-1]  # first axis of y and last of x get summed\n  out_chars = x_chars[:-1] + y_chars[1:]\n  return _einsum(x_chars, y_chars, out_chars, x, y)\n\n\n#class NIN(nn.Module):\nclass NIN_layer(nn.Module):\n  def __init__(self, in_dim, num_units, init_scale=0.1):\n    super().__init__()\n    self.W = nn.Parameter(default_init_layer(scale=init_scale)((in_dim, num_units)), requires_grad=True)\n    self.b = nn.Parameter(torch.zeros(num_units), requires_grad=True)\n\n  def forward(self, x):\n    x = x.permute(0, 2, 3, 1)\n    y = contract_inner(x, self.W) + self.b\n    return y.permute(0, 3, 1, 2)\n\n\n#class AttnBlock(nn.Module):\nclass AttnBlock_layer(nn.Module):\n  \"\"\"Channel-wise self-attention block.\"\"\"\n  def __init__(self, channels):\n    super().__init__()\n    self.GroupNorm_0 = nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6)\n    self.NIN_0 = NIN_layer(channels, channels)\n    self.NIN_1 = NIN_layer(channels, channels)\n    self.NIN_2 = NIN_layer(channels, channels)\n    self.NIN_3 = NIN_layer(channels, channels, init_scale=0.)\n\n  def forward(self, x):\n    B, C, H, W = x.shape\n    h = self.GroupNorm_0(x)\n    q = self.NIN_0(h)\n    k = self.NIN_1(h)\n    v = self.NIN_2(h)\n\n    w = torch.einsum('bchw,bcij->bhwij', q, k) * (int(C) ** (-0.5))\n    w = torch.reshape(w, (B, H, W, H * W))\n    w = F.softmax(w, dim=-1)\n    w = torch.reshape(w, (B, H, W, H, W))\n    h = torch.einsum('bhwij,bcij->bchw', w, v)\n    h = self.NIN_3(h)\n    return x + h\n\n\n#class Upsample(nn.Module):\nclass Upsample_layer(nn.Module):\n  def __init__(self, channels, with_conv=False):\n    super().__init__()\n    if with_conv:\n      self.Conv_0 = ddpm_conv3x3_layer(channels, channels)\n    self.with_conv = with_conv\n\n  def forward(self, x):\n    B, C, H, W = x.shape\n    h = F.interpolate(x, (H * 2, W * 2), mode='nearest')\n    if self.with_conv:\n      h = self.Conv_0(h)\n    return h\n\n\n#class Downsample(nn.Module):\nclass Downsample_layers(nn.Module):\n  def __init__(self, channels, with_conv=False):\n    super().__init__()\n    if with_conv:\n      self.Conv_0 = ddpm_conv3x3_layer(channels, channels, stride=2, padding=0)\n    self.with_conv = with_conv\n\n  def forward(self, x):\n    B, C, H, W = x.shape\n    # Emulate 'SAME' padding\n    if self.with_conv:\n      x = F.pad(x, (0, 1, 0, 1))\n      x = self.Conv_0(x)\n    else:\n      x = F.avg_pool2d(x, kernel_size=2, stride=2, padding=0)\n\n    assert x.shape == (B, C, H // 2, W // 2)\n    return x\n\n\n#class ResnetBlockDDPM(nn.Module):\nclass ResnetBlockDDPM_layers(nn.Module):\n  \"\"\"The ResNet Blocks used in DDPM.\"\"\"\n  def __init__(self, act, in_ch, out_ch=None, temb_dim=None, conv_shortcut=False, dropout=0.1):\n    super().__init__()\n    if out_ch is None:\n      out_ch = in_ch\n    self.GroupNorm_0 = nn.GroupNorm(num_groups=32, num_channels=in_ch, eps=1e-6)\n    self.act = act\n    self.Conv_0 = ddpm_conv3x3_layer(in_ch, out_ch)\n    if temb_dim is not None:\n      self.Dense_0 = nn.Linear(temb_dim, out_ch)\n      self.Dense_0.weight.data = default_init_layer()(self.Dense_0.weight.data.shape)\n      nn.init.zeros_(self.Dense_0.bias)\n\n    self.GroupNorm_1 = nn.GroupNorm(num_groups=32, num_channels=out_ch, eps=1e-6)\n    self.Dropout_0 = nn.Dropout(dropout)\n    self.Conv_1 = ddpm_conv3x3_layer(out_ch, out_ch, init_scale=0.)\n    if in_ch != out_ch:\n      if conv_shortcut:\n        self.Conv_2 = ddpm_conv3x3_layer(in_ch, out_ch)\n      else:\n        self.NIN_0 = NIN_layer(in_ch, out_ch)\n    self.out_ch = out_ch\n    self.in_ch = in_ch\n    self.conv_shortcut = conv_shortcut\n\n  def forward(self, x, temb=None):\n    B, C, H, W = x.shape\n    assert C == self.in_ch\n    out_ch = self.out_ch if self.out_ch else self.in_ch\n    h = self.act(self.GroupNorm_0(x))\n    h = self.Conv_0(h)\n    # Add bias to each feature map conditioned on the time embedding\n    if temb is not None:\n      h += self.Dense_0(self.act(temb))[:, :, None, None]\n    h = self.act(self.GroupNorm_1(h))\n    h = self.Dropout_0(h)\n    h = self.Conv_1(h)\n    if C != out_ch:\n      if self.conv_shortcut:\n        x = self.Conv_2(x)\n      else:\n        x = self.NIN_0(x)\n    return x + h","metadata":{"id":"FXt8HxR4FsxV","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:32.030974Z","iopub.execute_input":"2023-12-17T20:04:32.031239Z","iopub.status.idle":"2023-12-17T20:04:32.144163Z","shell.execute_reply.started":"2023-12-17T20:04:32.031216Z","shell.execute_reply":"2023-12-17T20:04:32.143306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Up-sampling or Down-sampling**","metadata":{"id":"rOeNcrAnFwb2"}},{"cell_type":"code","source":"# @title\n\"\"\"Layers used for up-sampling or down-sampling images.\nMany functions are ported from https://github.com/NVlabs/stylegan2.\n\"\"\"\n\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\n#from op import upfirdn2d conv2d\n\n\n# Function ported from StyleGAN2\ndef get_weight(module,\n               shape,\n               weight_var='weight',\n               kernel_init=None):\n  \"\"\"Get/create weight tensor for a convolution or fully-connected layer.\"\"\"\n\n  return module.param(weight_var, kernel_init, shape)\n\n#class Conv2d(nn.Module):\nclass Conv2d_up_down_sample(nn.Module):\n  \"\"\"Conv2d layer with optimal upsampling and downsampling (StyleGAN2).\"\"\"\n\n  def __init__(self, in_ch, out_ch, kernel, up=False, down=False,\n               resample_kernel=(1, 3, 3, 1),\n               use_bias=True,\n               kernel_init=None):\n    super().__init__()\n    assert not (up and down)\n    assert kernel >= 1 and kernel % 2 == 1\n    self.weight = nn.Parameter(torch.zeros(out_ch, in_ch, kernel, kernel))\n    if kernel_init is not None:\n      self.weight.data = kernel_init(self.weight.data.shape)\n    if use_bias:\n      self.bias = nn.Parameter(torch.zeros(out_ch))\n\n    self.up = up\n    self.down = down\n    self.resample_kernel = resample_kernel\n    self.kernel = kernel\n    self.use_bias = use_bias\n\n  def forward(self, x):\n    if self.up:\n      x = upsample_conv_2d(x, self.weight, k=self.resample_kernel)\n    elif self.down:\n      x = conv_downsample_2d(x, self.weight, k=self.resample_kernel)\n    else:\n      x = F.conv2d(x, self.weight, stride=1, padding=self.kernel // 2)\n\n    if self.use_bias:\n      x = x + self.bias.reshape(1, -1, 1, 1)\n\n    return x\n\n\ndef naive_upsample_2d(x, factor=2):\n  _N, C, H, W = x.shape\n  x = torch.reshape(x, (-1, C, H, 1, W, 1))\n  x = x.repeat(1, 1, 1, factor, 1, factor)\n  return torch.reshape(x, (-1, C, H * factor, W * factor))\n\n\ndef naive_downsample_2d(x, factor=2):\n  _N, C, H, W = x.shape\n  x = torch.reshape(x, (-1, C, H // factor, factor, W // factor, factor))\n  return torch.mean(x, dim=(3, 5))\n\n\ndef upsample_conv_2d(x, w, k=None, factor=2, gain=1):\n  \"\"\"Fused `upsample_2d()` followed by `tf.nn.conv2d()`.\n     Padding is performed only once at the beginning, not between the\n     operations.\n     The fused op is considerably more efficient than performing the same\n     calculation\n     using standard TensorFlow ops. It supports gradients of arbitrary order.\n     Args:\n       x:            Input tensor of the shape `[N, C, H, W]` or `[N, H, W,\n         C]`.\n       w:            Weight tensor of the shape `[filterH, filterW, inChannels,\n         outChannels]`. Grouped convolution can be performed by `inChannels =\n         x.shape[0] // numGroups`.\n       k:            FIR filter of the shape `[firH, firW]` or `[firN]`\n         (separable). The default is `[1] * factor`, which corresponds to\n         nearest-neighbor upsampling.\n       factor:       Integer upsampling factor (default: 2).\n       gain:         Scaling factor for signal magnitude (default: 1.0).\n     Returns:\n       Tensor of the shape `[N, C, H * factor, W * factor]` or\n       `[N, H * factor, W * factor, C]`, and same datatype as `x`.\n  \"\"\"\n\n  assert isinstance(factor, int) and factor >= 1\n\n  # Check weight shape.\n  assert len(w.shape) == 4\n  convH = w.shape[2]\n  convW = w.shape[3]\n  inC = w.shape[1]\n  outC = w.shape[0]\n\n  assert convW == convH\n\n  # Setup filter kernel.\n  if k is None:\n    k = [1] * factor\n  k = _setup_kernel(k) * (gain * (factor ** 2))\n  p = (k.shape[0] - factor) - (convW - 1)\n\n  stride = (factor, factor)\n\n  # Determine data dimensions.\n  stride = [1, 1, factor, factor]\n  output_shape = ((_shape(x, 2) - 1) * factor + convH, (_shape(x, 3) - 1) * factor + convW)\n  output_padding = (output_shape[0] - (_shape(x, 2) - 1) * stride[0] - convH,\n                    output_shape[1] - (_shape(x, 3) - 1) * stride[1] - convW)\n  assert output_padding[0] >= 0 and output_padding[1] >= 0\n  num_groups = _shape(x, 1) // inC\n\n  # Transpose weights.\n  w = torch.reshape(w, (num_groups, -1, inC, convH, convW))\n  w = w[..., ::-1, ::-1].permute(0, 2, 1, 3, 4)\n  w = torch.reshape(w, (num_groups * inC, -1, convH, convW))\n\n  x = F.conv_transpose2d(x, w, stride=stride, output_padding=output_padding, padding=0)\n  ## Original TF code.\n  # x = tf.nn.conv2d_transpose(\n  #     x,\n  #     w,\n  #     output_shape=output_shape,\n  #     strides=stride,\n  #     padding='VALID',\n  #     data_format=data_format)\n  ## JAX equivalent\n\n  return upfirdn2d(x, torch.tensor(k, device=x.device),\n                   pad=((p + 1) // 2 + factor - 1, p // 2 + 1))\n\n\ndef conv_downsample_2d(x, w, k=None, factor=2, gain=1):\n  \"\"\"Fused `tf.nn.conv2d()` followed by `downsample_2d()`.\n    Padding is performed only once at the beginning, not between the operations.\n    The fused op is considerably more efficient than performing the same\n    calculation\n    using standard TensorFlow ops. It supports gradients of arbitrary order.\n    Args:\n        x:            Input tensor of the shape `[N, C, H, W]` or `[N, H, W,\n          C]`.\n        w:            Weight tensor of the shape `[filterH, filterW, inChannels,\n          outChannels]`. Grouped convolution can be performed by `inChannels =\n          x.shape[0] // numGroups`.\n        k:            FIR filter of the shape `[firH, firW]` or `[firN]`\n          (separable). The default is `[1] * factor`, which corresponds to\n          average pooling.\n        factor:       Integer downsampling factor (default: 2).\n        gain:         Scaling factor for signal magnitude (default: 1.0).\n    Returns:\n        Tensor of the shape `[N, C, H // factor, W // factor]` or\n        `[N, H // factor, W // factor, C]`, and same datatype as `x`.\n  \"\"\"\n\n  assert isinstance(factor, int) and factor >= 1\n  _outC, _inC, convH, convW = w.shape\n  assert convW == convH\n  if k is None:\n    k = [1] * factor\n  k = _setup_kernel(k) * gain\n  p = (k.shape[0] - factor) + (convW - 1)\n  s = [factor, factor]\n  x = upfirdn2d(x, torch.tensor(k, device=x.device),\n                pad=((p + 1) // 2, p // 2))\n  return F.conv2d(x, w, stride=s, padding=0)\n\n\ndef _setup_kernel(k):\n  k = np.asarray(k, dtype=np.float32)\n  if k.ndim == 1:\n    k = np.outer(k, k)\n  k /= np.sum(k)\n  assert k.ndim == 2\n  assert k.shape[0] == k.shape[1]\n  return k\n\n\ndef _shape(x, dim):\n  return x.shape[dim]\n\n\ndef upsample_2d(x, k=None, factor=2, gain=1):\n  r\"\"\"Upsample a batch of 2D images with the given filter.\n    Accepts a batch of 2D images of the shape `[N, C, H, W]` or `[N, H, W, C]`\n    and upsamples each image with the given filter. The filter is normalized so\n    that\n    if the input pixels are constant, they will be scaled by the specified\n    `gain`.\n    Pixels outside the image are assumed to be zero, and the filter is padded\n    with\n    zeros so that its shape is a multiple of the upsampling factor.\n    Args:\n        x:            Input tensor of the shape `[N, C, H, W]` or `[N, H, W,\n          C]`.\n        k:            FIR filter of the shape `[firH, firW]` or `[firN]`\n          (separable). The default is `[1] * factor`, which corresponds to\n          nearest-neighbor upsampling.\n        factor:       Integer upsampling factor (default: 2).\n        gain:         Scaling factor for signal magnitude (default: 1.0).\n    Returns:\n        Tensor of the shape `[N, C, H * factor, W * factor]`\n  \"\"\"\n  assert isinstance(factor, int) and factor >= 1\n  if k is None:\n    k = [1] * factor\n  k = _setup_kernel(k) * (gain * (factor ** 2))\n  p = k.shape[0] - factor\n  return upfirdn2d(x, torch.tensor(k, device=x.device),\n                   up=factor, pad=((p + 1) // 2 + factor - 1, p // 2))\n\n\ndef downsample_2d(x, k=None, factor=2, gain=1):\n  r\"\"\"Downsample a batch of 2D images with the given filter.\n    Accepts a batch of 2D images of the shape `[N, C, H, W]` or `[N, H, W, C]`\n    and downsamples each image with the given filter. The filter is normalized\n    so that\n    if the input pixels are constant, they will be scaled by the specified\n    `gain`.\n    Pixels outside the image are assumed to be zero, and the filter is padded\n    with\n    zeros so that its shape is a multiple of the downsampling factor.\n    Args:\n        x:            Input tensor of the shape `[N, C, H, W]` or `[N, H, W,\n          C]`.\n        k:            FIR filter of the shape `[firH, firW]` or `[firN]`\n          (separable). The default is `[1] * factor`, which corresponds to\n          average pooling.\n        factor:       Integer downsampling factor (default: 2).\n        gain:         Scaling factor for signal magnitude (default: 1.0).\n    Returns:\n        Tensor of the shape `[N, C, H // factor, W // factor]`\n  \"\"\"\n\n  assert isinstance(factor, int) and factor >= 1\n  if k is None:\n    k = [1] * factor\n  k = _setup_kernel(k) * gain\n  p = k.shape[0] - factor\n  return upfirdn2d(x, torch.tensor(k, device=x.device),\n                   down=factor, pad=((p + 1) // 2, p // 2))","metadata":{"id":"oUzejnPCFzl1","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:32.145639Z","iopub.execute_input":"2023-12-17T20:04:32.146235Z","iopub.status.idle":"2023-12-17T20:04:32.181900Z","shell.execute_reply.started":"2023-12-17T20:04:32.146202Z","shell.execute_reply":"2023-12-17T20:04:32.180920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Layerspp**","metadata":{"id":"134_rXNuF2la"}},{"cell_type":"code","source":"# @title\n# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#conv1x1\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: skip-file\n\"\"\"Layers for defining NCSN++.\n\"\"\"\n#from . import layers\n#from . import up_or_down_sampling\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\n\nconv1x1 = ddpm_conv1x1_layer\nconv3x3 =  ddpm_conv3x3_layer\nNIN = NIN_layer\ndefault_init = default_init_layer\n\n\nclass GaussianFourierProjection(nn.Module):\n  \"\"\"Gaussian Fourier embeddings for noise levels.\"\"\"\n\n  def __init__(self, embedding_size=256, scale=1.0):\n    super().__init__()\n    self.W = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)\n\n  def forward(self, x):\n    x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n    return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n\n\n#class Combine(nn.Module):\nclass Combine_layerpp(nn.Module):\n  \"\"\"Combine information from skip connections.\"\"\"\n\n  def __init__(self, dim1, dim2, method='cat'):\n    super().__init__()\n    self.Conv_0 = conv1x1(dim1, dim2)\n    self.method = method\n\n  def forward(self, x, y):\n    h = self.Conv_0(x)\n    if self.method == 'cat':\n      return torch.cat([h, y], dim=1)\n    elif self.method == 'sum':\n      return h + y\n    else:\n      raise ValueError(f'Method {self.method} not recognized.')\n\n\nclass AttnBlockpp(nn.Module):\n  \"\"\"Channel-wise self-attention block. Modified from DDPM.\"\"\"\n\n  def __init__(self, channels, skip_rescale=False, init_scale=0.):\n    super().__init__()\n    self.GroupNorm_0 = nn.GroupNorm(num_groups=min(channels // 4, 32), num_channels=channels,\n                                  eps=1e-6)\n    self.NIN_0 = NIN_layer(channels, channels)\n    self.NIN_1 = NIN_layer(channels, channels)\n    self.NIN_2 = NIN_layer(channels, channels)\n    self.NIN_3 = NIN_layer(channels, channels, init_scale=init_scale)\n    self.skip_rescale = skip_rescale\n\n  def forward(self, x):\n    B, C, H, W = x.shape\n    h = self.GroupNorm_0(x)\n    q = self.NIN_0(h)\n    k = self.NIN_1(h)\n    v = self.NIN_2(h)\n\n    w = torch.einsum('bchw,bcij->bhwij', q, k) * (int(C) ** (-0.5))\n    w = torch.reshape(w, (B, H, W, H * W))\n    w = F.softmax(w, dim=-1)\n    w = torch.reshape(w, (B, H, W, H, W))\n    h = torch.einsum('bhwij,bcij->bchw', w, v)\n    h = self.NIN_3(h)\n    if not self.skip_rescale:\n      return x + h\n    else:\n      return (x + h) / np.sqrt(2.)\n\n\nclass Upsample_layerpp(nn.Module):\n  def __init__(self, in_ch=None, out_ch=None, with_conv=False, fir=False,\n               fir_kernel=(1, 3, 3, 1)):\n    super().__init__()\n    out_ch = out_ch if out_ch else in_ch\n    if not fir:\n      if with_conv:\n        self.Conv_0 = conv3x3(in_ch, out_ch)\n    else:\n      if with_conv:\n        self.Conv2d_0 = Conv2d_up_down_sample(in_ch, out_ch,\n                                                 kernel=3, up=True,\n                                                 resample_kernel=fir_kernel,\n                                                 use_bias=True,\n                                                 kernel_init=default_init_layer())\n    self.fir = fir\n    self.with_conv = with_conv\n    self.fir_kernel = fir_kernel\n    self.out_ch = out_ch\n\n  def forward(self, x):\n    B, C, H, W = x.shape\n    if not self.fir:\n      h = F.interpolate(x, (H * 2, W * 2), 'nearest')\n      if self.with_conv:\n        h = self.Conv_0(h)\n    else:\n      if not self.with_conv:\n        h = up_or_down_sampling.upsample_2d(x, self.fir_kernel, factor=2)\n      else:\n        h = self.Conv2d_0(x)\n\n    return h\n\n\nclass Downsample_layerpp(nn.Module):\n  def __init__(self, in_ch=None, out_ch=None, with_conv=False, fir=False,\n               fir_kernel=(1, 3, 3, 1)):\n    super().__init__()\n    out_ch = out_ch if out_ch else in_ch\n    if not fir:\n      if with_conv:\n        self.Conv_0 = conv3x3(in_ch, out_ch, stride=2, padding=0)\n    else:\n      if with_conv:\n        self.Conv2d_0 = Conv2d_up_down_sample(in_ch, out_ch,\n                                                 kernel=3, down=True,\n                                                 resample_kernel=fir_kernel,\n                                                 use_bias=True,\n                                                 kernel_init=default_init_layer())\n    self.fir = fir\n    self.fir_kernel = fir_kernel\n    self.with_conv = with_conv\n    self.out_ch = out_ch\n\n  def forward(self, x):\n    B, C, H, W = x.shape\n    if not self.fir:\n      if self.with_conv:\n        x = F.pad(x, (0, 1, 0, 1))\n        x = self.Conv_0(x)\n      else:\n        x = F.avg_pool2d(x, 2, stride=2)\n    else:\n      if not self.with_conv:\n        x = up_or_down_sampling.downsample_2d(x, self.fir_kernel, factor=2)\n      else:\n        x = self.Conv2d_0(x)\n\n    return x\n\n\nclass ResnetBlockDDPMpp_layrepp(nn.Module):\n  \"\"\"ResBlock adapted from DDPM.\"\"\"\n\n  def __init__(self, act, in_ch, out_ch=None, temb_dim=None, conv_shortcut=False,\n               dropout=0.1, skip_rescale=False, init_scale=0.):\n    super().__init__()\n    out_ch = out_ch if out_ch else in_ch\n    self.GroupNorm_0 = nn.GroupNorm(num_groups=min(in_ch // 4, 32), num_channels=in_ch, eps=1e-6)\n    self.Conv_0 = conv3x3(in_ch, out_ch)\n    if temb_dim is not None:\n      self.Dense_0 = nn.Linear(temb_dim, out_ch)\n      self.Dense_0.weight.data = default_init_layer()(self.Dense_0.weight.data.shape)\n      nn.init.zeros_(self.Dense_0.bias)\n    self.GroupNorm_1 = nn.GroupNorm(num_groups=min(out_ch // 4, 32), num_channels=out_ch, eps=1e-6)\n    self.Dropout_0 = nn.Dropout(dropout)\n    self.Conv_1 = conv3x3(out_ch, out_ch, init_scale=init_scale)\n    if in_ch != out_ch:\n      if conv_shortcut:\n        self.Conv_2 = conv3x3(in_ch, out_ch)\n      else:\n        self.NIN_0 = NIN_layer(in_ch, out_ch)\n\n    self.skip_rescale = skip_rescale\n    self.act = act\n    self.out_ch = out_ch\n    self.conv_shortcut = conv_shortcut\n\n  def forward(self, x, temb=None):\n    h = self.act(self.GroupNorm_0(x))\n    h = self.Conv_0(h)\n    if temb is not None:\n      h += self.Dense_0(self.act(temb))[:, :, None, None]\n    h = self.act(self.GroupNorm_1(h))\n    h = self.Dropout_0(h)\n    h = self.Conv_1(h)\n    if x.shape[1] != self.out_ch:\n      if self.conv_shortcut:\n        x = self.Conv_2(x)\n      else:\n        x = self.NIN_0(x)\n    if not self.skip_rescale:\n      return x + h\n    else:\n      return (x + h) / np.sqrt(2.)\n#combine\n\nclass ResnetBlockBigGANpp(nn.Module):\n  def __init__(self, act, in_ch, out_ch=None, temb_dim=None, up=False, down=False,\n               dropout=0.1, fir=False, fir_kernel=(1, 3, 3, 1),\n               skip_rescale=True, init_scale=0.):\n    super().__init__()\n\n    out_ch = out_ch if out_ch else in_ch\n    self.GroupNorm_0 = nn.GroupNorm(num_groups=min(in_ch // 4, 32), num_channels=in_ch, eps=1e-6)\n    self.up = up\n    self.down = down\n    self.fir = fir\n    self.fir_kernel = fir_kernel\n\n    self.Conv_0 = conv3x3(in_ch, out_ch)\n    if temb_dim is not None:\n      self.Dense_0 = nn.Linear(temb_dim, out_ch)\n      self.Dense_0.weight.data = default_init_layer()(self.Dense_0.weight.shape)\n      nn.init.zeros_(self.Dense_0.bias)\n\n    self.GroupNorm_1 = nn.GroupNorm(num_groups=min(out_ch // 4, 32), num_channels=out_ch, eps=1e-6)\n    self.Dropout_0 = nn.Dropout(dropout)\n    self.Conv_1 = conv3x3(out_ch, out_ch, init_scale=init_scale)\n    if in_ch != out_ch or up or down:\n      self.Conv_2 = conv1x1(in_ch, out_ch)\n\n    self.skip_rescale = skip_rescale\n    self.act = act\n    self.in_ch = in_ch\n    self.out_ch = out_ch\n\n  def forward(self, x, temb=None):\n    h = self.act(self.GroupNorm_0(x))\n\n    if self.up:\n      if self.fir:\n        h = upsample_2d(h, self.fir_kernel, factor=2)\n        x = upsample_2d(x, self.fir_kernel, factor=2)\n      else:\n        h = naive_upsample_2d(h, factor=2)\n        x =naive_upsample_2d(x, factor=2)\n    elif self.down:\n      if self.fir:\n        h = downsample_2d(h, self.fir_kernel, factor=2)\n        x = downsample_2d(x, self.fir_kernel, factor=2)\n      else:\n        h = naive_downsample_2d(h, factor=2)\n        x = naive_downsample_2d(x, factor=2)\n\n    h = self.Conv_0(h)\n    # Add bias to each feature map conditioned on the time embedding\n    if temb is not None:\n      h += self.Dense_0(self.act(temb))[:, :, None, None]\n    h = self.act(self.GroupNorm_1(h))\n    h = self.Dropout_0(h)\n    h = self.Conv_1(h)\n\n    if self.in_ch != self.out_ch or self.up or self.down:\n      x = self.Conv_2(x)\n\n    if not self.skip_rescale:\n      return x + h\n    else:\n      return (x + h) / np.sqrt(2.)","metadata":{"id":"3TPVIyO-F4mQ","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:32.183385Z","iopub.execute_input":"2023-12-17T20:04:32.183729Z","iopub.status.idle":"2023-12-17T20:04:32.229883Z","shell.execute_reply.started":"2023-12-17T20:04:32.183703Z","shell.execute_reply":"2023-12-17T20:04:32.228850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**NCSV2**","metadata":{"id":"2jkYtabdF7Z2"}},{"cell_type":"code","source":"# @title\n# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: skip-file\n\"\"\"The NCSNv2 model.\"\"\"\nimport torch\nimport torch.nn as nn\nimport functools\n\n\n\nCondResidualBlock = ConditionalResidualBlock\nconv3x3 = ncsn_conv3x3\n\n\ndef get_network(config):\n  if config.data.image_size < 96:\n    return functools.partial(NCSNv2, config=config)\n  elif 96 <= config.data.image_size <= 128:\n    return functools.partial(NCSNv2_128, config=config)\n  elif 128 < config.data.image_size <= 256:\n    return functools.partial(NCSNv2_256, config=config)\n  else:\n    raise NotImplementedError(\n      f'No network suitable for {config.data.image_size}px implemented yet.')\n\n\n@register_model(name='ncsnv2_64')\nclass NCSNv2(nn.Module):\n  def __init__(self, config):\n    super().__init__()\n    self.centered = config.data.centered\n    self.norm = get_normalization_Normalize(config)\n    self.nf = nf = config.model.nf\n\n    self.act = act = get_act_layers(config)\n    self.register_buffer('sigmas', torch.tensor(get_sigmas(config)))\n    self.config = config\n\n    self.begin_conv = nn.Conv2d(config.data.channels, nf, 3, stride=1, padding=1)\n\n    self.normalizer = self.norm(nf, config.model.num_scales)\n    self.end_conv = nn.Conv2d(nf, config.data.channels, 3, stride=1, padding=1)\n\n    self.res1 = nn.ModuleList([\n      ResidualBlock(self.nf, self.nf, resample=None, act=act,\n                    normalization=self.norm),\n      ResidualBlock(self.nf, self.nf, resample=None, act=act,\n                    normalization=self.norm)]\n    )\n\n    self.res2 = nn.ModuleList([\n      ResidualBlock(self.nf, 2 * self.nf, resample='down', act=act,\n                    normalization=self.norm),\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample=None, act=act,\n                    normalization=self.norm)]\n    )\n\n    self.res3 = nn.ModuleList([\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample='down', act=act,\n                    normalization=self.norm, dilation=2),\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample=None, act=act,\n                    normalization=self.norm, dilation=2)]\n    )\n\n    if config.data.image_size == 28:\n      self.res4 = nn.ModuleList([\n        ResidualBlock(2 * self.nf, 2 * self.nf, resample='down', act=act,\n                      normalization=self.norm, adjust_padding=True, dilation=4),\n        ResidualBlock(2 * self.nf, 2 * self.nf, resample=None, act=act,\n                      normalization=self.norm, dilation=4)]\n      )\n    else:\n      self.res4 = nn.ModuleList([\n        ResidualBlock(2 * self.nf, 2 * self.nf, resample='down', act=act,\n                      normalization=self.norm, adjust_padding=False, dilation=4),\n        ResidualBlock(2 * self.nf, 2 * self.nf, resample=None, act=act,\n                      normalization=self.norm, dilation=4)]\n      )\n\n    self.refine1 = RefineBlock([2 * self.nf], 2 * self.nf, act=act, start=True)\n    self.refine2 = RefineBlock([2 * self.nf, 2 * self.nf], 2 * self.nf, act=act)\n    self.refine3 = RefineBlock([2 * self.nf, 2 * self.nf], self.nf, act=act)\n    self.refine4 = RefineBlock([self.nf, self.nf], self.nf, act=act, end=True)\n\n  def _compute_cond_module(self, module, x):\n    for m in module:\n      x = m(x)\n    return x\n\n  def forward(self, x, y):\n    if not self.centered:\n      h = 2 * x - 1.\n    else:\n      h = x\n\n    output = self.begin_conv(h)\n\n    layer1 = self._compute_cond_module(self.res1, output)\n    layer2 = self._compute_cond_module(self.res2, layer1)\n    layer3 = self._compute_cond_module(self.res3, layer2)\n    layer4 = self._compute_cond_module(self.res4, layer3)\n\n    ref1 = self.refine1([layer4], layer4.shape[2:])\n    ref2 = self.refine2([layer3, ref1], layer3.shape[2:])\n    ref3 = self.refine3([layer2, ref2], layer2.shape[2:])\n    output = self.refine4([layer1, ref3], layer1.shape[2:])\n\n    output = self.normalizer(output)\n    output = self.act(output)\n    output = self.end_conv(output)\n\n    used_sigmas = self.sigmas[y].view(x.shape[0], *([1] * len(x.shape[1:])))\n\n    output = output / used_sigmas\n\n    return output\n\n\n@register_model(name='ncsn')\nclass NCSN(nn.Module):\n  def __init__(self, config):\n    super().__init__()\n    self.centered = config.data.centered\n    self.norm = get_normalization_Normalize(config)\n    self.nf = nf = config.model.nf\n    self.act = act = get_act_layers(config)\n    self.config = config\n\n    self.begin_conv = nn.Conv2d(config.data.channels, nf, 3, stride=1, padding=1)\n\n    self.normalizer = self.norm(nf, config.model.num_scales)\n    self.end_conv = nn.Conv2d(nf, config.data.channels, 3, stride=1, padding=1)\n\n    self.res1 = nn.ModuleList([\n      ConditionalResidualBlock(self.nf, self.nf, config.model.num_scales, resample=None, act=act,\n                               normalization=self.norm),\n      ConditionalResidualBlock(self.nf, self.nf, config.model.num_scales, resample=None, act=act,\n                               normalization=self.norm)]\n    )\n\n    self.res2 = nn.ModuleList([\n      ConditionalResidualBlock(self.nf, 2 * self.nf, config.model.num_scales, resample='down', act=act,\n                               normalization=self.norm),\n      ConditionalResidualBlock(2 * self.nf, 2 * self.nf, config.model.num_scales, resample=None, act=act,\n                               normalization=self.norm)]\n    )\n\n    self.res3 = nn.ModuleList([\n      ConditionalResidualBlock(2 * self.nf, 2 * self.nf, config.model.num_scales, resample='down', act=act,\n                               normalization=self.norm, dilation=2),\n      ConditionalResidualBlock(2 * self.nf, 2 * self.nf, config.model.num_scales, resample=None, act=act,\n                               normalization=self.norm, dilation=2)]\n    )\n\n    if config.data.image_size == 28:\n      self.res4 = nn.ModuleList([\n        ConditionalResidualBlock(2 * self.nf, 2 * self.nf, config.model.num_scales, resample='down', act=act,\n                                 normalization=self.norm, adjust_padding=True, dilation=4),\n        ConditionalResidualBlock(2 * self.nf, 2 * self.nf, config.model.num_scales, resample=None, act=act,\n                                 normalization=self.norm, dilation=4)]\n      )\n    else:\n      self.res4 = nn.ModuleList([\n        ConditionalResidualBlock(2 * self.nf, 2 * self.nf, config.model.num_scales, resample='down', act=act,\n                                 normalization=self.norm, adjust_padding=False, dilation=4),\n        ConditionalResidualBlock(2 * self.nf, 2 * self.nf, config.model.num_scales, resample=None, act=act,\n                                 normalization=self.norm, dilation=4)]\n      )\n\n    self.refine1 = CondRefineBlock([2 * self.nf], 2 * self.nf, config.model.num_scales, self.norm, act=act, start=True)\n    self.refine2 = CondRefineBlock([2 * self.nf, 2 * self.nf], 2 * self.nf, config.model.num_scales, self.norm, act=act)\n    self.refine3 = CondRefineBlock([2 * self.nf, 2 * self.nf], self.nf, config.model.num_scales, self.norm, act=act)\n    self.refine4 = CondRefineBlock([self.nf, self.nf], self.nf, config.model.num_scales, self.norm, act=act, end=True)\n\n  def _compute_cond_module(self, module, x, y):\n    for m in module:\n      x = m(x, y)\n    return x\n\n  def forward(self, x, y):\n    if not self.centered:\n      h = 2 * x - 1.\n    else:\n      h = x\n\n    output = self.begin_conv(h)\n\n    layer1 = self._compute_cond_module(self.res1, output, y)\n    layer2 = self._compute_cond_module(self.res2, layer1, y)\n    layer3 = self._compute_cond_module(self.res3, layer2, y)\n    layer4 = self._compute_cond_module(self.res4, layer3, y)\n\n    ref1 = self.refine1([layer4], y, layer4.shape[2:])\n    ref2 = self.refine2([layer3, ref1], y, layer3.shape[2:])\n    ref3 = self.refine3([layer2, ref2], y, layer2.shape[2:])\n    output = self.refine4([layer1, ref3], y, layer1.shape[2:])\n\n    output = self.normalizer(output, y)\n    output = self.act(output)\n    output = self.end_conv(output)\n\n    return output\n\n\n@register_model(name='ncsnv2_128')\nclass NCSNv2_128(nn.Module):\n  \"\"\"NCSNv2 model architecture for 128px images.\"\"\"\n  def __init__(self, config):\n    super().__init__()\n    self.centered = config.data.centered\n    self.norm = get_normalization_Normalize(config)\n    self.nf = nf = config.model.nf\n    self.act = act = get_act_layers(config)\n    self.register_buffer('sigmas', torch.tensor(get_sigmas(config)))\n    self.config = config\n\n    self.begin_conv = nn.Conv2d(config.data.channels, nf, 3, stride=1, padding=1)\n    self.normalizer = self.norm(nf, config.model.num_scales)\n\n    self.end_conv = nn.Conv2d(nf, config.data.channels, 3, stride=1, padding=1)\n\n    self.res1 = nn.ModuleList([\n      ResidualBlock(self.nf, self.nf, resample=None, act=act,\n                    normalization=self.norm),\n      ResidualBlock(self.nf, self.nf, resample=None, act=act,\n                    normalization=self.norm)]\n    )\n\n    self.res2 = nn.ModuleList([\n      ResidualBlock(self.nf, 2 * self.nf, resample='down', act=act,\n                    normalization=self.norm),\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample=None, act=act,\n                    normalization=self.norm)]\n    )\n\n    self.res3 = nn.ModuleList([\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample='down', act=act,\n                    normalization=self.norm),\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample=None, act=act,\n                    normalization=self.norm)]\n    )\n\n    self.res4 = nn.ModuleList([\n      ResidualBlock(2 * self.nf, 4 * self.nf, resample='down', act=act,\n                    normalization=self.norm, dilation=2),\n      ResidualBlock(4 * self.nf, 4 * self.nf, resample=None, act=act,\n                    normalization=self.norm, dilation=2)]\n    )\n\n    self.res5 = nn.ModuleList([\n      ResidualBlock(4 * self.nf, 4 * self.nf, resample='down', act=act,\n                    normalization=self.norm, dilation=4),\n      ResidualBlock(4 * self.nf, 4 * self.nf, resample=None, act=act,\n                    normalization=self.norm, dilation=4)]\n    )\n\n    self.refine1 = RefineBlock([4 * self.nf], 4 * self.nf, act=act, start=True)\n    self.refine2 = RefineBlock([4 * self.nf, 4 * self.nf], 2 * self.nf, act=act)\n    self.refine3 = RefineBlock([2 * self.nf, 2 * self.nf], 2 * self.nf, act=act)\n    self.refine4 = RefineBlock([2 * self.nf, 2 * self.nf], self.nf, act=act)\n    self.refine5 = RefineBlock([self.nf, self.nf], self.nf, act=act, end=True)\n\n  def _compute_cond_module(self, module, x):\n    for m in module:\n      x = m(x)\n    return x\n\n  def forward(self, x, y):\n    if not self.centered:\n      h = 2 * x - 1.\n    else:\n      h = x\n\n    output = self.begin_conv(h)\n\n    layer1 = self._compute_cond_module(self.res1, output)\n    layer2 = self._compute_cond_module(self.res2, layer1)\n    layer3 = self._compute_cond_module(self.res3, layer2)\n    layer4 = self._compute_cond_module(self.res4, layer3)\n    layer5 = self._compute_cond_module(self.res5, layer4)\n\n    ref1 = self.refine1([layer5], layer5.shape[2:])\n    ref2 = self.refine2([layer4, ref1], layer4.shape[2:])\n    ref3 = self.refine3([layer3, ref2], layer3.shape[2:])\n    ref4 = self.refine4([layer2, ref3], layer2.shape[2:])\n    output = self.refine5([layer1, ref4], layer1.shape[2:])\n\n    output = self.normalizer(output)\n    output = self.act(output)\n    output = self.end_conv(output)\n\n    used_sigmas = self.sigmas[y].view(x.shape[0], *([1] * len(x.shape[1:])))\n\n    output = output / used_sigmas\n\n    return output\n\n\n@register_model(name='ncsnv2_256')\nclass NCSNv2_256(nn.Module):\n  \"\"\"NCSNv2 model architecture for 256px images.\"\"\"\n  def __init__(self, config):\n    super().__init__()\n    self.centered = config.data.centered\n    self.norm = get_normalization_Normalize(config)\n    self.nf = nf = config.model.nf\n    self.act = act = get_act_layers(config)\n    self.register_buffer('sigmas', torch.tensor(get_sigmas(config)))\n    self.config = config\n\n    self.begin_conv = nn.Conv2d(config.data.channels, nf, 3, stride=1, padding=1)\n    self.normalizer = self.norm(nf, config.model.num_scales)\n\n    self.end_conv = nn.Conv2d(nf, config.data.channels, 3, stride=1, padding=1)\n\n    self.res1 = nn.ModuleList([\n      ResidualBlock(self.nf, self.nf, resample=None, act=act,\n                    normalization=self.norm),\n      ResidualBlock(self.nf, self.nf, resample=None, act=act,\n                    normalization=self.norm)]\n    )\n\n    self.res2 = nn.ModuleList([\n      ResidualBlock(self.nf, 2 * self.nf, resample='down', act=act,\n                    normalization=self.norm),\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample=None, act=act,\n                    normalization=self.norm)]\n    )\n\n    self.res3 = nn.ModuleList([\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample='down', act=act,\n                    normalization=self.norm),\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample=None, act=act,\n                    normalization=self.norm)]\n    )\n\n    self.res31 = nn.ModuleList([\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample='down', act=act,\n                    normalization=self.norm),\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample=None, act=act,\n                    normalization=self.norm)]\n    )\n\n    self.res4 = nn.ModuleList([\n      ResidualBlock(2 * self.nf, 4 * self.nf, resample='down', act=act,\n                    normalization=self.norm, dilation=2),\n      ResidualBlock(4 * self.nf, 4 * self.nf, resample=None, act=act,\n                    normalization=self.norm, dilation=2)]\n    )\n\n    self.res5 = nn.ModuleList([\n      ResidualBlock(4 * self.nf, 4 * self.nf, resample='down', act=act,\n                    normalization=self.norm, dilation=4),\n      ResidualBlock(4 * self.nf, 4 * self.nf, resample=None, act=act,\n                    normalization=self.norm, dilation=4)]\n    )\n\n    self.refine1 = RefineBlock([4 * self.nf], 4 * self.nf, act=act, start=True)\n    self.refine2 = RefineBlock([4 * self.nf, 4 * self.nf], 2 * self.nf, act=act)\n    self.refine3 = RefineBlock([2 * self.nf, 2 * self.nf], 2 * self.nf, act=act)\n    self.refine31 = RefineBlock([2 * self.nf, 2 * self.nf], 2 * self.nf, act=act)\n    self.refine4 = RefineBlock([2 * self.nf, 2 * self.nf], self.nf, act=act)\n    self.refine5 = RefineBlock([self.nf, self.nf], self.nf, act=act, end=True)\n\n  def _compute_cond_module(self, module, x):\n    for m in module:\n      x = m(x)\n    return x\n\n  def forward(self, x, y):\n    if not self.centered:\n      h = 2 * x - 1.\n    else:\n      h = x\n\n    output = self.begin_conv(h)\n\n    layer1 = self._compute_cond_module(self.res1, output)\n    layer2 = self._compute_cond_module(self.res2, layer1)\n    layer3 = self._compute_cond_module(self.res3, layer2)\n    layer31 = self._compute_cond_module(self.res31, layer3)\n    layer4 = self._compute_cond_module(self.res4, layer31)\n    layer5 = self._compute_cond_module(self.res5, layer4)\n\n    ref1 = self.refine1([layer5], layer5.shape[2:])\n    ref2 = self.refine2([layer4, ref1], layer4.shape[2:])\n    ref31 = self.refine31([layer31, ref2], layer31.shape[2:])\n    ref3 = self.refine3([layer3, ref31], layer3.shape[2:])\n    ref4 = self.refine4([layer2, ref3], layer2.shape[2:])\n    output = self.refine5([layer1, ref4], layer1.shape[2:])\n\n    output = self.normalizer(output)\n    output = self.act(output)\n    output = self.end_conv(output)\n\n    used_sigmas = self.sigmas[y].view(x.shape[0], *([1] * len(x.shape[1:])))\n\n    output = output / used_sigmas\n\n    return output","metadata":{"id":"CqWqJRK1F9qU","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:32.233145Z","iopub.execute_input":"2023-12-17T20:04:32.233484Z","iopub.status.idle":"2023-12-17T20:04:32.311903Z","shell.execute_reply.started":"2023-12-17T20:04:32.233446Z","shell.execute_reply":"2023-12-17T20:04:32.311004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**NCSNpp**","metadata":{"id":"xYd883vJGAnz"}},{"cell_type":"code","source":"# @title\n# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: skip-file\n\n#mir from . import utils, layers, layerspp, normalization\nimport torch.nn as nn\nimport functools\nimport torch\nimport numpy as np\n\n#ResnetBlockDDPM = layerspp.ResnetBlockDDPMpp\nResnetBlockDDPM =ResnetBlockDDPMpp_layrepp\n\n#ResnetBlockBigGAN = layerspp.ResnetBlockBigGANpp\nResnetBlockBigGAN =  ResnetBlockBigGANpp\n#Combine = layerspp.Combine\nCombine=Combine_layerpp\n\n#conv3x3 = layerspp.conv3x3\n#fek konm az bala to khodesh monde va tarif shode va niyazi b tarif nadare\n\n\n#conv1x1 = layerspp.conv1x1\n\n\n\n#get_act = layers.get_act\nget_act = get_act_layers\n\n#get_normalization = normalization.get_normalization\nget_normalization =get_normalization_Normalize\n\n#default_initializer = layers.default_init\ndefault_initializer = default_init_layer\n\n\n@register_model(name='ncsnpp')\nclass NCSNpp(nn.Module):\n  \"\"\"NCSN++ model\"\"\"\n\n  def __init__(self, config):\n    super().__init__()\n    self.config = config\n    self.act = act = get_act(config)\n    self.register_buffer('sigmas', torch.tensor( get_sigmas(config)))\n\n    self.nf = nf = config.model.nf\n    ch_mult = config.model.ch_mult\n    self.num_res_blocks = num_res_blocks = config.model.num_res_blocks\n    self.attn_resolutions = attn_resolutions = config.model.attn_resolutions\n    dropout = config.model.dropout\n    resamp_with_conv = config.model.resamp_with_conv\n    self.num_resolutions = num_resolutions = len(ch_mult)\n    self.all_resolutions = all_resolutions = [config.data.image_size // (2 ** i) for i in range(num_resolutions)]\n\n    self.conditional = conditional = config.model.conditional  # noise-conditional\n    fir = config.model.fir\n    fir_kernel = config.model.fir_kernel\n    self.skip_rescale = skip_rescale = config.model.skip_rescale\n    self.resblock_type = resblock_type = config.model.resblock_type.lower()\n    self.progressive = progressive = config.model.progressive.lower()\n    self.progressive_input = progressive_input = config.model.progressive_input.lower()\n    self.embedding_type = embedding_type = config.model.embedding_type.lower()\n    init_scale = config.model.init_scale\n    assert progressive in ['none', 'output_skip', 'residual']\n    assert progressive_input in ['none', 'input_skip', 'residual']\n    assert embedding_type in ['fourier', 'positional']\n    combine_method = config.model.progressive_combine.lower()\n    combiner = functools.partial(Combine, method=combine_method)\n\n    modules = []\n    # timestep/noise_level embedding; only for continuous training\n    if embedding_type == 'fourier':\n      # Gaussian Fourier features embeddings.\n      assert config.training.continuous, \"Fourier features are only used for continuous training.\"\n\n      modules.append( GaussianFourierProjection(\n        embedding_size=nf, scale=config.model.fourier_scale\n      ))\n      embed_dim = 2 * nf\n\n    elif embedding_type == 'positional':\n      embed_dim = nf\n\n    else:\n      raise ValueError(f'embedding type {embedding_type} unknown.')\n\n    if conditional:\n      modules.append(nn.Linear(embed_dim, nf * 4))\n      modules[-1].weight.data = default_initializer()(modules[-1].weight.shape)\n      nn.init.zeros_(modules[-1].bias)\n      modules.append(nn.Linear(nf * 4, nf * 4))\n      modules[-1].weight.data = default_initializer()(modules[-1].weight.shape)\n      nn.init.zeros_(modules[-1].bias)\n\n    AttnBlock = functools.partial( AttnBlockpp,\n                                  init_scale=init_scale,\n                                  skip_rescale=skip_rescale)\n\n    Upsample = functools.partial(Upsample_layerpp,\n                                 with_conv=resamp_with_conv, fir=fir, fir_kernel=fir_kernel)\n\n    if progressive == 'output_skip':\n      self.pyramid_upsample = Upsample_layerpp(fir=fir, fir_kernel=fir_kernel, with_conv=False)\n    elif progressive == 'residual':\n      pyramid_upsample = functools.partial(Upsample_layerpp,\n                                           fir=fir, fir_kernel=fir_kernel, with_conv=True)\n\n    Downsample = functools.partial(Downsample_layerpp,\n                                   with_conv=resamp_with_conv, fir=fir, fir_kernel=fir_kernel)\n\n    if progressive_input == 'input_skip':\n      self.pyramid_downsample = Downsample_layerpp(fir=fir, fir_kernel=fir_kernel, with_conv=False)\n    elif progressive_input == 'residual':\n      pyramid_downsample = functools.partial(Downsample_layerpp,\n                                             fir=fir, fir_kernel=fir_kernel, with_conv=True)\n\n    if resblock_type == 'ddpm':\n      ResnetBlock = functools.partial(ResnetBlockDDPM,\n                                      act=act,\n                                      dropout=dropout,\n                                      init_scale=init_scale,\n                                      skip_rescale=skip_rescale,\n                                      temb_dim=nf * 4)\n\n    elif resblock_type == 'biggan':\n      ResnetBlock = functools.partial(ResnetBlockBigGAN,\n                                      act=act,\n                                      dropout=dropout,\n                                      fir=fir,\n                                      fir_kernel=fir_kernel,\n                                      init_scale=init_scale,\n                                      skip_rescale=skip_rescale,\n                                      temb_dim=nf * 4)\n\n    else:\n      raise ValueError(f'resblock type {resblock_type} unrecognized.')\n\n    # Downsampling block\n\n    channels = config.data.num_channels\n    if progressive_input != 'none':\n      input_pyramid_ch = channels\n\n    modules.append(conv3x3(channels, nf))\n    hs_c = [nf]\n\n    in_ch = nf\n    for i_level in range(num_resolutions):\n      # Residual blocks for this resolution\n      for i_block in range(num_res_blocks):\n        out_ch = nf * ch_mult[i_level]\n        modules.append(ResnetBlock(in_ch=in_ch, out_ch=out_ch))\n        in_ch = out_ch\n\n        if all_resolutions[i_level] in attn_resolutions:\n          modules.append(AttnBlock(channels=in_ch))\n        hs_c.append(in_ch)\n\n      if i_level != num_resolutions - 1:\n        if resblock_type == 'ddpm':\n          modules.append(Downsample(in_ch=in_ch))\n        else:\n          modules.append(ResnetBlock(down=True, in_ch=in_ch))\n\n        if progressive_input == 'input_skip':\n          modules.append(combiner(dim1=input_pyramid_ch, dim2=in_ch))\n          if combine_method == 'cat':\n            in_ch *= 2\n\n        elif progressive_input == 'residual':\n          modules.append(pyramid_downsample(in_ch=input_pyramid_ch, out_ch=in_ch))\n          input_pyramid_ch = in_ch\n\n        hs_c.append(in_ch)\n\n    in_ch = hs_c[-1]\n    modules.append(ResnetBlock(in_ch=in_ch))\n    modules.append(AttnBlock(channels=in_ch))\n    modules.append(ResnetBlock(in_ch=in_ch))\n\n    pyramid_ch = 0\n    # Upsampling block\n    for i_level in reversed(range(num_resolutions)):\n      for i_block in range(num_res_blocks + 1):\n        out_ch = nf * ch_mult[i_level]\n        modules.append(ResnetBlock(in_ch=in_ch + hs_c.pop(),\n                                   out_ch=out_ch))\n        in_ch = out_ch\n\n      if all_resolutions[i_level] in attn_resolutions:\n        modules.append(AttnBlock(channels=in_ch))\n\n      if progressive != 'none':\n        if i_level == num_resolutions - 1:\n          if progressive == 'output_skip':\n            modules.append(nn.GroupNorm(num_groups=min(in_ch // 4, 32),\n                                        num_channels=in_ch, eps=1e-6))\n            modules.append(conv3x3(in_ch, channels, init_scale=init_scale))\n            pyramid_ch = channels\n          elif progressive == 'residual':\n            modules.append(nn.GroupNorm(num_groups=min(in_ch // 4, 32),\n                                        num_channels=in_ch, eps=1e-6))\n            modules.append(conv3x3(in_ch, in_ch, bias=True))\n            pyramid_ch = in_ch\n          else:\n            raise ValueError(f'{progressive} is not a valid name.')\n        else:\n          if progressive == 'output_skip':\n            modules.append(nn.GroupNorm(num_groups=min(in_ch // 4, 32),\n                                        num_channels=in_ch, eps=1e-6))\n            modules.append(conv3x3(in_ch, channels, bias=True, init_scale=init_scale))\n            pyramid_ch = channels\n          elif progressive == 'residual':\n            modules.append(pyramid_upsample(in_ch=pyramid_ch, out_ch=in_ch))\n            pyramid_ch = in_ch\n          else:\n            raise ValueError(f'{progressive} is not a valid name')\n\n      if i_level != 0:\n        if resblock_type == 'ddpm':\n          modules.append(Upsample(in_ch=in_ch))\n        else:\n          modules.append(ResnetBlock(in_ch=in_ch, up=True))\n\n    assert not hs_c\n\n    if progressive != 'output_skip':\n      modules.append(nn.GroupNorm(num_groups=min(in_ch // 4, 32),\n                                  num_channels=in_ch, eps=1e-6))\n      modules.append(conv3x3(in_ch, channels, init_scale=init_scale))\n\n    self.all_modules = nn.ModuleList(modules)\n\n  def forward(self, x, time_cond):\n    # timestep/noise_level embedding; only for continuous training\n    modules = self.all_modules\n    m_idx = 0\n    if self.embedding_type == 'fourier':\n      # Gaussian Fourier features embeddings.\n      used_sigmas = time_cond\n      temb = modules[m_idx](torch.log(used_sigmas))\n      m_idx += 1\n\n    elif self.embedding_type == 'positional':\n      # Sinusoidal positional embeddings.\n      timesteps = time_cond\n      used_sigmas = self.sigmas[time_cond.long()]\n      temb = layers.get_timestep_embedding(timesteps, self.nf)\n\n    else:\n      raise ValueError(f'embedding type {self.embedding_type} unknown.')\n\n    if self.conditional:\n      temb = modules[m_idx](temb)\n      m_idx += 1\n      temb = modules[m_idx](self.act(temb))\n      m_idx += 1\n    else:\n      temb = None\n\n    if not self.config.data.centered:\n      # If input data is in [0, 1]\n      x = 2 * x - 1.\n\n    # Downsampling block\n    input_pyramid = None\n    if self.progressive_input != 'none':\n      input_pyramid = x\n\n    hs = [modules[m_idx](x)]\n    m_idx += 1\n    for i_level in range(self.num_resolutions):\n      # Residual blocks for this resolution\n      for i_block in range(self.num_res_blocks):\n        h = modules[m_idx](hs[-1], temb)\n        m_idx += 1\n        if h.shape[-1] in self.attn_resolutions:\n          h = modules[m_idx](h)\n          m_idx += 1\n\n        hs.append(h)\n\n      if i_level != self.num_resolutions - 1:\n        if self.resblock_type == 'ddpm':\n          h = modules[m_idx](hs[-1])\n          m_idx += 1\n        else:\n          h = modules[m_idx](hs[-1], temb)\n          m_idx += 1\n\n        if self.progressive_input == 'input_skip':\n          input_pyramid = self.pyramid_downsample(input_pyramid)\n          h = modules[m_idx](input_pyramid, h)\n          m_idx += 1\n\n        elif self.progressive_input == 'residual':\n          input_pyramid = modules[m_idx](input_pyramid)\n          m_idx += 1\n          if self.skip_rescale:\n            input_pyramid = (input_pyramid + h) / np.sqrt(2.)\n          else:\n            input_pyramid = input_pyramid + h\n          h = input_pyramid\n\n        hs.append(h)\n\n    h = hs[-1]\n    h = modules[m_idx](h, temb)\n    m_idx += 1\n    h = modules[m_idx](h)\n    m_idx += 1\n    h = modules[m_idx](h, temb)\n    m_idx += 1\n\n    pyramid = None\n\n    # Upsampling block\n    for i_level in reversed(range(self.num_resolutions)):\n      for i_block in range(self.num_res_blocks + 1):\n        h = modules[m_idx](torch.cat([h, hs.pop()], dim=1), temb)\n        m_idx += 1\n\n      if h.shape[-1] in self.attn_resolutions:\n        h = modules[m_idx](h)\n        m_idx += 1\n\n      if self.progressive != 'none':\n        if i_level == self.num_resolutions - 1:\n          if self.progressive == 'output_skip':\n            pyramid = self.act(modules[m_idx](h))\n            m_idx += 1\n            pyramid = modules[m_idx](pyramid)\n            m_idx += 1\n          elif self.progressive == 'residual':\n            pyramid = self.act(modules[m_idx](h))\n            m_idx += 1\n            pyramid = modules[m_idx](pyramid)\n            m_idx += 1\n          else:\n            raise ValueError(f'{self.progressive} is not a valid name.')\n        else:\n          if self.progressive == 'output_skip':\n            pyramid = self.pyramid_upsample(pyramid)\n            pyramid_h = self.act(modules[m_idx](h))\n            m_idx += 1\n            pyramid_h = modules[m_idx](pyramid_h)\n            m_idx += 1\n            pyramid = pyramid + pyramid_h\n          elif self.progressive == 'residual':\n            pyramid = modules[m_idx](pyramid)\n            m_idx += 1\n            if self.skip_rescale:\n              pyramid = (pyramid + h) / np.sqrt(2.)\n            else:\n              pyramid = pyramid + h\n            h = pyramid\n          else:\n            raise ValueError(f'{self.progressive} is not a valid name')\n\n      if i_level != 0:\n        if self.resblock_type == 'ddpm':\n          h = modules[m_idx](h)\n          m_idx += 1\n        else:\n          h = modules[m_idx](h, temb)\n          m_idx += 1\n\n    assert not hs\n\n    if self.progressive == 'output_skip':\n      h = pyramid\n    else:\n      h = self.act(modules[m_idx](h))\n      m_idx += 1\n      h = modules[m_idx](h)\n      m_idx += 1\n\n    assert m_idx == len(modules)\n    if self.config.model.scale_by_sigma:\n      used_sigmas = used_sigmas.reshape((x.shape[0], *([1] * len(x.shape[1:]))))\n      h = h / used_sigmas\n\n    return h","metadata":{"id":"GWkUw07AGDFB","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:32.313223Z","iopub.execute_input":"2023-12-17T20:04:32.313537Z","iopub.status.idle":"2023-12-17T20:04:32.369891Z","shell.execute_reply.started":"2023-12-17T20:04:32.313512Z","shell.execute_reply":"2023-12-17T20:04:32.369025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DDPM**","metadata":{"id":"7DTm6URnGGF3"}},{"cell_type":"code","source":"# @title\n# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: skip-file\n\"\"\"DDPM model.\nThis code is the pytorch equivalent of:\nhttps://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/models/unet.py\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport functools\n\n#from . import utils, layers, normalization\n\n#RefineBlock = layers.RefineBlock\nRefineBlock = RefineBlock\n\n\n\n#ResidualBlock = layers.RefineBlock\nResidualBlock = RefineBlock\n\n#ResnetBlockDDPM = layers.ResnetBlockDDPM\nResnetBlockDDPM = ResnetBlockDDPM_layers\n\n#Upsample = layers.Upsample\nUpsample =Upsample_layer\n\n#Downsample = layers.Downsample\nDownsample = Downsample_layers\n\n#conv3x3 = layers.ddpm_conv3x3\nconv3x3 = ddpm_conv3x3_layer\n\n\n#get_act = layers.get_act\nget_act =get_act_layers\n\n\n#get_normalization = normalization.get_normalization\nget_normalization = get_normalization_Normalize\n\n\n#default_initializer = layers.default_init\ndefault_initializer = default_init_layer\n\n@register_model(name='ddpm')\nclass DDPM(nn.Module):\n  def __init__(self, config):\n    super().__init__()\n    self.act = act = get_act(config)\n    self.register_buffer('sigmas', torch.tensor(get_sigmas(config)))\n\n    self.nf = nf = config.model.nf\n    ch_mult = config.model.ch_mult\n    self.num_res_blocks = num_res_blocks = config.model.num_res_blocks\n    self.attn_resolutions = attn_resolutions = config.model.attn_resolutions\n    dropout = config.model.dropout\n    resamp_with_conv = config.model.resamp_with_conv\n    self.num_resolutions = num_resolutions = len(ch_mult)\n    self.all_resolutions = all_resolutions = [config.data.image_size // (2 ** i) for i in range(num_resolutions)]\n\n    AttnBlock = functools.partial(layers.AttnBlock)\n    self.conditional = conditional = config.model.conditional\n    ResnetBlock = functools.partial(ResnetBlockDDPM, act=act, temb_dim=4 * nf, dropout=dropout)\n    if conditional:\n      # Condition on noise levels.\n      modules = [nn.Linear(nf, nf * 4)]\n      modules[0].weight.data = default_initializer()(modules[0].weight.data.shape)\n      nn.init.zeros_(modules[0].bias)\n      modules.append(nn.Linear(nf * 4, nf * 4))\n      modules[1].weight.data = default_initializer()(modules[1].weight.data.shape)\n      nn.init.zeros_(modules[1].bias)\n\n    self.centered = config.data.centered\n    channels = config.data.num_channels\n\n    # Downsampling block\n    modules.append(conv3x3(channels, nf))\n    hs_c = [nf]\n    in_ch = nf\n    for i_level in range(num_resolutions):\n      # Residual blocks for this resolution\n      for i_block in range(num_res_blocks):\n        out_ch = nf * ch_mult[i_level]\n        modules.append(ResnetBlock(in_ch=in_ch, out_ch=out_ch))\n        in_ch = out_ch\n        if all_resolutions[i_level] in attn_resolutions:\n          modules.append(AttnBlock(channels=in_ch))\n        hs_c.append(in_ch)\n      if i_level != num_resolutions - 1:\n        modules.append(Downsample(channels=in_ch, with_conv=resamp_with_conv))\n        hs_c.append(in_ch)\n\n    in_ch = hs_c[-1]\n    modules.append(ResnetBlock(in_ch=in_ch))\n    modules.append(AttnBlock(channels=in_ch))\n    modules.append(ResnetBlock(in_ch=in_ch))\n\n    # Upsampling block\n    for i_level in reversed(range(num_resolutions)):\n      for i_block in range(num_res_blocks + 1):\n        out_ch = nf * ch_mult[i_level]\n        modules.append(ResnetBlock(in_ch=in_ch + hs_c.pop(), out_ch=out_ch))\n        in_ch = out_ch\n      if all_resolutions[i_level] in attn_resolutions:\n        modules.append(AttnBlock(channels=in_ch))\n      if i_level != 0:\n        modules.append(Upsample(channels=in_ch, with_conv=resamp_with_conv))\n\n    assert not hs_c\n    modules.append(nn.GroupNorm(num_channels=in_ch, num_groups=32, eps=1e-6))\n    modules.append(conv3x3(in_ch, channels, init_scale=0.))\n    self.all_modules = nn.ModuleList(modules)\n\n    self.scale_by_sigma = config.model.scale_by_sigma\n\n  def forward(self, x, labels):\n    modules = self.all_modules\n    m_idx = 0\n    if self.conditional:\n      # timestep/scale embedding\n      timesteps = labels\n      temb = layers.get_timestep_embedding(timesteps, self.nf)\n      temb = modules[m_idx](temb)\n      m_idx += 1\n      temb = modules[m_idx](self.act(temb))\n      m_idx += 1\n    else:\n      temb = None\n\n    if self.centered:\n      # Input is in [-1, 1]\n      h = x\n    else:\n      # Input is in [0, 1]\n      h = 2 * x - 1.\n\n    # Downsampling block\n    hs = [modules[m_idx](h)]\n    m_idx += 1\n    for i_level in range(self.num_resolutions):\n      # Residual blocks for this resolution\n      for i_block in range(self.num_res_blocks):\n        h = modules[m_idx](hs[-1], temb)\n        m_idx += 1\n        if h.shape[-1] in self.attn_resolutions:\n          h = modules[m_idx](h)\n          m_idx += 1\n        hs.append(h)\n      if i_level != self.num_resolutions - 1:\n        hs.append(modules[m_idx](hs[-1]))\n        m_idx += 1\n\n    h = hs[-1]\n    h = modules[m_idx](h, temb)\n    m_idx += 1\n    h = modules[m_idx](h)\n    m_idx += 1\n    h = modules[m_idx](h, temb)\n    m_idx += 1\n\n    # Upsampling block\n    for i_level in reversed(range(self.num_resolutions)):\n      for i_block in range(self.num_res_blocks + 1):\n        h = modules[m_idx](torch.cat([h, hs.pop()], dim=1), temb)\n        m_idx += 1\n      if h.shape[-1] in self.attn_resolutions:\n        h = modules[m_idx](h)\n        m_idx += 1\n      if i_level != 0:\n        h = modules[m_idx](h)\n        m_idx += 1\n\n    assert not hs\n    h = self.act(modules[m_idx](h))\n    m_idx += 1\n    h = modules[m_idx](h)\n    m_idx += 1\n    assert m_idx == len(modules)\n\n    if self.scale_by_sigma:\n      # Divide the output by sigmas. Useful for training with the NCSN loss.\n      # The DDPM loss scales the network output by sigma in the loss function,\n      # so no need of doing it here.\n      used_sigmas = self.sigmas[labels, None, None, None]\n      h = h / used_sigmas\n\n    return h","metadata":{"id":"tKhpH5CcGHgf","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:32.371425Z","iopub.execute_input":"2023-12-17T20:04:32.371805Z","iopub.status.idle":"2023-12-17T20:04:32.401124Z","shell.execute_reply.started":"2023-12-17T20:04:32.371778Z","shell.execute_reply":"2023-12-17T20:04:32.400170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Likelihood**","metadata":{"id":"iDWyt2TVGN_w"}},{"cell_type":"code","source":"# @title\n# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: skip-file\n# pytype: skip-file\n\"\"\"Various sampling methods.\"\"\"\n\nimport torch\nimport numpy as np\nfrom scipy import integrate\n#from models import utils as mutils\n\n\ndef get_div_fn(fn):\n  \"\"\"Create the divergence function of `fn` using the Hutchinson-Skilling trace estimator.\"\"\"\n\n  def div_fn(x, t, eps):\n    with torch.enable_grad():\n      x.requires_grad_(True)\n      fn_eps = torch.sum(fn(x, t) * eps)\n      grad_fn_eps = torch.autograd.grad(fn_eps, x)[0]\n    x.requires_grad_(False)\n    return torch.sum(grad_fn_eps * eps, dim=tuple(range(1, len(x.shape))))\n\n  return div_fn\n\n\ndef get_likelihood_fn(sde, inverse_scaler, hutchinson_type='Rademacher',\n                      rtol=1e-5, atol=1e-5, method='RK45', eps=1e-5):\n  \"\"\"Create a function to compute the unbiased log-likelihood estimate of a given data point.\n  Args:\n    sde: A `sde_lib.SDE` object that represents the forward SDE.\n    inverse_scaler: The inverse data normalizer.\n    hutchinson_type: \"Rademacher\" or \"Gaussian\". The type of noise for Hutchinson-Skilling trace estimator.\n    rtol: A `float` number. The relative tolerance level of the black-box ODE solver.\n    atol: A `float` number. The absolute tolerance level of the black-box ODE solver.\n    method: A `str`. The algorithm for the black-box ODE solver.\n      See documentation for `scipy.integrate.solve_ivp`.\n    eps: A `float` number. The probability flow ODE is integrated to `eps` for numerical stability.\n  Returns:\n    A function that a batch of data points and returns the log-likelihoods in bits/dim,\n      the latent code, and the number of function evaluations cost by computation.\n  \"\"\"\n\n  def drift_fn(model, x, t):\n    \"\"\"The drift function of the reverse-time SDE.\"\"\"\n    score_fn = get_score_fn(sde, model, train=False, continuous=True)\n    # Probability flow ODE is a special case of Reverse SDE\n    rsde = sde.reverse(score_fn, probability_flow=True)\n    return rsde.sde(x, t)[0]\n\n  def div_fn(model, x, t, noise):\n    return get_div_fn(lambda xx, tt: drift_fn(model, xx, tt))(x, t, noise)\n\n  def likelihood_fn(model, data):\n    \"\"\"Compute an unbiased estimate to the log-likelihood in bits/dim.\n    Args:\n      model: A score model.\n      data: A PyTorch tensor.\n    Returns:\n      bpd: A PyTorch tensor of shape [batch size]. The log-likelihoods on `data` in bits/dim.\n      z: A PyTorch tensor of the same shape as `data`. The latent representation of `data` under the\n        probability flow ODE.\n      nfe: An integer. The number of function evaluations used for running the black-box ODE solver.\n    \"\"\"\n    with torch.no_grad():\n      shape = data.shape\n      if hutchinson_type == 'Gaussian':\n        epsilon = torch.randn_like(data)\n      elif hutchinson_type == 'Rademacher':\n        epsilon = torch.randint_like(data, low=0, high=2).float() * 2 - 1.\n      else:\n        raise NotImplementedError(f\"Hutchinson type {hutchinson_type} unknown.\")\n\n      def ode_func(t, x):\n        sample = from_flattened_numpy(x[:-shape[0]], shape).to(data.device).type(torch.float32)\n        vec_t = torch.ones(sample.shape[0], device=sample.device) * t\n        drift = to_flattened_numpy(drift_fn(model, sample, vec_t))\n        logp_grad =to_flattened_numpy(div_fn(model, sample, vec_t, epsilon))\n        return np.concatenate([drift, logp_grad], axis=0)\n\n      init = np.concatenate([to_flattened_numpy(data), np.zeros((shape[0],))], axis=0)\n      solution = integrate.solve_ivp(ode_func, (eps, sde.T), init, rtol=rtol, atol=atol, method=method)\n      nfe = solution.nfev\n      zp = solution.y[:, -1]\n      z = from_flattened_numpy(zp[:-shape[0]], shape).to(data.device).type(torch.float32)\n      delta_logp = from_flattened_numpy(zp[-shape[0]:], (shape[0],)).to(data.device).type(torch.float32)\n      prior_logp = sde.prior_logp(z)\n      bpd = -(prior_logp + delta_logp) / np.log(2)\n      N = np.prod(shape[1:])\n      bpd = bpd / N\n      # A hack to convert log-likelihoods to bits/dim\n      offset = 7. - inverse_scaler(-1.)\n      bpd = bpd + offset\n      return bpd, z, nfe\n\n  return likelihood_fn","metadata":{"id":"2BUrBYbQGP2U","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:32.402653Z","iopub.execute_input":"2023-12-17T20:04:32.403251Z","iopub.status.idle":"2023-12-17T20:04:32.672039Z","shell.execute_reply.started":"2023-12-17T20:04:32.403220Z","shell.execute_reply":"2023-12-17T20:04:32.671264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SDE_lib**","metadata":{"id":"G9OYfJJuGSnj"}},{"cell_type":"code","source":"# @title\n\"\"\"Abstract SDE classes, Reverse SDE, and VE/VP SDEs.\"\"\"\nimport abc\nimport torch\nimport numpy as np\n\n\nclass SDE(abc.ABC):\n  \"\"\"SDE abstract class. Functions are designed for a mini-batch of inputs.\"\"\"\n\n  def __init__(self, N):\n    \"\"\"Construct an SDE.\n    Args:\n      N: number of discretization time steps.\n    \"\"\"\n    super().__init__()\n    self.N = N\n\n  @property\n  @abc.abstractmethod\n  def T(self):\n    \"\"\"End time of the SDE.\"\"\"\n    pass\n\n  @abc.abstractmethod\n  def sde(self, x, t):\n    pass\n\n  @abc.abstractmethod\n  def marginal_prob(self, x, t):\n    \"\"\"Parameters to determine the marginal distribution of the SDE, $p_t(x)$.\"\"\"\n    pass\n\n  @abc.abstractmethod\n  def prior_sampling(self, shape):\n    \"\"\"Generate one sample from the prior distribution, $p_T(x)$.\"\"\"\n    pass\n\n  @abc.abstractmethod\n  def prior_logp(self, z):\n    \"\"\"Compute log-density of the prior distribution.\n    Useful for computing the log-likelihood via probability flow ODE.\n    Args:\n      z: latent code\n    Returns:\n      log probability density\n    \"\"\"\n    pass\n\n  def discretize(self, x, t):\n    \"\"\"Discretize the SDE in the form: x_{i+1} = x_i + f_i(x_i) + G_i z_i.\n    Useful for reverse diffusion sampling and probabiliy flow sampling.\n    Defaults to Euler-Maruyama discretization.\n    Args:\n      x: a torch tensor\n      t: a torch float representing the time step (from 0 to `self.T`)\n    Returns:\n      f, G\n    \"\"\"\n    dt = 1 / self.N\n    drift, diffusion = self.sde(x, t)\n    f = drift * dt\n    G = diffusion * torch.sqrt(torch.tensor(dt, device=t.device))\n    return f, G\n\n  def reverse(self, score_fn, probability_flow=False):\n    \"\"\"Create the reverse-time SDE/ODE.\n    Args:\n      score_fn: A time-dependent score-based model that takes x and t and returns the score.\n      probability_flow: If `True`, create the reverse-time ODE used for probability flow sampling.\n    \"\"\"\n    N = self.N\n    T = self.T\n    sde_fn = self.sde\n    discretize_fn = self.discretize\n\n    # Build the class for reverse-time SDE.\n    class RSDE(self.__class__):\n      def __init__(self):\n        self.N = N\n        self.probability_flow = probability_flow\n\n      @property\n      def T(self):\n        return T\n\n      def sde(self, x, t):\n        \"\"\"Create the drift and diffusion functions for the reverse SDE/ODE.\"\"\"\n        drift, diffusion = sde_fn(x, t)\n        score = score_fn(x, t)\n        drift = drift - diffusion[:, None, None, None] ** 2 * score * (0.5 if self.probability_flow else 1.)\n        # Set the diffusion function to zero for ODEs.\n        diffusion = 0. if self.probability_flow else diffusion\n\n        return drift, diffusion\n\n      def discretize(self, x, t):\n        \"\"\"Create discretized iteration rules for the reverse diffusion sampler.\"\"\"\n        f, G = discretize_fn(x, t)\n        rev_f = f - G[:, None, None, None] ** 2 * score_fn(x, t) * (0.5 if self.probability_flow else 1.)\n        rev_G = torch.zeros_like(G) if self.probability_flow else G\n        return rev_f, rev_G\n\n    return RSDE()\n\n\nclass VPSDE(SDE):\n  def __init__(self, beta_min=0.1, beta_max=20, N=1000):\n    \"\"\"Construct a Variance Preserving SDE.\n    Args:\n      beta_min: value of beta(0)\n      beta_max: value of beta(1)\n      N: number of discretization steps\n    \"\"\"\n    super().__init__(N)\n    self.beta_0 = beta_min\n    self.beta_1 = beta_max\n    self.N = N\n    self.discrete_betas = torch.linspace(beta_min / N, beta_max / N, N)\n    self.alphas = 1. - self.discrete_betas\n    self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n    self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n    self.sqrt_1m_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n\n  @property\n  def T(self):\n    return 1\n\n  def sde(self, x, t):\n    beta_t = self.beta_0 + t * (self.beta_1 - self.beta_0)\n    drift = -0.5 * beta_t[:, None, None, None] * x\n    diffusion = torch.sqrt(beta_t)\n\n    return drift, diffusion\n\n  def marginal_prob(self, x, t):\n    log_mean_coeff = -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n    mean = torch.exp(log_mean_coeff[:, None, None, None]) * x\n    std = torch.sqrt(1. - torch.exp(2. * log_mean_coeff))\n    return mean, std\n\n  def prior_sampling(self, shape):\n    return torch.randn(*shape)\n\n  def prior_logp(self, z):\n    shape = z.shape\n    N = np.prod(shape[1:])\n    logps = -N / 2. * np.log(2 * np.pi) - torch.sum(z ** 2, dim=(1, 2, 3)) / 2.\n    return logps\n\n  def discretize(self, x, t):\n    \"\"\"DDPM discretization.\"\"\"\n    timestep = (t * (self.N - 1) / self.T).long()\n    beta = self.discrete_betas.to(x.device)[timestep]\n    alpha = self.alphas.to(x.device)[timestep]\n    sqrt_beta = torch.sqrt(beta)\n    f = torch.sqrt(alpha)[:, None, None, None] * x - x\n    G = sqrt_beta\n    return f, G\n\n\nclass subVPSDE(SDE):\n  def __init__(self, beta_min=0.1, beta_max=20, N=1000):\n    \"\"\"Construct the sub-VP SDE that excels at likelihoods.\n    Args:\n      beta_min: value of beta(0)\n      beta_max: value of beta(1)\n      N: number of discretization steps\n    \"\"\"\n    super().__init__(N)\n    self.beta_0 = beta_min\n    self.beta_1 = beta_max\n    self.N = N\n\n  @property\n  def T(self):\n    return 1\n\n  def sde(self, x, t):\n    beta_t = self.beta_0 + t * (self.beta_1 - self.beta_0)\n    drift = -0.5 * beta_t[:, None, None, None] * x\n    discount = 1. - torch.exp(-2 * self.beta_0 * t - (self.beta_1 - self.beta_0) * t ** 2)\n    diffusion = torch.sqrt(beta_t * discount)\n    return drift, diffusion\n\n  def marginal_prob(self, x, t):\n    log_mean_coeff = -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n    mean = torch.exp(log_mean_coeff)[:, None, None, None] * x\n    std = 1 - torch.exp(2. * log_mean_coeff)\n    return mean, std\n\n  def prior_sampling(self, shape):\n    return torch.randn(*shape)\n\n  def prior_logp(self, z):\n    shape = z.shape\n    N = np.prod(shape[1:])\n    return -N / 2. * np.log(2 * np.pi) - torch.sum(z ** 2, dim=(1, 2, 3)) / 2.\n\n\nclass VESDE(SDE):\n  def __init__(self, sigma_min=0.01, sigma_max=50, N=1000):\n    \"\"\"Construct a Variance Exploding SDE.\n    Args:\n      sigma_min: smallest sigma.\n      sigma_max: largest sigma.\n      N: number of discretization steps\n    \"\"\"\n    super().__init__(N)\n    self.sigma_min = sigma_min\n    self.sigma_max = sigma_max\n    self.discrete_sigmas = torch.exp(torch.linspace(np.log(self.sigma_min), np.log(self.sigma_max), N))\n    self.N = N\n\n  @property\n  def T(self):\n    return 1\n\n  def sde(self, x, t):\n    sigma = self.sigma_min * (self.sigma_max / self.sigma_min) ** t\n    drift = torch.zeros_like(x)\n    diffusion = sigma * torch.sqrt(torch.tensor(2 * (np.log(self.sigma_max) - np.log(self.sigma_min)),\n                                                device=t.device))\n    return drift, diffusion\n\n  def marginal_prob(self, x, t):\n    std = self.sigma_min * (self.sigma_max / self.sigma_min) ** t\n    mean = x\n    return mean, std\n\n  def prior_sampling(self, shape):\n    return torch.randn(*shape) * self.sigma_max\n\n  def prior_logp(self, z):\n    shape = z.shape\n    N = np.prod(shape[1:])\n    return -N / 2. * np.log(2 * np.pi * self.sigma_max ** 2) - torch.sum(z ** 2, dim=(1, 2, 3)) / (2 * self.sigma_max ** 2)\n\n  def discretize(self, x, t):\n    \"\"\"SMLD(NCSN) discretization.\"\"\"\n    timestep = (t * (self.N - 1) / self.T).long()\n    sigma = self.discrete_sigmas.to(t.device)[timestep]\n    adjacent_sigma = torch.where(timestep == 0, torch.zeros_like(t),\n                                 self.discrete_sigmas.to(t.device)[timestep - 1])\n    f = torch.zeros_like(x)\n    G = torch.sqrt(sigma ** 2 - adjacent_sigma ** 2)\n    return f, G","metadata":{"id":"UzXNep76GVPu","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:32.673547Z","iopub.execute_input":"2023-12-17T20:04:32.674089Z","iopub.status.idle":"2023-12-17T20:04:32.714291Z","shell.execute_reply.started":"2023-12-17T20:04:32.674050Z","shell.execute_reply":"2023-12-17T20:04:32.713331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Losses**","metadata":{"id":"mvOYQsGSGXTK"}},{"cell_type":"code","source":"# @title\n# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"All functions related to loss computation and optimization.\n\"\"\"\n\nimport torch\nimport torch.optim as optim\nimport numpy as np\n#from models import utils as mutils\n#from sde_lib import VESDE, VPSDE\n\n\ndef get_optimizer(config, params):\n  \"\"\"Returns a flax optimizer object based on `config`.\"\"\"\n  if config.optim.optimizer == 'Adam':\n    optimizer = optim.Adam(params, lr=config.optim.lr, betas=(config.optim.beta1, 0.999), eps=config.optim.eps,\n                           weight_decay=config.optim.weight_decay)\n  else:\n    raise NotImplementedError(\n      f'Optimizer {config.optim.optimizer} not supported yet!')\n\n  return optimizer\n\n\ndef optimization_manager(config):\n  \"\"\"Returns an optimize_fn based on `config`.\"\"\"\n\n  def optimize_fn(optimizer, params, step, lr=config.optim.lr,\n                  warmup=config.optim.warmup,\n                  grad_clip=config.optim.grad_clip):\n    \"\"\"Optimizes with warmup and gradient clipping (disabled if negative).\"\"\"\n    if warmup > 0:\n      for g in optimizer.param_groups:\n        g['lr'] = lr * np.minimum(step / warmup, 1.0)\n    if grad_clip >= 0:\n      torch.nn.utils.clip_grad_norm_(params, max_norm=grad_clip)\n    optimizer.step()\n\n  return optimize_fn\n\n\ndef get_sde_loss_fn(sde, train, reduce_mean=True, continuous=True, likelihood_weighting=True, eps=1e-5):\n  \"\"\"Create a loss function for training with arbirary SDEs.\n  Args:\n    sde: An `sde_lib.SDE` object that represents the forward SDE.\n    train: `True` for training loss and `False` for evaluation loss.\n    reduce_mean: If `True`, average the loss across data dimensions. Otherwise sum the loss across data dimensions.\n    continuous: `True` indicates that the model is defined to take continuous time steps. Otherwise it requires\n      ad-hoc interpolation to take continuous time steps.\n    likelihood_weighting: If `True`, weight the mixture of score matching losses\n      according to https://arxiv.org/abs/2101.09258; otherwise use the weighting recommended in our paper.\n    eps: A `float` number. The smallest time step to sample from.\n  Returns:\n    A loss function.\n  \"\"\"\n  reduce_op = torch.mean if reduce_mean else lambda *args, **kwargs: 0.5 * torch.sum(*args, **kwargs)\n\n  def loss_fn(model, batch):\n    \"\"\"Compute the loss function.\n    Args:\n      model: A score model.\n      batch: A mini-batch of training data.\n    Returns:\n      loss: A scalar that represents the average loss value across the mini-batch.\n    \"\"\"\n    score_fn = get_score_fn(sde, model, train=train, continuous=continuous)\n    t = torch.rand(batch.shape[0], device=batch.device) * (sde.T - eps) + eps\n    z = torch.randn_like(batch)\n    mean, std = sde.marginal_prob(batch, t)\n    perturbed_data = mean + std[:, None, None, None] * z\n    score = score_fn(perturbed_data, t)\n\n    if not likelihood_weighting:\n      losses = torch.square(score * std[:, None, None, None] + z)\n      losses = reduce_op(losses.reshape(losses.shape[0], -1), dim=-1)\n    else:\n      g2 = sde.sde(torch.zeros_like(batch), t)[1] ** 2\n      losses = torch.square(score + z / std[:, None, None, None])\n      losses = reduce_op(losses.reshape(losses.shape[0], -1), dim=-1) * g2\n\n    loss = torch.mean(losses)\n    return loss\n\n  return loss_fn\n\n\ndef get_smld_loss_fn(vesde, train, reduce_mean=False):\n  \"\"\"Legacy code to reproduce previous results on SMLD(NCSN). Not recommended for new work.\"\"\"\n  assert isinstance(vesde, VESDE), \"SMLD training only works for VESDEs.\"\n\n  # Previous SMLD models assume descending sigmas\n  smld_sigma_array = torch.flip(vesde.discrete_sigmas, dims=(0,))\n  reduce_op = torch.mean if reduce_mean else lambda *args, **kwargs: 0.5 * torch.sum(*args, **kwargs)\n\n  def loss_fn(model, batch):\n    model_fn = get_model_fn(model, train=train)\n    labels = torch.randint(0, vesde.N, (batch.shape[0],), device=batch.device)\n    sigmas = smld_sigma_array.to(batch.device)[labels]\n    noise = torch.randn_like(batch) * sigmas[:, None, None, None]\n    perturbed_data = noise + batch\n    score = model_fn(perturbed_data, labels)\n    target = -noise / (sigmas ** 2)[:, None, None, None]\n    losses = torch.square(score - target)\n    losses = reduce_op(losses.reshape(losses.shape[0], -1), dim=-1) * sigmas ** 2\n    loss = torch.mean(losses)\n    return loss\n\n  return loss_fn\n\n\ndef get_ddpm_loss_fn(vpsde, train, reduce_mean=True):\n  \"\"\"Legacy code to reproduce previous results on DDPM. Not recommended for new work.\"\"\"\n  assert isinstance(vpsde, VPSDE), \"DDPM training only works for VPSDEs.\"\n\n  reduce_op = torch.mean if reduce_mean else lambda *args, **kwargs: 0.5 * torch.sum(*args, **kwargs)\n\n  def loss_fn(model, batch):\n    model_fn =  get_model_fn(model, train=train)\n    labels = torch.randint(0, vpsde.N, (batch.shape[0],), device=batch.device)\n    sqrt_alphas_cumprod = vpsde.sqrt_alphas_cumprod.to(batch.device)\n    sqrt_1m_alphas_cumprod = vpsde.sqrt_1m_alphas_cumprod.to(batch.device)\n    noise = torch.randn_like(batch)\n    perturbed_data = sqrt_alphas_cumprod[labels, None, None, None] * batch + \\\n                     sqrt_1m_alphas_cumprod[labels, None, None, None] * noise\n    score = model_fn(perturbed_data, labels)\n    losses = torch.square(score - noise)\n    losses = reduce_op(losses.reshape(losses.shape[0], -1), dim=-1)\n    loss = torch.mean(losses)\n    return loss\n\n  return loss_fn\n\n\ndef get_step_fn(sde, train, optimize_fn=None, reduce_mean=False, continuous=True, likelihood_weighting=False):\n  \"\"\"Create a one-step training/evaluation function.\n  Args:\n    sde: An `sde_lib.SDE` object that represents the forward SDE.\n    optimize_fn: An optimization function.\n    reduce_mean: If `True`, average the loss across data dimensions. Otherwise sum the loss across data dimensions.\n    continuous: `True` indicates that the model is defined to take continuous time steps.\n    likelihood_weighting: If `True`, weight the mixture of score matching losses according to\n      https://arxiv.org/abs/2101.09258; otherwise use the weighting recommended by our paper.\n  Returns:\n    A one-step function for training or evaluation.\n  \"\"\"\n  if continuous:\n    print(1)\n    loss_fn = get_sde_loss_fn(sde, train, reduce_mean=reduce_mean,\n                              continuous=True, likelihood_weighting=likelihood_weighting)\n  else:\n    print(2)\n    assert not likelihood_weighting, \"Likelihood weighting is not supported for original SMLD/DDPM training.\"\n    if isinstance(sde, VESDE):\n      print(3)\n      loss_fn = get_smld_loss_fn(sde, train, reduce_mean=reduce_mean)\n    elif isinstance(sde, VPSDE):\n      print(4)\n      loss_fn = get_ddpm_loss_fn(sde, train, reduce_mean=reduce_mean)\n    else:\n      raise ValueError(f\"Discrete training for {sde.__class__.__name__} is not recommended.\")\n    print(5)\n\n  def step_fn(state, batch):\n    \"\"\"Running one step of training or evaluation.\n    This function will undergo `jax.lax.scan` so that multiple steps can be pmapped and jit-compiled together\n    for faster execution.\n    Args:\n      state: A dictionary of training information, containing the score model, optimizer,\n       EMA status, and number of optimization steps.\n      batch: A mini-batch of training/evaluation data.\n    Returns:\n      loss: The average loss value of this state.\n    \"\"\"\n#     print(6)\n    model = state['model']\n    if train:\n#       print(8)\n      optimizer = state['optimizer']\n      optimizer.zero_grad()\n      loss = loss_fn(model, batch)\n      loss.backward()\n      optimize_fn(optimizer, model.parameters(), step=state['step'])\n      state['step'] += 1\n      state['ema'].update(model.parameters())\n    else:\n      with torch.no_grad():\n#         print(9)\n        ema = state['ema']\n        ema.store(model.parameters())\n        ema.copy_to(model.parameters())\n        loss = loss_fn(model, batch)\n        ema.restore(model.parameters())\n#     print(7)\n\n    return loss\n\n    print(8)\n  return step_fn","metadata":{"id":"SI8_73JiGYnR","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:32.716012Z","iopub.execute_input":"2023-12-17T20:04:32.716360Z","iopub.status.idle":"2023-12-17T20:04:32.748068Z","shell.execute_reply.started":"2023-12-17T20:04:32.716329Z","shell.execute_reply":"2023-12-17T20:04:32.746976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sampling**","metadata":{"id":"sSL4T9T-GbQK"}},{"cell_type":"code","source":"# @title\n# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# Seezzz the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: skip-file\n# pytype: skip-file\n\"\"\"Various sampling methods.\"\"\"\nimport functools\n\nimport torch\nimport numpy as np\nimport abc\n\n#from models.utils import from_flattened_numpy, to_flattened_numpy, get_score_fn\nfrom scipy import integrate\n#import sde_lib\n#from models import utils as mutils\n\n_CORRECTORS = {}\n_PREDICTORS = {}\n\n\ndef register_predictor(cls=None, *, name=None):\n  \"\"\"A decorator for registering predictor classes.\"\"\"\n\n  def _register(cls):\n    if name is None:\n      local_name = cls.__name__\n    else:\n      local_name = name\n    if local_name in _PREDICTORS:\n      raise ValueError(f'Already registered model with name: {local_name}')\n    _PREDICTORS[local_name] = cls\n    return cls\n\n  if cls is None:\n    return _register\n  else:\n    return _register(cls)\n\n\ndef register_corrector(cls=None, *, name=None):\n  \"\"\"A decorator for registering corrector classes.\"\"\"\n\n  def _register(cls):\n    if name is None:\n      local_name = cls.__name__\n    else:\n      local_name = name\n    if local_name in _CORRECTORS:\n      raise ValueError(f'Already registered model with name: {local_name}')\n    _CORRECTORS[local_name] = cls\n    return cls\n\n  if cls is None:\n    return _register\n  else:\n    return _register(cls)\n\n\ndef get_predictor(name):\n  return _PREDICTORS[name]\n\n\ndef get_corrector(name):\n  return _CORRECTORS[name]\n\n\ndef get_sampling_fn(config, sde, shape, inverse_scaler, eps):\n  \"\"\"Create a sampling function.\n  Args:\n    config: A `ml_collections.ConfigDict` object that contains all configuration information.\n    sde: A `sde_lib.SDE` object that represents the forward SDE.\n    shape: A sequence of integers representing the expected shape of a single sample.\n    inverse_scaler: The inverse data normalizer function.\n    eps: A `float` number. The reverse-time SDE is only integrated to `eps` for numerical stability.\n  Returns:\n    A function that takes random states and a replicated training state and outputs samples with the\n      trailing dimensions matching `shape`.\n  \"\"\"\n\n  sampler_name = config.sampling.method\n  # Probability flow ODE sampling with black-box ODE solvers\n  if sampler_name.lower() == 'ode':\n    sampling_fn = get_ode_sampler(sde=sde,\n                                  shape=shape,\n                                  inverse_scaler=inverse_scaler,\n                                  denoise=config.sampling.noise_removal,\n                                  eps=eps,\n                                  device=config.device)\n  # Predictor-Corrector sampling. Predictor-only and Corrector-only samplers are special cases.\n  elif sampler_name.lower() == 'pc':\n    predictor = get_predictor(config.sampling.predictor.lower())\n    corrector = get_corrector(config.sampling.corrector.lower())\n    sampling_fn = get_pc_sampler(sde=sde,\n                                 shape=shape,\n                                 predictor=predictor,\n                                 corrector=corrector,\n                                 inverse_scaler=inverse_scaler,\n                                 snr=config.sampling.snr,\n                                 n_steps=config.sampling.n_steps_each,\n                                 probability_flow=config.sampling.probability_flow,\n                                 continuous=config.training.continuous,\n                                 denoise=config.sampling.noise_removal,\n                                 eps=eps,\n                                 device=config.device)\n  else:\n    raise ValueError(f\"Sampler name {sampler_name} unknown.\")\n\n  return sampling_fn\n\n\nclass Predictor(abc.ABC):\n  \"\"\"The abstract class for a predictor algorithm.\"\"\"\n\n  def __init__(self, sde, score_fn, probability_flow=False):\n    super().__init__()\n    self.sde = sde\n    # Compute the reverse SDE/ODE\n    self.rsde = sde.reverse(score_fn, probability_flow)\n    self.score_fn = score_fn\n\n  @abc.abstractmethod\n  def update_fn(self, x, t):\n    \"\"\"One update of the predictor.\n    Args:\n      x: A PyTorch tensor representing the current state\n      t: A Pytorch tensor representing the current time step.\n    Returns:\n      x: A PyTorch tensor of the next state.\n      x_mean: A PyTorch tensor. The next state without random noise. Useful for denoising.\n    \"\"\"\n    pass\n\n\nclass Corrector(abc.ABC):\n  \"\"\"The abstract class for a corrector algorithm.\"\"\"\n\n  def __init__(self, sde, score_fn, snr, n_steps):\n    super().__init__()\n    self.sde = sde\n    self.score_fn = score_fn\n    self.snr = snr\n    self.n_steps = n_steps\n\n  @abc.abstractmethod\n  def update_fn(self, x, t):\n    \"\"\"One update of the corrector.\n    Args:\n      x: A PyTorch tensor representing the current state\n      t: A PyTorch tensor representing the current time step.\n    Returns:\n      x: A PyTorch tensor of the next state.\n      x_mean: A PyTorch tensor. The next state without random noise. Useful for denoising.\n    \"\"\"\n    pass\n\n\n@register_predictor(name='euler_maruyama')\nclass EulerMaruyamaPredictor(Predictor):\n  def __init__(self, sde, score_fn, probability_flow=False):\n    super().__init__(sde, score_fn, probability_flow)\n\n  def update_fn(self, x, t):\n    dt = -1. / self.rsde.N\n    z = torch.randn_like(x)\n    drift, diffusion = self.rsde.sde(x, t)\n    x_mean = x + drift * dt\n    x = x_mean + diffusion[:, None, None, None] * np.sqrt(-dt) * z\n    return x, x_mean\n\n\n@register_predictor(name='reverse_diffusion')\nclass ReverseDiffusionPredictor(Predictor):\n  def __init__(self, sde, score_fn, probability_flow=False):\n    super().__init__(sde, score_fn, probability_flow)\n\n  def update_fn(self, x, t):\n    f, G = self.rsde.discretize(x, t)\n    z = torch.randn_like(x)\n    x_mean = x - f\n    x = x_mean + G[:, None, None, None] * z\n    return x, x_mean\n\n\n@register_predictor(name='ancestral_sampling')\nclass AncestralSamplingPredictor(Predictor):\n  \"\"\"The ancestral sampling predictor. Currently only supports VE/VP SDEs.\"\"\"\n\n  def __init__(self, sde, score_fn, probability_flow=False):\n    super().__init__(sde, score_fn, probability_flow)\n    if not isinstance(sde,  VPSDE) and not isinstance(sde, VESDE):\n      raise NotImplementedError(f\"SDE class {sde.__class__.__name__} not yet supported.\")\n    assert not probability_flow, \"Probability flow not supported by ancestral sampling\"\n\n  def vesde_update_fn(self, x, t):\n    sde = self.sde\n    timestep = (t * (sde.N - 1) / sde.T).long()\n    sigma = sde.discrete_sigmas.to(t.device)[timestep]\n    adjacent_sigma = torch.where(timestep == 0, torch.zeros_like(t), sde.discrete_sigmas.to(t.device)[timestep - 1])\n    score = self.score_fn(x, t)\n    x_mean = x + score * (sigma ** 2 - adjacent_sigma ** 2)[:, None, None, None]\n    std = torch.sqrt((adjacent_sigma ** 2 * (sigma ** 2 - adjacent_sigma ** 2)) / (sigma ** 2))\n    noise = torch.randn_like(x)\n    x = x_mean + std[:, None, None, None] * noise\n    return x, x_mean\n\n  def vpsde_update_fn(self, x, t):\n    sde = self.sde\n    timestep = (t * (sde.N - 1) / sde.T).long()\n    beta = sde.discrete_betas.to(t.device)[timestep]\n    score = self.score_fn(x, t)\n    x_mean = (x + beta[:, None, None, None] * score) / torch.sqrt(1. - beta)[:, None, None, None]\n    noise = torch.randn_like(x)\n    x = x_mean + torch.sqrt(beta)[:, None, None, None] * noise\n    return x, x_mean\n\n  def update_fn(self, x, t):\n    if isinstance(self.sde,  VESDE):\n      return self.vesde_update_fn(x, t)\n    elif isinstance(self.sde,  VPSDE):\n      return self.vpsde_update_fn(x, t)\n\n\n@register_predictor(name='none')\nclass NonePredictor(Predictor):\n  \"\"\"An empty predictor that does nothing.\"\"\"\n\n  def __init__(self, sde, score_fn, probability_flow=False):\n    pass\n\n  def update_fn(self, x, t):\n    return x, x\n\n\n@register_corrector(name='langevin')\nclass LangevinCorrector(Corrector):\n  def __init__(self, sde, score_fn, snr, n_steps):\n    super().__init__(sde, score_fn, snr, n_steps)\n    if not isinstance(sde,  VPSDE) \\\n        and not isinstance(sde,  VESDE) \\\n        and not isinstance(sde,  subVPSDE):\n      raise NotImplementedError(f\"SDE class {sde.__class__.__name__} not yet supported.\")\n\n  def update_fn(self, x, t):\n    sde = self.sde\n    score_fn = self.score_fn\n    n_steps = self.n_steps\n    target_snr = self.snr\n    if isinstance(sde, VPSDE) or isinstance(sde,  subVPSDE):\n      timestep = (t * (sde.N - 1) / sde.T).long()\n      alpha = sde.alphas.to(t.device)[timestep]\n    else:\n      alpha = torch.ones_like(t)\n\n    for i in range(n_steps):\n      grad = score_fn(x, t)\n      noise = torch.randn_like(x)\n      grad_norm = torch.norm(grad.reshape(grad.shape[0], -1), dim=-1).mean()\n      noise_norm = torch.norm(noise.reshape(noise.shape[0], -1), dim=-1).mean()\n      step_size = (target_snr * noise_norm / grad_norm) ** 2 * 2 * alpha\n      x_mean = x + step_size[:, None, None, None] * grad\n      x = x_mean + torch.sqrt(step_size * 2)[:, None, None, None] * noise\n\n    return x, x_mean\n\n\n@register_corrector(name='ald')\nclass AnnealedLangevinDynamics(Corrector):\n  \"\"\"The original annealed Langevin dynamics predictor in NCSN/NCSNv2.\n  We include this corrector only for completeness. It was not directly used in our paper.\n  \"\"\"\n\n  def __init__(self, sde, score_fn, snr, n_steps):\n    super().__init__(sde, score_fn, snr, n_steps)\n    if not isinstance(sde,  VPSDE) \\\n        and not isinstance(sde,  VESDE) \\\n        and not isinstance(sde,  subVPSDE):\n      raise NotImplementedError(f\"SDE class {sde.__class__.__name__} not yet supported.\")\n\n  def update_fn(self, x, t):\n    sde = self.sde\n    score_fn = self.score_fn\n    n_steps = self.n_steps\n    target_snr = self.snr\n    if isinstance(sde,  VPSDE) or isinstance(sde,  subVPSDE):\n      timestep = (t * (sde.N - 1) / sde.T).long()\n      alpha = sde.alphas.to(t.device)[timestep]\n    else:\n      alpha = torch.ones_like(t)\n\n    std = self.sde.marginal_prob(x, t)[1]\n\n    for i in range(n_steps):\n      grad = score_fn(x, t)\n      noise = torch.randn_like(x)\n      step_size = (target_snr * std) ** 2 * 2 * alpha\n      x_mean = x + step_size[:, None, None, None] * grad\n      x = x_mean + noise * torch.sqrt(step_size * 2)[:, None, None, None]\n\n    return x, x_mean\n\n\n@register_corrector(name='none')\nclass NoneCorrector(Corrector):\n  \"\"\"An empty corrector that does nothing.\"\"\"\n\n  def __init__(self, sde, score_fn, snr, n_steps):\n    pass\n\n  def update_fn(self, x, t):\n    return x, x\n\n\ndef shared_predictor_update_fn(x, t, sde, model, predictor, probability_flow, continuous):\n  \"\"\"A wrapper that configures and returns the update function of predictors.\"\"\"\n  score_fn = get_score_fn(sde, model, train=False, continuous=continuous)\n  if predictor is None:\n    # Corrector-only sampler\n    predictor_obj = NonePredictor(sde, score_fn, probability_flow)\n  else:\n    predictor_obj = predictor(sde, score_fn, probability_flow)\n  return predictor_obj.update_fn(x, t)\n\n\ndef shared_corrector_update_fn(x, t, sde, model, corrector, continuous, snr, n_steps):\n  \"\"\"A wrapper tha configures and returns the update function of correctors.\"\"\"\n  score_fn = get_score_fn(sde, model, train=False, continuous=continuous)\n  if corrector is None:\n    # Predictor-only sampler\n    corrector_obj = NoneCorrector(sde, score_fn, snr, n_steps)\n  else:\n    corrector_obj = corrector(sde, score_fn, snr, n_steps)\n  return corrector_obj.update_fn(x, t)\n\n\ndef get_pc_sampler(sde, shape, predictor, corrector, inverse_scaler, snr,\n                   n_steps=1, probability_flow=False, continuous=False,\n                   denoise=True, eps=1e-3, device='cuda'):\n    \"\"\"Create a Predictor-Corrector (PC) sampler.\n      Args:\n        sde: An `sde_lib.SDE` object representing the forward SDE.\n        shape: A sequence of integers. The expected shape of a single sample.\n        predictor: A subclass of `sampling.Predictor` representing the predictor algorithm.\n        corrector: A subclass of `sampling.Corrector` representing the corrector algorithm.\n        inverse_scaler: The inverse data normalizer.\n        snr: A `float` number. The signal-to-noise ratio for configuring correctors.\n        n_steps: An integer. The number of corrector steps per predictor update.\n        probability_flow: If `True`, solve the reverse-time probability flow ODE when running the predictor.\n        continuous: `True` indicates that the score model was continuously trained.\n        denoise: If `True`, add one-step denoising to the final samples.\n        eps: A `float` number. The reverse-time SDE and ODE are integrated to `epsilon` to avoid numerical issues.\n        device: PyTorch device.\n      Returns:\n        A sampling function that returns samples and the number of function evaluations during sampling.\n      \"\"\"\n    # Create predictor & corrector update functions\n    predictor_update_fn = functools.partial(shared_predictor_update_fn,\n                                          sde=sde,\n                                          predictor=predictor,\n                                          probability_flow=probability_flow,\n                                          continuous=continuous)\n    corrector_update_fn = functools.partial(shared_corrector_update_fn,\n                                          sde=sde,\n                                          corrector=corrector,\n                                          continuous=continuous,\n                                          snr=snr,\n                                          n_steps=n_steps)\n    \n    def pc_sampler(model):\n        \"\"\" The PC sampler funciton.\n        Args:\n          model: A score model.\n        Returns:\n          Samples, number of function evaluations.\n        \"\"\"\n        with torch.no_grad():\n            # Initial sample\n#             x = sde.prior_sampling(shape).to(device)\n            x = init_input.cuda()\n            timesteps = torch.linspace(sde.T, eps, sde.N, device=device)\n\n            for i in range(9 * sde.N // 10, sde.N): # sde.N\n                t = timesteps[i]\n                vec_t = torch.ones(shape[0], device=t.device) * t\n                x, x_mean = corrector_update_fn(x, vec_t, model=model)\n                x, x_mean = predictor_update_fn(x, vec_t, model=model)\n\n            return inverse_scaler(x_mean if denoise else x), sde.N * (n_steps + 1)\n\n    return pc_sampler\n\n\n\n\n#   def pc_sampler(model):\n#     \"\"\" The PC sampler funciton.\n#     Args:\n#       model: A score model.\n#     Returns:\n#       Samples, number of function evaluations.\n#     \"\"\"\n#     with torch.no_grad():\n#       # Initial sample\n#       x = sde.prior_sampling(shape).to(device)\n#       timesteps = torch.linspace(sde.T, eps, sde.N, device=device)\n\n#       for i in range(sde.N):\n#         t = timesteps[i]\n#         vec_t = torch.ones(shape[0], device=t.device) * t\n#         x, x_mean = corrector_update_fn(x, vec_t, model=model)\n#         x, x_mean = predictor_update_fn(x, vec_t, model=model)\n\n#       return inverse_scaler(x_mean if denoise else x), sde.N * (n_steps + 1)\n\n#   return pc_sampler\n\n\n\ndef get_ode_sampler(sde, shape, inverse_scaler,\n                    denoise=False, rtol=1e-5, atol=1e-5,\n                    method='RK45', eps=1e-3, device='cuda'):\n  \"\"\"Probability flow ODE sampler with the black-box ODE solver.\n  Args:\n    sde: An `sde_lib.SDE` object that represents the forward SDE.\n    shape: A sequence of integers. The expected shape of a single sample.\n    inverse_scaler: The inverse data normalizer.\n    denoise: If `True`, add one-step denoising to final samples.\n    rtol: A `float` number. The relative tolerance level of the ODE solver.\n    atol: A `float` number. The absolute tolerance level of the ODE solver.\n    method: A `str`. The algorithm used for the black-box ODE solver.\n      See the documentation of `scipy.integrate.solve_ivp`.\n    eps: A `float` number. The reverse-time SDE/ODE will be integrated to `eps` for numerical stability.\n    device: PyTorch device.\n  Returns:\n    A sampling function that returns samples and the number of function evaluations during sampling.\n  \"\"\"\n\n  def denoise_update_fn(model, x):\n    score_fn = get_score_fn(sde, model, train=False, continuous=True)\n    # Reverse diffusion predictor for denoising\n    predictor_obj = ReverseDiffusionPredictor(sde, score_fn, probability_flow=False)\n    vec_eps = torch.ones(x.shape[0], device=x.device) * eps\n    _, x = predictor_obj.update_fn(x, vec_eps)\n    return x\n\n  def drift_fn(model, x, t):\n    \"\"\"Get the drift function of the reverse-time SDE.\"\"\"\n    score_fn = get_score_fn(sde, model, train=False, continuous=True)\n    rsde = sde.reverse(score_fn, probability_flow=True)\n    return rsde.sde(x, t)[0]\n\n  def ode_sampler(model, z=None):\n    \"\"\"The probability flow ODE sampler with black-box ODE solver.\n    Args:\n      model: A score model.\n      z: If present, generate samples from latent code `z`.\n    Returns:\n      samples, number of function evaluations.\n    \"\"\"\n    with torch.no_grad():\n      # Initial sample\n      if z is None:\n        # If not represent, sample the latent code from the prior distibution of the SDE.\n        x = sde.prior_sampling(shape).to(device)\n      else:\n        x = z\n\n      def ode_func(t, x):\n        x = from_flattened_numpy(x, shape).to(device).type(torch.float32)\n        vec_t = torch.ones(shape[0], device=x.device) * t\n        drift = drift_fn(model, x, vec_t)\n        return to_flattened_numpy(drift)\n\n      # Black-box ODE solver for the probability flow ODE\n      solution = integrate.solve_ivp(ode_func, (sde.T, eps), to_flattened_numpy(x),\n                                     rtol=rtol, atol=atol, method=method)\n      nfe = solution.nfev\n      x = torch.tensor(solution.y[:, -1]).reshape(shape).to(device).type(torch.float32)\n\n      # Denoising is equivalent to running one predictor step without adding noise\n      if denoise:\n        x = denoise_update_fn(model, x)\n\n      x = inverse_scaler(x)\n      return x, nfe\n\n  return ode_sampler","metadata":{"id":"VMK81EoZGc0o","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:32.749835Z","iopub.execute_input":"2023-12-17T20:04:32.750121Z","iopub.status.idle":"2023-12-17T20:04:32.815563Z","shell.execute_reply.started":"2023-12-17T20:04:32.750098Z","shell.execute_reply":"2023-12-17T20:04:32.814602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Datasets**","metadata":{"id":"bo4E9yMgGeJv"}},{"cell_type":"code","source":"# @title\n# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: skip-file\n\"\"\"Return training and evaluation/test datasets from config files.\"\"\"\nimport jax\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\n\ndef get_data_scaler(config):\n  \"\"\"Data normalizer. Assume data are always in [0, 1].\"\"\"\n  if config.data.centered:\n    # Rescale to [-1, 1]\n    return lambda x: x * 2. - 1.\n  else:\n    return lambda x: x\n\n\ndef get_data_inverse_scaler(config):\n  \"\"\"Inverse data normalizer.\"\"\"\n  if config.data.centered:\n    # Rescale [-1, 1] to [0, 1]\n    return lambda x: (x + 1.) / 2.\n  else:\n    return lambda x: x\n\n\ndef crop_resize(image, resolution):\n  \"\"\"Crop and resize an image to the given resolution.\"\"\"\n  crop = tf.minimum(tf.shape(image)[0], tf.shape(image)[1])\n  h, w = tf.shape(image)[0], tf.shape(image)[1]\n  image = image[(h - crop) // 2:(h + crop) // 2,\n          (w - crop) // 2:(w + crop) // 2]\n  image = tf.image.resize(\n    image,\n    size=(resolution, resolution),\n    antialias=True,\n    method=tf.image.ResizeMethod.BICUBIC)\n  return tf.cast(image, tf.uint8)\n\n\ndef resize_small(image, resolution):\n  \"\"\"Shrink an image to the given resolution.\"\"\"\n  h, w = image.shape[0], image.shape[1]\n  ratio = resolution / min(h, w)\n  h = tf.round(h * ratio, tf.int32)\n  w = tf.round(w * ratio, tf.int32)\n  return tf.image.resize(image, [h, w], antialias=True)\n\n\ndef central_crop(image, size):\n  \"\"\"Crop the center of an image to the given size.\"\"\"\n  top = (image.shape[0] - size) // 2\n  left = (image.shape[1] - size) // 2\n  return tf.image.crop_to_bounding_box(image, top, left, size, size)\n\n\ndef get_dataset(config, uniform_dequantization=False, evaluation=False):\n  \"\"\"Create data loaders for training and evaluation.\n  Args:\n    config: A ml_collection.ConfigDict parsed from config files.\n    uniform_dequantization: If `True`, add uniform dequantization to images.\n    evaluation: If `True`, fix number of epochs to 1.\n  Returns:\n    train_ds, eval_ds, dataset_builder.\n  \"\"\"\n  # Compute batch size for this worker.\n  batch_size = config.training.batch_size if not evaluation else config.eval.batch_size\n  if batch_size % jax.device_count() != 0:\n    raise ValueError(f'Batch sizes ({batch_size} must be divided by'\n                     f'the number of devices ({jax.device_count()})')\n\n  # Reduce this when image resolution is too large and data pointer is stored\n  shuffle_buffer_size = 10000\n  prefetch_size = tf.data.experimental.AUTOTUNE\n  num_epochs = None if not evaluation else 1\n\n  # Create dataset builders for each dataset.\n  if config.data.dataset == 'CIFAR10':\n    dataset_builder = tfds.builder('cifar10')\n    train_split_name = 'train'\n    eval_split_name = 'test'\n\n    def resize_op(img):\n      img = tf.image.convert_image_dtype(img, tf.float32)\n      return tf.image.resize(img, [config.data.image_size, config.data.image_size], antialias=True)\n\n  elif config.data.dataset == 'SVHN':\n    dataset_builder = tfds.builder('svhn_cropped')\n    train_split_name = 'train'\n    eval_split_name = 'test'\n\n    def resize_op(img):\n      img = tf.image.convert_image_dtype(img, tf.float32)\n      return tf.image.resize(img, [config.data.image_size, config.data.image_size], antialias=True)\n\n  elif config.data.dataset == 'CELEBA':\n    dataset_builder = tfds.builder('celeb_a')\n    train_split_name = 'train'\n    eval_split_name = 'validation'\n\n    def resize_op(img):\n      img = tf.image.convert_image_dtype(img, tf.float32)\n      img = central_crop(img, 140)\n      img = resize_small(img, config.data.image_size)\n      return img\n\n  elif config.data.dataset == 'LSUN':\n    dataset_builder = tfds.builder(f'lsun/{config.data.category}')\n    train_split_name = 'train'\n    eval_split_name = 'validation'\n\n    if config.data.image_size == 128:\n      def resize_op(img):\n        img = tf.image.convert_image_dtype(img, tf.float32)\n        img = resize_small(img, config.data.image_size)\n        img = central_crop(img, config.data.image_size)\n        return img\n\n    else:\n      def resize_op(img):\n        img = crop_resize(img, config.data.image_size)\n        img = tf.image.convert_image_dtype(img, tf.float32)\n        return img\n\n  elif config.data.dataset in ['FFHQ', 'CelebAHQ']:\n    dataset_builder = tf.data.TFRecordDataset(config.data.tfrecords_path)\n    train_split_name = eval_split_name = 'train'\n\n  else:\n    raise NotImplementedError(\n      f'Dataset {config.data.dataset} not yet supported.')\n\n  # Customize preprocess functions for each dataset.\n  if config.data.dataset in ['FFHQ', 'CelebAHQ']:\n    def preprocess_fn(d):\n      sample = tf.io.parse_single_example(d, features={\n        'shape': tf.io.FixedLenFeature([3], tf.int64),\n        'data': tf.io.FixedLenFeature([], tf.string)})\n      data = tf.io.decode_raw(sample['data'], tf.uint8)\n      data = tf.reshape(data, sample['shape'])\n      data = tf.transpose(data, (1, 2, 0))\n      img = tf.image.convert_image_dtype(data, tf.float32)\n      if config.data.random_flip and not evaluation:\n        img = tf.image.random_flip_left_right(img)\n      if uniform_dequantization:\n        img = (tf.random.uniform(img.shape, dtype=tf.float32) + img * 255.) / 256.\n      return dict(image=img, label=None)\n\n  else:\n    def preprocess_fn(d):\n      \"\"\"Basic preprocessing function scales data to [0, 1) and randomly flips.\"\"\"\n      img = resize_op(d['image'])\n      if config.data.random_flip and not evaluation:\n        img = tf.image.random_flip_left_right(img)\n      if uniform_dequantization:\n        img = (tf.random.uniform(img.shape, dtype=tf.float32) + img * 255.) / 256.\n\n      return dict(image=img, label=d.get('label', None))\n\n  def create_dataset(dataset_builder, split):\n    dataset_options = tf.data.Options()\n    dataset_options.experimental_optimization.map_parallelization = True\n    dataset_options.experimental_threading.private_threadpool_size = 48\n    dataset_options.experimental_threading.max_intra_op_parallelism = 1\n    read_config = tfds.ReadConfig(options=dataset_options)\n    if isinstance(dataset_builder, tfds.core.DatasetBuilder):\n      dataset_builder.download_and_prepare()\n      ds = dataset_builder.as_dataset(\n        split=split, shuffle_files=True, read_config=read_config)\n    else:\n      ds = dataset_builder.with_options(dataset_options)\n    ds = ds.repeat(count=num_epochs)\n    ds = ds.shuffle(shuffle_buffer_size)\n    ds = ds.map(preprocess_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    ds = ds.batch(batch_size, drop_remainder=True)\n    return ds.prefetch(prefetch_size)\n\n  train_ds = create_dataset(dataset_builder, train_split_name)\n  eval_ds = create_dataset(dataset_builder, eval_split_name)\n  return train_ds, eval_ds, dataset_builder","metadata":{"id":"nf6jwZYxGhd5","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:32.817522Z","iopub.execute_input":"2023-12-17T20:04:32.817802Z","iopub.status.idle":"2023-12-17T20:04:34.527274Z","shell.execute_reply.started":"2023-12-17T20:04:32.817769Z","shell.execute_reply":"2023-12-17T20:04:34.526298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**EMA**","metadata":{"id":"W0lPS3bNGkBX"}},{"cell_type":"code","source":"# @title\n# Modified from https://raw.githubusercontent.com/fadel/pytorch_ema/master/torch_ema/ema.py\n\nfrom __future__ import division\nfrom __future__ import unicode_literals\n\nimport torch\n\n\n# Partially based on: https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/training/moving_averages.py\nclass ExponentialMovingAverage:\n  \"\"\"\n  Maintains (exponential) moving average of a set of parameters.\n  \"\"\"\n\n  def __init__(self, parameters, decay, use_num_updates=True):\n    \"\"\"\n    Args:\n      parameters: Iterable of `torch.nn.Parameter`; usually the result of\n        `model.parameters()`.\n      decay: The exponential decay.\n      use_num_updates: Whether to use number of updates when computing\n        averages.\n    \"\"\"\n    if decay < 0.0 or decay > 1.0:\n      raise ValueError('Decay must be between 0 and 1')\n    self.decay = decay\n    self.num_updates = 0 if use_num_updates else None\n    self.shadow_params = [p.clone().detach()\n                          for p in parameters if p.requires_grad]\n    self.collected_params = []\n\n  def update(self, parameters):\n    \"\"\"\n    Update currently maintained parameters.\n    Call this every time the parameters are updated, such as the result of\n    the `optimizer.step()` call.\n    Args:\n      parameters: Iterable of `torch.nn.Parameter`; usually the same set of\n        parameters used to initialize this object.\n    \"\"\"\n    decay = self.decay\n    if self.num_updates is not None:\n      self.num_updates += 1\n      decay = min(decay, (1 + self.num_updates) / (10 + self.num_updates))\n    one_minus_decay = 1.0 - decay\n    with torch.no_grad():\n      parameters = [p for p in parameters if p.requires_grad]\n      for s_param, param in zip(self.shadow_params, parameters):\n        s_param.sub_(one_minus_decay * (s_param - param))\n\n  def copy_to(self, parameters):\n    \"\"\"\n    Copy current parameters into given collection of parameters.\n    Args:\n      parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n        updated with the stored moving averages.\n    \"\"\"\n    parameters = [p for p in parameters if p.requires_grad]\n    for s_param, param in zip(self.shadow_params, parameters):\n      if param.requires_grad:\n        param.data.copy_(s_param.data)\n\n  def store(self, parameters):\n    \"\"\"\n    Save the current parameters for restoring later.\n    Args:\n      parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n        temporarily stored.\n    \"\"\"\n    self.collected_params = [param.clone() for param in parameters]\n\n  def restore(self, parameters):\n    \"\"\"\n    Restore the parameters stored with the `store` method.\n    Useful to validate the model with EMA parameters without affecting the\n    original optimization process. Store the parameters before the\n    `copy_to` method. After validation (or model saving), use this to\n    restore the former parameters.\n    Args:\n      parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n        updated with the stored parameters.\n    \"\"\"\n    for c_param, param in zip(self.collected_params, parameters):\n      param.data.copy_(c_param.data)\n\n  def state_dict(self):\n    return dict(decay=self.decay, num_updates=self.num_updates,\n                shadow_params=self.shadow_params)\n\n  def load_state_dict(self, state_dict):\n    self.decay = state_dict['decay']\n    self.num_updates = state_dict['num_updates']\n    self.shadow_params = state_dict['shadow_params']","metadata":{"id":"7Ey80kk9GlOJ","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:34.528954Z","iopub.execute_input":"2023-12-17T20:04:34.529394Z","iopub.status.idle":"2023-12-17T20:04:34.543914Z","shell.execute_reply.started":"2023-12-17T20:04:34.529354Z","shell.execute_reply":"2023-12-17T20:04:34.542822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Utils**","metadata":{"id":"piGNOXhdGnoU"}},{"cell_type":"code","source":"# @title\nimport torch\nimport tensorflow as tf\nimport os\nimport logging\n\n\ndef restore_checkpoint(ckpt_dir, state, device):\n  if not tf.io.gfile.exists(ckpt_dir):\n    tf.io.gfile.makedirs(os.path.dirname(ckpt_dir))\n    logging.warning(f\"No checkpoint found at {ckpt_dir}. \"\n                    f\"Returned the same state as input\")\n    return state\n  else:\n    loaded_state = torch.load(ckpt_dir, map_location=device)\n    state['optimizer'].load_state_dict(loaded_state['optimizer'])\n    state['model'].load_state_dict(loaded_state['model'], strict=False)\n    state['ema'].load_state_dict(loaded_state['ema'])\n    state['step'] = loaded_state['step']\n    return state\n\n\ndef save_checkpoint(ckpt_dir, state):\n  saved_state = {\n    'optimizer': state['optimizer'].state_dict(),\n    'model': state['model'].state_dict(),\n    'ema': state['ema'].state_dict(),\n    'step': state['step']\n  }\n  torch.save(saved_state, ckpt_dir)","metadata":{"id":"g-nmBrPNGpLJ","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:34.545212Z","iopub.execute_input":"2023-12-17T20:04:34.545608Z","iopub.status.idle":"2023-12-17T20:04:34.569053Z","shell.execute_reply.started":"2023-12-17T20:04:34.545565Z","shell.execute_reply":"2023-12-17T20:04:34.568182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Run_lib(Train)**","metadata":{"id":"IQNkVq--GsHU"}},{"cell_type":"code","source":"# !pip install tensorflow-gan\nimport tensorflow_gan","metadata":{"id":"1buvmr1PGuhq","execution":{"iopub.status.busy":"2023-12-17T20:04:34.570201Z","iopub.execute_input":"2023-12-17T20:04:34.570508Z","iopub.status.idle":"2023-12-17T20:04:34.582699Z","shell.execute_reply.started":"2023-12-17T20:04:34.570475Z","shell.execute_reply":"2023-12-17T20:04:34.581906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @title\n# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: skip-file\n\"\"\"Training and evaluation for score-based generative models. \"\"\"\n\nimport gc\nimport io\nimport os\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_gan as tfgan\nimport logging\n# Keep the import below for registering all model definitions\n#from models import ddpm, ncsnv2, ncsnpp\n#import losses\n#import sampling\n#from models import utils as mutils\n#from models.ema import ExponentialMovingAverage\n#import datasets\n#import evaluation\n#import likelihood\n#import sde_lib\nfrom absl import flags\nimport torch\nfrom torch.utils import tensorboard\nfrom torchvision.utils import make_grid, save_image\n#from utils import save_checkpoint, restore_checkpoint\n\nFLAGS = flags.FLAGS\n\n\ndef train(config, workdir):\n  \"\"\"Runs the training pipeline.\n  Args:\n    config: Configuration to use.\n    workdir: Working directory for checkpoints and TF summaries. If this\n      contains checkpoint training will be resumed from the latest checkpoint.\n  \"\"\"\n\n  # Create directories for experimental logs\n  sample_dir = os.path.join(workdir, \"samples\")\n  tf.io.gfile.makedirs(sample_dir)\n\n  tb_dir = os.path.join(workdir, \"tensorboard\")\n  tf.io.gfile.makedirs(tb_dir)\n  #writer = tensorboard.SummaryWriter(tb_dir)\n\n  # Initialize model.\n  score_model = create_model(config)\n  ema = ExponentialMovingAverage(score_model.parameters(), decay=config.model.ema_rate)\n  optimizer =   get_optimizer(config, score_model.parameters())\n  state = dict(optimizer=optimizer, model=score_model, ema=ema, step=0)\n\n  # Create checkpoints directory\n  checkpoint_dir = os.path.join(workdir, \"checkpoints\")\n  # Intermediate checkpoints to resume training after pre-emption in cloud environments\n  checkpoint_meta_dir = os.path.join(workdir, \"checkpoints-meta\", \"checkpoint.pth\")\n  tf.io.gfile.makedirs(checkpoint_dir)\n  tf.io.gfile.makedirs(os.path.dirname(checkpoint_meta_dir))\n  # Resume training when intermediate checkpoints are detected\n  state = restore_checkpoint(checkpoint_meta_dir, state, config.device)\n  #print(\"initial_step\",initial_step)\n\n  # Build data iterators\n  train_ds, eval_ds, _ = datasets.get_dataset(config,\n                                              uniform_dequantization=config.data.uniform_dequantization)\n  train_iter = iter(train_ds)  # pytype: disable=wrong-arg-types\n  eval_iter = iter(eval_ds)  # pytype: disable=wrong-arg-types\n  # Create data normalizer and its inverse\n  scaler = datasets.get_data_scaler(config)\n  inverse_scaler = datasets.get_data_inverse_scaler(config)\n\n  # Setup SDEs\n  if config.training.sde.lower() == 'vpsde':\n    sde =  VPSDE(beta_min=config.model.beta_min, beta_max=config.model.beta_max, N=config.model.num_scales)\n    sampling_eps = 1e-3\n  elif config.training.sde.lower() == 'subvpsde':\n    sde =  subVPSDE(beta_min=config.model.beta_min, beta_max=config.model.beta_max, N=config.model.num_scales)\n    sampling_eps = 1e-3\n  elif config.training.sde.lower() == 'vesde':\n    sde = VESDE(sigma_min=config.model.sigma_min, \n                sigma_max=config.model.sigma_max, \n                N=config.model.num_scales)\n    sampling_eps = 1e-5\n  else:\n    raise NotImplementedError(f\"SDE {config.training.sde} unknown.\")\n\n  # Build one-step training and evaluation functions\n  optimize_fn = losses.optimization_manager(config)\n  continuous = config.training.continuous\n  reduce_mean = config.training.reduce_mean\n  likelihood_weighting = config.training.likelihood_weighting\n  train_step_fn = losses.get_step_fn(sde, train=True, optimize_fn=optimize_fn,\n                                     reduce_mean=reduce_mean, continuous=continuous,\n                                     likelihood_weighting=likelihood_weighting)\n  eval_step_fn = losses.get_step_fn(sde, train=False, optimize_fn=optimize_fn,\n                                    reduce_mean=reduce_mean, continuous=continuous,\n                                    likelihood_weighting=likelihood_weighting)\n\n  # Building sampling functions\n  if config.training.snapshot_sampling:\n    sampling_shape = (1, config.data.num_channels,\n                      config.data.image_size, config.data.image_size)\n    sampling_fn = sampling.get_sampling_fn(config, sde, sampling_shape, inverse_scaler, sampling_eps)\n\n  num_train_steps = config.training.n_iters\n\n  # In case there are multiple hosts (e.g., TPU pods), only log to host 0\n  logging.info(\"Starting training loop at step %d.\" % (initial_step,))\n\n  for step in range(initial_step, num_train_steps + 1):\n    # Convert data to JAX arrays and normalize them. Use ._numpy() to avoid copy.\n\n\n    batch = torch.from_numpy(next(train_iter)['image']._numpy()).to(config.device).float()\n    batch = batch.permute(0, 3, 1, 2)\n    batch = scaler(batch)\n    # Execute one training step\n    loss = train_step_fn(state, batch)\n    if step % config.training.log_freq == 0:\n      logging.info(\"step: %d, training_loss: %.5e\" % (step, loss.item()))\n    #  writer.add_scalar(\"training_loss\", loss, step)\n\n    # Save a temporary checkpoint to resume training after pre-emption periodically\n    if step != 0 and step % config.training.snapshot_freq_for_preemption == 0:\n      save_checkpoint(checkpoint_meta_dir, state)\n\n    # Report the loss on an evaluation dataset periodically\n    if step % config.training.eval_freq == 0:\n      eval_batch = torch.from_numpy(next(eval_iter)['image']._numpy()).to(config.device).float()\n      eval_batch = eval_batch.permute(0, 3, 1, 2)\n      eval_batch = scaler(eval_batch)\n      eval_loss = eval_step_fn(state, eval_batch)\n      logging.info(\"step: %d, eval_loss: %.5e\" % (step, eval_loss.item()))\n     # writer.add_scalar(\"eval_loss\", eval_loss.item(), step)\n\n    # Save a checkpoint periodically and generate samples if needed\n    if step != 0 and step % config.training.snapshot_freq == 0 or step == num_train_steps:\n      # Save the checkpoint.\n      save_step = step // config.training.snapshot_freq\n      save_checkpoint(os.path.join(checkpoint_dir, f'checkpoint_{save_step}.pth'), state)\n\n      # Generate and save samples\n      if config.training.snapshot_sampling:\n        ema.store(score_model.parameters())\n        ema.copy_to(score_model.parameters())\n        sample, n = sampling_fn(score_model)\n        ema.restore(score_model.parameters())\n        this_sample_dir = os.path.join(sample_dir, \"iter_{}\".format(step))\n        tf.io.gfile.makedirs(this_sample_dir)\n        nrow = int(np.sqrt(sample.shape[0]))\n        image_grid = make_grid(sample, nrow, padding=2)\n        sample = np.clip(sample.permute(0, 2, 3, 1).cpu().numpy() * 255, 0, 255).astype(np.uint8)\n        with tf.io.gfile.GFile(\n            os.path.join(this_sample_dir, \"sample.np\"), \"wb\") as fout:\n          np.save(fout, sample)\n\n        with tf.io.gfile.GFile(\n            os.path.join(this_sample_dir, \"sample.png\"), \"wb\") as fout:\n          save_image(image_grid, fout)\n\n\ndef evaluate(config,\n             workdir,\n             eval_folder=\"eval\"):\n  \"\"\"Evaluate trained models.\n  Args:\n    config: Configuration to use.\n    workdir: Working directory for checkpoints.\n    eval_folder: The subfolder for storing evaluation results. Default to\n      \"eval\".\n  \"\"\"\n  # Create directory to eval_folder\n  eval_dir = os.path.join(workdir, eval_folder)\n  tf.io.gfile.makedirs(eval_dir)\n\n  # Build data pipeline\n  train_ds, eval_ds, _ = datasets.get_dataset(config,\n                                              uniform_dequantization=config.data.uniform_dequantization,\n                                              evaluation=True)\n\n  # Create data normalizer and its inverse\n  scaler = datasets.get_data_scaler(config)\n  inverse_scaler = datasets.get_data_inverse_scaler(config)\n\n  # Initialize model\n  score_model =  create_model(config)\n  optimizer = losses.get_optimizer(config, score_model.parameters())\n  ema = ExponentialMovingAverage(score_model.parameters(), decay=config.model.ema_rate)\n  state = dict(optimizer=optimizer, model=score_model, ema=ema, step=0)\n\n  checkpoint_dir = os.path.join(workdir, \"checkpoints\")\n\n  # Setup SDEs\n  if config.training.sde.lower() == 'vpsde':\n    sde = VPSDE(beta_min=config.model.beta_min, beta_max=config.model.beta_max, N=config.model.num_scales)\n    sampling_eps = 1e-3\n  elif config.training.sde.lower() == 'subvpsde':\n    sde = subVPSDE(beta_min=config.model.beta_min, beta_max=config.model.beta_max, N=config.model.num_scales)\n    sampling_eps = 1e-3\n  elif config.training.sde.lower() == 'vesde':\n    sde =  VESDE(sigma_min=config.model.sigma_min, sigma_max=config.model.sigma_max, N=config.model.num_scales)\n    sampling_eps = 1e-5\n  else:\n    raise NotImplementedError(f\"SDE {config.training.sde} unknown.\")\n\n  # Create the one-step evaluation function when loss computation is enabled\n  if config.eval.euler_maruyama_sampler:\n    optimize_fn = losses.optimization_manager(config)\n    continuous = config.training.continuous\n    likelihood_weighting = config.training.likelihood_weighting\n\n    reduce_mean = config.training.reduce_mean\n    eval_step = losses.get_step_fn(sde, train=False, optimize_fn=optimize_fn,\n                                   reduce_mean=reduce_mean,\n                                   continuous=continuous,\n                                   likelihood_weighting=likelihood_weighting)\n\n\n  # Create data loaders for likelihood evaluation. Only evaluate on uniformly dequantized data\n  train_ds_bpd, eval_ds_bpd, _ = datasets.get_dataset(config,\n                                                      uniform_dequantization=True, evaluation=True)\n  if config.eval.bpd_dataset.lower() == 'train':\n    ds_bpd = train_ds_bpd\n    bpd_num_repeats = 1\n  elif config.eval.bpd_dataset.lower() == 'test':\n    # Go over the dataset 5 times when computing likelihood on the test dataset\n    ds_bpd = eval_ds_bpd\n    bpd_num_repeats = 5\n  else:\n    raise ValueError(f\"No bpd dataset {config.eval.bpd_dataset} recognized.\")\n\n  # Build the likelihood computation function when likelihood is enabled\n  if config.eval.enable_bpd:\n    likelihood_fn = likelihood.get_likelihood_fn(sde, inverse_scaler)\n\n  # Build the sampling function when sampling is enabled\n  if config.eval.enable_sampling:\n    sampling_shape = (config.eval.batch_size,\n                      config.data.num_channels,\n                      config.data.image_size, config.data.image_size)\n    sampling_fn = sampling.get_sampling_fn(config, sde, sampling_shape, inverse_scaler, sampling_eps)\n\n  # Use inceptionV3 for images with resolution higher than 256.\n  inceptionv3 = config.data.image_size >= 256\n  inception_model = evaluation.get_inception_model(inceptionv3=inceptionv3)\n\n  begin_ckpt = config.eval.begin_ckpt\n  logging.info(\"begin checkpoint: %d\" % (begin_ckpt,))\n  for ckpt in range(begin_ckpt, config.eval.end_ckpt + 1):\n    # Wait if the target checkpoint doesn't exist yet\n    waiting_message_printed = False\n    ckpt_filename = os.path.join(checkpoint_dir, \"checkpoint_{}.pth\".format(ckpt))\n    while not tf.io.gfile.exists(ckpt_filename):\n      if not waiting_message_printed:\n        logging.warning(\"Waiting for the arrival of checkpoint_%d\" % (ckpt,))\n        waiting_message_printed = True\n      time.sleep(60)\n\n    # Wait for 2 additional mins in case the file exists but is not ready for reading\n    ckpt_path = os.path.join(checkpoint_dir, f'checkpoint_{ckpt}.pth')\n    try:\n      state = restore_checkpoint(ckpt_path, state, device=config.device)\n    except:\n      time.sleep(60)\n      try:\n        state = restore_checkpoint(ckpt_path, state, device=config.device)\n      except:\n        time.sleep(120)\n        state = restore_checkpoint(ckpt_path, state, device=config.device)\n    ema.copy_to(score_model.parameters())\n    # Compute the loss function on the full evaluation dataset if loss computation is enabled\n    if config.eval.enable_loss:\n      all_losses = []\n      eval_iter = iter(eval_ds)  # pytype: disable=wrong-arg-types\n      for i, batch in enumerate(eval_iter):\n        eval_batch = torch.from_numpy(batch['image']._numpy()).to(config.device).float()\n        eval_batch = eval_batch.permute(0, 3, 1, 2)\n        eval_batch = scaler(eval_batch)\n        eval_loss = eval_step(state, eval_batch)\n        all_losses.append(eval_loss.item())\n        if (i + 1) % 1000 == 0:\n          logging.info(\"Finished %dth step loss evaluation\" % (i + 1))\n\n      # Save loss values to disk or Google Cloud Storage\n      all_losses = np.asarray(all_losses)\n      with tf.io.gfile.GFile(os.path.join(eval_dir, f\"ckpt_{ckpt}_loss.npz\"), \"wb\") as fout:\n        io_buffer = io.BytesIO()\n        np.savez_compressed(io_buffer, all_losses=all_losses, mean_loss=all_losses.mean())\n        fout.write(io_buffer.getvalue())\n\n    # Compute log-likelihoods (bits/dim) if enabled\n    if config.eval.enable_bpd:\n      bpds = []\n      for repeat in range(bpd_num_repeats):\n        bpd_iter = iter(ds_bpd)  # pytype: disable=wrong-arg-types\n        for batch_id in range(len(ds_bpd)):\n          batch = next(bpd_iter)\n          eval_batch = torch.from_numpy(batch['image']._numpy()).to(config.device).float()\n          eval_batch = eval_batch.permute(0, 3, 1, 2)\n          eval_batch = scaler(eval_batch)\n          bpd = likelihood_fn(score_model, eval_batch)[0]\n          bpd = bpd.detach().cpu().numpy().reshape(-1)\n          bpds.extend(bpd)\n          logging.info(\n            \"ckpt: %d, repeat: %d, batch: %d, mean bpd: %6f\" % (ckpt, repeat, batch_id, np.mean(np.asarray(bpds))))\n          bpd_round_id = batch_id + len(ds_bpd) * repeat\n          # Save bits/dim to disk or Google Cloud Storage\n          with tf.io.gfile.GFile(os.path.join(eval_dir,\n                                              f\"{config.eval.bpd_dataset}_ckpt_{ckpt}_bpd_{bpd_round_id}.npz\"),\n                                 \"wb\") as fout:\n            io_buffer = io.BytesIO()\n            np.savez_compressed(io_buffer, bpd)\n            fout.write(io_buffer.getvalue())\n\n    # Generate samples and compute IS/FID/KID when enabled\n    if config.eval.enable_sampling:\n      num_sampling_rounds = config.eval.num_samples // config.eval.batch_size + 1\n      for r in range(num_sampling_rounds):\n        logging.info(\"sampling -- ckpt: %d, round: %d\" % (ckpt, r))\n\n        # Directory to save samples. Different for each host to avoid writing conflicts\n        this_sample_dir = os.path.join(\n          eval_dir, f\"ckpt_{ckpt}\")\n        tf.io.gfile.makedirs(this_sample_dir)\n        samples, n = sampling_fn(score_model)\n        samples = np.clip(samples.permute(0, 2, 3, 1).cpu().numpy() * 255., 0, 255).astype(np.uint8)\n        samples = samples.reshape(\n          (-1, config.data.image_size, config.data.image_size, config.data.num_channels))\n        # Write samples to disk or Google Cloud Storage\n        with tf.io.gfile.GFile(\n            os.path.join(this_sample_dir, f\"samples_{r}.npz\"), \"wb\") as fout:\n          io_buffer = io.BytesIO()\n          np.savez_compressed(io_buffer, samples=samples)\n          fout.write(io_buffer.getvalue())\n\n        # Force garbage collection before calling TensorFlow code for Inception network\n        gc.collect()\n        latents = evaluation.run_inception_distributed(samples, inception_model,\n                                                       inceptionv3=inceptionv3)\n        # Force garbage collection again before returning to JAX code\n        gc.collect()\n        # Save latent represents of the Inception network to disk or Google Cloud Storage\n        with tf.io.gfile.GFile(\n            os.path.join(this_sample_dir, f\"statistics_{r}.npz\"), \"wb\") as fout:\n          io_buffer = io.BytesIO()\n          np.savez_compressed(\n            io_buffer, pool_3=latents[\"pool_3\"], logits=latents[\"logits\"])\n          fout.write(io_buffer.getvalue())\n\n      # Compute inception scores, FIDs and KIDs.\n      # Load all statistics that have been previously computed and saved for each host\n      all_logits = []\n      all_pools = []\n      this_sample_dir = os.path.join(eval_dir, f\"ckpt_{ckpt}\")\n      stats = tf.io.gfile.glob(os.path.join(this_sample_dir, \"statistics_*.npz\"))\n      for stat_file in stats:\n        with tf.io.gfile.GFile(stat_file, \"rb\") as fin:\n          stat = np.load(fin)\n          if not inceptionv3:\n            all_logits.append(stat[\"logits\"])\n          all_pools.append(stat[\"pool_3\"])\n\n      if not inceptionv3:\n        all_logits = np.concatenate(all_logits, axis=0)[:config.eval.num_samples]\n      all_pools = np.concatenate(all_pools, axis=0)[:config.eval.num_samples]\n\n      # Load pre-computed dataset statistics.\n      data_stats = evaluation.load_dataset_stats(config)\n      data_pools = data_stats[\"pool_3\"]\n\n      # Compute FID/KID/IS on all samples together.\n      if not inceptionv3:\n        inception_score = tfgan.eval.classifier_score_from_logits(all_logits)\n      else:\n        inception_score = -1\n\n      fid = tfgan.eval.frechet_classifier_distance_from_activations(\n        data_pools, all_pools)\n      # Hack to get tfgan KID work for eager execution.\n      tf_data_pools = tf.convert_to_tensor(data_pools)\n      tf_all_pools = tf.convert_to_tensor(all_pools)\n      kid = tfgan.eval.kernel_classifier_distance_from_activations(\n        tf_data_pools, tf_all_pools).numpy()\n      del tf_data_pools, tf_all_pools\n\n      logging.info(\n        \"ckpt-%d --- inception_score: %.6e, FID: %.6e, KID: %.6e\" % (\n          ckpt, inception_score, fid, kid))\n\n      with tf.io.gfile.GFile(os.path.join(eval_dir, f\"report_{ckpt}.npz\"),\n                             \"wb\") as f:\n        io_buffer = io.BytesIO()\n        np.savez_compressed(io_buffer, IS=inception_score, fid=fid, kid=kid)\n        f.write(io_buffer.getvalue())","metadata":{"id":"UAy1AsohGxdF","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:34.584260Z","iopub.execute_input":"2023-12-17T20:04:34.584622Z","iopub.status.idle":"2023-12-17T20:04:34.665220Z","shell.execute_reply.started":"2023-12-17T20:04:34.584590Z","shell.execute_reply":"2023-12-17T20:04:34.664247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Torch Dataloader**","metadata":{"id":"g5FUa03TGzem"}},{"cell_type":"code","source":"# @title\ndef Plot_(samles_):\n    samles_ = samles_.clamp(0.0, 1.0)\n    sample_grid = make_grid(samles_[:64], nrow=int(np.sqrt(64)))\n    plt.figure(figsize=(10,10))\n    plt.axis('off')\n    plt.imshow(sample_grid.permute(1, 2, 0).cpu(), vmin=0., vmax=1.)\n    plt.show()","metadata":{"id":"5-SUW920G1m2","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:34.666638Z","iopub.execute_input":"2023-12-17T20:04:34.667299Z","iopub.status.idle":"2023-12-17T20:04:34.673260Z","shell.execute_reply.started":"2023-12-17T20:04:34.667264Z","shell.execute_reply":"2023-12-17T20:04:34.672327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.datasets import CIFAR100\nfrom torchvision import transforms\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision.models import resnet\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data.dataset import Dataset\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\ndef image_grid(x):\n  size =256# config.data.image_size train_model\n  channels =3# config.data.num_channels\n  img = x.reshape(-1, size, size, channels)\n  w = int(np.sqrt(img.shape[0]))\n  img = img.reshape((w, w, size, size, channels)).transpose((0, 2, 1, 3, 4)).reshape((w * size, w * size, channels))\n  return img\nimport matplotlib.pyplot as plt\n\ndef show_samples(x):\n  x = x.permute(0, 2, 3, 1).detach().cpu().numpy()\n  img = image_grid(x)\n  plt.figure(figsize=(15,15))\n  plt.axis('off')\n  plt.imshow(img)\n  plt.show()","metadata":{"id":"Z46-38ZUG3r2","outputId":"0ced235e-ea7b-4b0e-cea9-aa48eeacdada","execution":{"iopub.status.busy":"2023-12-17T20:04:34.674496Z","iopub.execute_input":"2023-12-17T20:04:34.674838Z","iopub.status.idle":"2023-12-17T20:04:34.687568Z","shell.execute_reply.started":"2023-12-17T20:04:34.674814Z","shell.execute_reply":"2023-12-17T20:04:34.686767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train**","metadata":{"id":"Jm3X-oQSG-fD"}},{"cell_type":"code","source":"# @title\ndef train(config, workdir):\n  \"\"\"Runs the training pipeline.\n  Args:\n    config: Configuration to use.\n    workdir: Working directory for checkpoints and TF summaries. If this\n      contains checkpoint training will be resumed from the latest checkpoint.\n  \"\"\"\n\n  # Create directories for experimental logs\n  sample_dir = os.path.join(workdir, \"samples\")\n  tf.io.gfile.makedirs(sample_dir)\n\n  tb_dir = os.path.join(workdir, \"tensorboard\")\n  tf.io.gfile.makedirs(tb_dir)\n  ###writer = SummaryWriter(tb_dir)\n\n  # Initialize model.\n  score_model =  create_model(config)\n  ema = ExponentialMovingAverage(score_model.parameters(), decay=config.model.ema_rate)\n  optimizer =  get_optimizer(config, score_model.parameters())\n  state = dict(optimizer=optimizer, model=score_model, ema=ema, step=0)\n\n  # Create checkpoints directory\n  checkpoint_dir = os.path.join(workdir, \"checkpoints\")\n  # Intermediate checkpoints to resume training after pre-emption in cloud environments\n  checkpoint_meta_dir = os.path.join(workdir, \"checkpoints-meta\", \"checkpoint.pth\")\n  tf.io.gfile.makedirs(checkpoint_dir)\n  tf.io.gfile.makedirs(os.path.dirname(checkpoint_meta_dir))\n  # Resume training when intermediate checkpoints are detected\n  state = restore_checkpoint(checkpoint_meta_dir, state, config.device)\n  initial_step = int(state['step'])\n  print(\"initial_step\",initial_step)\n\n  # Build data iterators\n  train_ds, eval_ds, _ =  get_dataset(config,\n                                              uniform_dequantization=config.data.uniform_dequantization)\n  train_iter = iter(train_ds)  # pytype: disable=wrong-arg-types\n  eval_iter = iter(eval_ds)  # pytype: disable=wrong-arg-types\n  # Create data normalizer and its inverse\n  scaler =  get_data_scaler(config)\n  inverse_scaler =  get_data_inverse_scaler(config)\n\n  # Setup SDEs\n  if config.training.sde.lower() == 'vpsde':\n    sde =  VPSDE(beta_min=config.model.beta_min, beta_max=config.model.beta_max, N=config.model.num_scales)\n    sampling_eps = 1e-3\n  elif config.training.sde.lower() == 'subvpsde':\n    sde =  subVPSDE(beta_min=config.model.beta_min, beta_max=config.model.beta_max, N=config.model.num_scales)\n    sampling_eps = 1e-3\n  elif config.training.sde.lower() == 'vesde':\n    sde =  VESDE(sigma_min=config.model.sigma_min, sigma_max=config.model.sigma_max, N=config.model.num_scales)\n    sampling_eps = 1e-5\n  else:\n    raise NotImplementedError(f\"SDE {config.training.sde} unknown.\")\n\n  # Build one-step training and evaluation functions\n  optimize_fn =  optimization_manager(config)\n  continuous = config.training.continuous\n  reduce_mean = config.training.reduce_mean\n  likelihood_weighting = config.training.likelihood_weighting\n  train_step_fn = get_step_fn(sde, train=True, optimize_fn=optimize_fn,\n                                     reduce_mean=reduce_mean, continuous=continuous,\n                                     likelihood_weighting=likelihood_weighting)\n  eval_step_fn = get_step_fn(sde, train=False, optimize_fn=optimize_fn,\n                                    reduce_mean=reduce_mean, continuous=continuous,\n                                    likelihood_weighting=likelihood_weighting)\n\n  # Building sampling functions\n  if config.training.snapshot_sampling:\n    sampling_shape = (1, config.data.num_channels,\n                      config.data.image_size, config.data.image_size)\n    sampling_fn =   get_sampling_fn(config, sde, sampling_shape, inverse_scaler, sampling_eps)\n\n  num_train_steps = config.training.n_iters\n\n  # In case there are multiple hosts (e.g., TPU pods), only log to host 0\n  logging.info(\"Starting training loop at step %d.\" % (initial_step,))\n  for step in range(initial_step, num_train_steps  + 1):\n    # Convert data to JAX arrays and normalize them. Use ._numpy() to avoid copy.\n    for myiter, (batch_images, batch_labels) in enumerate(train_loader_sde):\n      #print(myiter)\n      batch = batch_images.cuda()\n      batch = scaler(batch)\n      loss = train_step_fn(state, batch)\n      print(loss.item(),step)\n     # if step % config.training.log_freq == 0:\n     #    logging.info(\"step: %d, training_loss: %.5e\" % (step, loss.item()))\n\n      #if step != 0 and step % config.training.snapshot_freq_for_preemption == 0:\n         #save_checkpoint(checkpoint_meta_dir, state)\n      if step != 0 and step % 300== 0 and (myiter==50):\n         checkpoint_meta_dirz = os.path.join(workdir, \"checkpoints-meta\", \"iter_{}_\".format(step) +\"checkpoint.pth\")\n         save_checkpoint(checkpoint_meta_dirz, state)\n   #   if step % config.training.eval_freq == 0:\n        #eval_batch = torch.from_numpy(next(eval_iter)['image']._numpy()).to(config.device).float()\n      #  eval_batch = eval_batch.permute(0, 3, 1, 2)\n      #  eval_batch = scaler(eval_batch)\n       # eval_loss = eval_step_fn(state, eval_batch)\n       # logging.info(\"step: %d, eval_loss: %.5e\" % (step, eval_loss.item()))\n      #  writer.add_scalar(\"eval_loss\", eval_loss.item(), step)\n      if step != 0 and (step % 300 == 0)  and (myiter==50):\n\n      # Save the checkpoint.\n       # save_step = step // config.training.snapshot_freq\n       # save_checkpoint(os.path.join(checkpoint_dir, f'checkpoint_{save_step}.pth'), state)\n\n        # Generate and save samples meta\n        if config.training.snapshot_sampling:\n          ema.store(score_model.parameters())\n          ema.copy_to(score_model.parameters())\n          sample, n = sampling_fn(score_model)\n          ema.restore(score_model.parameters())\n          this_sample_dir = os.path.join(sample_dir, \"iter_{}\".format(step))\n          tf.io.gfile.makedirs(this_sample_dir)\n          nrow = int(np.sqrt(sample.shape[0]))\n          image_grid = make_grid(sample, nrow, padding=2)\n          sample = np.clip(sample.permute(0, 2, 3, 1).cpu().numpy() * 255, 0, 255).astype(np.uint8)\n          with tf.io.gfile.GFile(\n              os.path.join(this_sample_dir, \"sample.np\"), \"wb\") as fout:\n            np.save(fout, sample)\n\n          with tf.io.gfile.GFile(\n              os.path.join(this_sample_dir, \"sample.png\"), \"wb\") as fout:\n            save_image(image_grid, fout)","metadata":{"id":"CNIZnC0wHBix","cellView":"form","execution":{"iopub.status.busy":"2023-12-17T20:04:34.689135Z","iopub.execute_input":"2023-12-17T20:04:34.689408Z","iopub.status.idle":"2023-12-17T20:04:34.712867Z","shell.execute_reply.started":"2023-12-17T20:04:34.689384Z","shell.execute_reply":"2023-12-17T20:04:34.711827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Configs**","metadata":{"id":"F0YtXkGPHEcX"}},{"cell_type":"code","source":"# !pip install ml_collections\nimport ml_collections\nimport torch\n\n\ndef get_default_configs():\n  config = ml_collections.ConfigDict()\n  # training 3\n  config.training = training = ml_collections.ConfigDict()\n  config.training.batch_size = 2\n  training.n_iters = 50000         # changed////////////////////////////////////////////\n  training.snapshot_freq =10\n  training.log_freq = 5\n  training.eval_freq = 10\n  ## store additional checkpoints for preemption in cloud computing environments\n  training.snapshot_freq_for_preemption = 100\n  ## produce samples at each snapshot.\n  training.snapshot_sampling = True\n  training.likelihood_weighting = False\n  training.continuous = True\n  training.reduce_mean = False\n\n  # sampling\n  config.sampling = sampling = ml_collections.ConfigDict()\n  sampling.n_steps_each = 1\n  sampling.noise_removal = True\n  sampling.probability_flow = False\n  sampling.snr = 0.16\n\n  # evaluation channel\n  config.eval = evaluate = ml_collections.ConfigDict()\n  evaluate.begin_ckpt = 9\n  evaluate.end_ckpt = 26\n  evaluate.batch_size = 3\n  evaluate.enable_sampling = False\n  evaluate.num_samples = 5000 #50000\n  evaluate.enable_loss = True\n  evaluate.enable_bpd = False\n  evaluate.bpd_dataset = 'test'\n\n  # data\n  config.data = data = ml_collections.ConfigDict()\n  data.dataset = 'MIDOG'\n  data.image_size = 128  # changed\n  data.random_flip = True\n  data.centered = False\n  data.uniform_dequantization = False\n  data.num_channels = 3\n\n  # model\n  config.model = model = ml_collections.ConfigDict()\n  model.sigma_min = 0.01\n  model.sigma_max = 50\n  model.num_scales = 50000                     # changed ///////////////////////////////////\n  model.beta_min = 0.1\n  model.beta_max = 20.\n  model.dropout = 0.1\n  model.embedding_type = 'fourier'\n\n  # optimization\n  config.optim = optim = ml_collections.ConfigDict()\n  optim.weight_decay = 0\n  optim.optimizer = 'Adam'\n  optim.lr = 2e-4\n  optim.beta1 = 0.9\n  optim.eps = 1e-8\n  optim.warmup = 5000\n  optim.grad_clip = 1.\n\n  config.seed = 42\n  config.device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n  #########################################\n#   config.training.num_epochs = 3\n\n  return config\n# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n\"\"\"Training NCSN++ on CIFAR-10 with VE SDE.\"\"\"\n\n\ndef get_config():\n  config = get_default_configs()\n  # training\n  training = config.training\n  training.sde = 'vesde'\n  training.continuous = True\n\n  # sampling\n  sampling = config.sampling\n  sampling.method = 'pc'\n  sampling.predictor = 'reverse_diffusion'\n  sampling.corrector = 'langevin'\n\n  # model chan\n  model = config.model\n  model.name = 'ncsnpp'\n  model.scale_by_sigma = True\n  model.ema_rate = 0.999\n  model.normalization = 'InstanceNorm'\n  model.nonlinearity = 'swish'\n  model.nf = 128\n  model.ch_mult = (1, 2, 2, 2)\n  model.num_res_blocks = 6\n  model.attn_resolutions = (16,)\n  model.resamp_with_conv = True\n  model.conditional = True\n  model.fir = True\n  model.fir_kernel = [1, 3, 3, 1]\n  model.skip_rescale = True\n  model.resblock_type = 'biggan'\n  model.progressive = 'none'\n  model.progressive_input = 'residual'\n  model.progressive_combine = 'sum'\n  model.attention_type = 'ddpm'\n  model.init_scale = 0.\n  model.fourier_scale = 16\n  model.conv_size = 3\n\n  return config","metadata":{"id":"tNNq0K7OHGO5","execution":{"iopub.status.busy":"2023-12-17T20:04:34.717793Z","iopub.execute_input":"2023-12-17T20:04:34.718118Z","iopub.status.idle":"2023-12-17T20:04:34.759929Z","shell.execute_reply.started":"2023-12-17T20:04:34.718090Z","shell.execute_reply":"2023-12-17T20:04:34.759115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config =  get_config()\nconfig","metadata":{"id":"6vKNO6tzHJrS","outputId":"0cb81952-66b6-4c2d-8b50-45e5a4f981bb","execution":{"iopub.status.busy":"2023-12-17T20:04:34.761106Z","iopub.execute_input":"2023-12-17T20:04:34.761730Z","iopub.status.idle":"2023-12-17T20:04:34.777498Z","shell.execute_reply.started":"2023-12-17T20:04:34.761695Z","shell.execute_reply":"2023-12-17T20:04:34.776645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Final Run**","metadata":{"id":"RUHXDYZ_HLQB"}},{"cell_type":"code","source":"import logging\nconfig =  get_config()\ntf.io.gfile.makedirs(\"/kaggle/working\")\ngfile_stream = open(os.path.join(\"/kaggle/working\", 'stdout.txt'), 'w')\nhandler = logging.StreamHandler(gfile_stream)\nformatter = logging.Formatter('%(levelname)s - %(filename)s - %(asctime)s - %(message)s')\nhandler.setFormatter(formatter)\nlogger = logging.getLogger()\nlogger.addHandler(handler)\nlogger.setLevel('INFO')\nworkdir=\"/kaggle/working\"","metadata":{"id":"PFUOkHZ_HNbB","execution":{"iopub.status.busy":"2023-12-17T20:04:34.778692Z","iopub.execute_input":"2023-12-17T20:04:34.778976Z","iopub.status.idle":"2023-12-17T20:04:34.788152Z","shell.execute_reply.started":"2023-12-17T20:04:34.778938Z","shell.execute_reply":"2023-12-17T20:04:34.787263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get checkpoint (get api from kaggle)\n\n!wget https://www.kaggleusercontent.com/kf/159213590/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..P7r5_R3dDnznpxld6gK4Pw.CDbeBRqalzDunXoFgJiMqVeaXurNZExpctrJuw19kx-Z9kMErtt7OG91ptONpamgAKaGdx47nZ__8n899M2rsrlLqMw_hJ-4evB3vYf_Kvu4rEqx3-bAdSDMAzirjp2Da6UAg18yrQyeakxvP7bmxC9Gep-sT8p6d495xO77QehFiD1xHnAIX8wgoDyI6JaDa5KY3cSD99VZYokmaJ2Mpw9hvwKK01kO-CdwEpoyNb42UmFJgndRCqn80lmbp4veiiNhqv6ncR1tfpuidjSd9vynVn7OxKfyGl2VVNK8YC0ZHewjQM9NXDXdafj_f63e9qXQuiGig_4RgIHtLiNStGziW-XhPm2Ni3hcLmlCe4RXHSi73-P2yqQPwTBaMvY3hQr93M1r3c1EyvCJ0H1vOCuwlwZvLjY1xQIXVpioJfTwXalSDDnSG3kSfMgYxAAeTVQF416Wf4O2hoaVFV7j5Ms8ZeDh5crWwxiWjE_dBXRKMI5F-9BOyDRmE227AVNoqnCKv790uZstRrOcOgTGkWN-v78sHdd60c0_CTYaPEURQPtsex2P66Gpkx0CZEVI-iPy8rNucCBfzTyNMZM2KqgCLhbxIW-zLDpX9a8dUtturTeaBA0StRmzPDssZOFNxVbwxdQZLwTta57rhWiCvb7mkyPLlBuARVcGuAJKaJU.b8_g7Rr2CXJlDwTRRphyqw/checkpoints-meta/iter_100_checkpoint.pth","metadata":{"execution":{"iopub.status.busy":"2023-12-18T10:38:57.147269Z","iopub.execute_input":"2023-12-18T10:38:57.148232Z","iopub.status.idle":"2023-12-18T10:39:36.001377Z","shell.execute_reply.started":"2023-12-18T10:38:57.148197Z","shell.execute_reply":"2023-12-18T10:39:36.000264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_meta_dir = os.path.join(workdir, \"checkpoints-meta\", \"checkpoint.pth\")\ntf.io.gfile.makedirs(os.path.dirname(checkpoint_meta_dir))\n!mv iter_100_checkpoint.pth \"/kaggle/working/checkpoints-meta/checkpoint.pth\"","metadata":{"execution":{"iopub.status.busy":"2023-12-17T20:04:59.211988Z","iopub.execute_input":"2023-12-17T20:04:59.212318Z","iopub.status.idle":"2023-12-17T20:05:00.171814Z","shell.execute_reply.started":"2023-12-17T20:04:59.212288Z","shell.execute_reply":"2023-12-17T20:05:00.170420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nsample_dir = os.path.join(workdir, \"samples\")\ntf.io.gfile.makedirs(sample_dir)\n\ntb_dir = os.path.join(workdir, \"tensorboard\")\ntf.io.gfile.makedirs(tb_dir)\n###writer = SummaryWriter(tb_dir)\n\n# Initialize model.\nscore_model =  create_model(config)\nema = ExponentialMovingAverage(score_model.parameters(), decay=config.model.ema_rate)\noptimizer =  get_optimizer(config, score_model.parameters())\nstate = dict(optimizer=optimizer, model=score_model, ema=ema, step=0)\n\n# Create checkpoints directory\ncheckpoint_dir = os.path.join(workdir, \"checkpoints\")\n# Intermediate checkpoints to resume training after pre-emption in cloud environments\ncheckpoint_meta_dir = os.path.join(workdir, \"checkpoints-meta\", \"checkpoint.pth\")\ntf.io.gfile.makedirs(checkpoint_dir)\ntf.io.gfile.makedirs(os.path.dirname(checkpoint_meta_dir))\n# Resume training when intermediate checkpoints are detected\nstate = restore_checkpoint(checkpoint_meta_dir, state, config.device)\ninitial_step = int(state['step'])\nprint(\"initial_step\",initial_step)\n\nscaler =  get_data_scaler(config)\ninverse_scaler =  get_data_inverse_scaler(config)\n\n# Setup SDEs\nif config.training.sde.lower() == 'vpsde':\n    sde =  VPSDE(beta_min=config.model.beta_min, beta_max=config.model.beta_max, N=config.model.num_scales)\n    sampling_eps = 1e-3\nelif config.training.sde.lower() == 'subvpsde':\n    sde =  subVPSDE(beta_min=config.model.beta_min, beta_max=config.model.beta_max, N=config.model.num_scales)\n    sampling_eps = 1e-3\nelif config.training.sde.lower() == 'vesde':\n    sde =  VESDE(sigma_min=config.model.sigma_min, sigma_max=config.model.sigma_max, N=config.model.num_scales)\n    sampling_eps = 2e-5 ##changed\nelse:\n    raise NotImplementedError(f\"SDE {config.training.sde} unknown.\")\n\n# Build one-step training and evaluation functions state\noptimize_fn =  optimization_manager(config)\ncontinuous = config.training.continuous\nreduce_mean = config.training.reduce_mean\nlikelihood_weighting = config.training.likelihood_weighting\ntrain_step_fn = get_step_fn(sde, train=True, optimize_fn=optimize_fn,\n                                    reduce_mean=reduce_mean, continuous=continuous,\n                                    likelihood_weighting=likelihood_weighting)\neval_step_fn = get_step_fn(sde, train=False, optimize_fn=optimize_fn,\n                                reduce_mean=reduce_mean, continuous=continuous,\n                                likelihood_weighting=likelihood_weighting)\n\n# Building sampling functions\nif config.training.snapshot_sampling:\n    sampling_shape = (1, config.data.num_channels,\n                    config.data.image_size, config.data.image_size)\nsampling_fn =  get_sampling_fn(config, sde, sampling_shape, inverse_scaler, sampling_eps)\n\nnum_train_steps = config.training.n_iters","metadata":{"id":"9PX3iwI1HR-R","outputId":"3cc21164-08c1-42a1-b393-c4b6ead80b68","execution":{"iopub.status.busy":"2023-12-17T20:05:00.173751Z","iopub.execute_input":"2023-12-17T20:05:00.174180Z","iopub.status.idle":"2023-12-17T20:05:08.337287Z","shell.execute_reply.started":"2023-12-17T20:05:00.174143Z","shell.execute_reply":"2023-12-17T20:05:08.336330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom PIL import Image","metadata":{"id":"YkFr_adLIuyj","execution":{"iopub.status.busy":"2023-12-17T20:05:08.338477Z","iopub.execute_input":"2023-12-17T20:05:08.338785Z","iopub.status.idle":"2023-12-17T20:05:08.343582Z","shell.execute_reply.started":"2023-12-17T20:05:08.338761Z","shell.execute_reply":"2023-12-17T20:05:08.342614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gdown\n!pip install wget","metadata":{"execution":{"iopub.status.busy":"2024-01-15T10:06:09.123849Z","iopub.execute_input":"2024-01-15T10:06:09.124128Z","iopub.status.idle":"2024-01-15T10:06:22.146766Z","shell.execute_reply.started":"2024-01-15T10:06:09.124101Z","shell.execute_reply":"2024-01-15T10:06:22.145566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wget\n\nwget.download('https://drive.usercontent.google.com/download?id=1Un97mn-Lpi5wCHC36nC9dx8asMNqNlSX&authuser=0&confirm=t&uuid=2453b8b2-5613-479d-bf51-0044d98693df&at=APZUnTUIzrWWY56--veu6BemuuR1%3A1705248529416')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !gdown 1Un97mn-Lpi5wCHC36nC9dx8asMNqNlSX -O dataset_v7.zip\n!unzip -q dataset_v7.zip -d \"/kaggle/working\"\n!rm dataset_v7.zip","metadata":{"execution":{"iopub.status.busy":"2024-01-15T10:06:22.149239Z","iopub.execute_input":"2024-01-15T10:06:22.149952Z","iopub.status.idle":"2024-01-15T10:06:25.865456Z","shell.execute_reply.started":"2024-01-15T10:06:22.149915Z","shell.execute_reply":"2024-01-15T10:06:25.864186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\n\nclass MidogDataset(Dataset):\n    def __init__(self, root_dir, json_file, transform=None, scanner=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.image_files = os.listdir(root_dir)\n        self.scanner = scanner\n        \n        with open(json_file, 'r') as f:\n            coco_data = json.load(f)\n        \n        if self.scanner:\n            self.image_files = [\n                im['file_name'].split('/')[-1] for im in coco_data['images']\n                if im['scanner'] == scanner\n            ]\n\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, self.image_files[idx])\n        image = Image.open(img_name)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, img_name\n\ntrain_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.5),\n    transforms.RandomAffine(degrees=20,translate=(0.1,0.1), scale=(0.9,1.1),shear=0.1)\n])\n\ntest_transforms = transforms.Compose([\n    transforms.ToTensor(),\n])","metadata":{"id":"Mg74wS86HS0O","execution":{"iopub.status.busy":"2023-12-17T20:50:34.325743Z","iopub.execute_input":"2023-12-17T20:50:34.326065Z","iopub.status.idle":"2023-12-17T20:50:34.337705Z","shell.execute_reply.started":"2023-12-17T20:50:34.326036Z","shell.execute_reply":"2023-12-17T20:50:34.336759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Subset\n\nroot = \"/kaggle/working/dataset_v7\"\n# train_dir = os.path.join(root, 'train')\n# eval_dir = os.path.join(root, 'eval')\ntest_dir = os.path.join(root, 'test')\n\nbatch_size = 4\ntest_batch_size = 1\n\n\n# midog_train_ds = MidogDataset(\n#     root_dir=train_dir, json_file=os.path.join(root, 'train.json'), \n#     transform=train_transforms\n# )\n# train_data_loader = DataLoader(midog_train_ds, batch_size=batch_size, shuffle=True)\n\n# midog_val_ds = MidogDataset(\n#     root_dir=eval_dir, json_file=os.path.join(root, 'eval.json'),\n#     transform=test_transforms\n# )\n# val_data_loader = DataLoader(midog_val_ds, batch_size=batch_size, shuffle=False)\n\n\ntest_loaders = []\nscanners = [1, 2, 3, 4]\nfor scanner in scanners:\n    midog_test_ds = MidogDataset(\n        root_dir=test_dir, json_file=os.path.join(root, 'test.json'),\n        transform=test_transforms, scanner=scanner\n    )\n    test_data_loader = DataLoader(\n        midog_test_ds, batch_size=test_batch_size, shuffle=True) # for random sample\n    test_loaders.append(test_data_loader)","metadata":{"id":"ox_slVWKI5Fl","execution":{"iopub.status.busy":"2023-12-17T20:50:34.338921Z","iopub.execute_input":"2023-12-17T20:50:34.339290Z","iopub.status.idle":"2023-12-17T20:50:35.316142Z","shell.execute_reply.started":"2023-12-17T20:50:34.339263Z","shell.execute_reply":"2023-12-17T20:50:35.315275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for images, names in test_loaders[2]:\n    # Get the first image and its size\n    image = images[0]\n    image_size = image.shape\n\n    # Display the image\n    plt.imshow(image.permute(1, 2, 0))  # Permute dimensions for displaying with matplotlib\n    plt.title(f\"Image Size: {image_size}\")\n    plt.axis('off')\n    plt.show()\n\n    break","metadata":{"id":"gyi3Eo0p-QDO","outputId":"54a77d04-8cb3-44d1-f545-3d5a131a371f","execution":{"iopub.status.busy":"2023-12-17T20:09:04.103557Z","iopub.execute_input":"2023-12-17T20:09:04.104487Z","iopub.status.idle":"2023-12-17T20:09:04.239226Z","shell.execute_reply.started":"2023-12-17T20:09:04.104433Z","shell.execute_reply":"2023-12-17T20:09:04.237884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TRAIN!","metadata":{"id":"KbNfiCTPHcGV"}},{"cell_type":"code","source":"!pip install GPUtil\nimport time\nimport gc\nfrom GPUtil import showUtilization as gpu_usage\ngc.collect()\n\ndef timer(start,end):\n    hours, rem = divmod(end-start, 3600)\n    minutes, seconds = divmod(rem, 60)\n    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n\ndef custom_train():\n    num_train_steps=100\n    losses = []\n    for step in range(0, num_train_steps + 1):\n        print(\"\\ncurrent step: \", step, \"\\n\")\n        # Convert data to JAX arrays and normalize them. Use ._numpy() to avoid copy.\n        a_loss=0\n        tik=time.time()\n        for myiter, (batch_images, _) in enumerate(train_data_loader):\n          batch = batch_images.cuda()\n          batch = scaler(batch)\n          torch.cuda.empty_cache()\n          loss = train_step_fn(state, batch)\n          a_loss+=loss.item()\n        losses.append(a_loss // myiter)\n        print(\"\\ncurrent step: \", step, \"  |  loss: \",  a_loss // myiter)\n        tok=time.time()\n        timer(tik,tok)\n\n        if (step % 10 == 0):\n            checkpoint_meta_dirz = os.path.join(workdir, \"checkpoints-meta\", \"iter_{}_\".format(step) + \"checkpoint.pth\")\n            save_checkpoint(checkpoint_meta_dirz, state)\n\n        tik=time.time()\n        if (step % 25 == 0):\n            if config.training.snapshot_sampling:\n              ema.store(score_model.parameters())\n              ema.copy_to(score_model.parameters())\n              sample, n = sampling_fn(score_model)\n              ema.restore(score_model.parameters())\n              this_sample_dir = os.path.join(sample_dir, \"iter_{}\".format(step))\n              tf.io.gfile.makedirs(this_sample_dir)\n              nrow = int(np.sqrt(sample.shape[0]))\n              image_grid = make_grid(sample, nrow, padding=2)\n              sample = np.clip(sample.permute(0, 2, 3, 1).cpu().numpy() * 255, 0, 255).astype(np.uint8)\n              with tf.io.gfile.GFile(os.path.join(this_sample_dir, \"sample.np\"), \"wb\") as fout:\n                np.save(fout, sample)\n\n              with tf.io.gfile.GFile(os.path.join(this_sample_dir, \"sample.png\"), \"wb\") as fout:\n                save_image(image_grid, fout)\n        tok=time.time()\n        if (step % 25 == 0):\n            print(\"generation time\")\n            timer(tik,tok)\n    \n    plt.plot(losses)\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training Loss Over Epochs')\n    plt.grid(True)\n    plt.savefig('loss_plot.png')","metadata":{"id":"wo0adCtrHdsP","outputId":"b8020820-c773-4deb-c230-6429c14f650d","execution":{"iopub.status.busy":"2023-12-07T12:43:31.616892Z","iopub.execute_input":"2023-12-07T12:43:31.617648Z","iopub.status.idle":"2023-12-07T12:43:43.365199Z","shell.execute_reply.started":"2023-12-07T12:43:31.617610Z","shell.execute_reply":"2023-12-07T12:43:43.364128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# custom_train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.cuda.mem_get_info()\n# !nvidia-smi","metadata":{"id":"HKa-IpRDhUhv","execution":{"iopub.status.busy":"2023-11-07T19:19:47.701803Z","iopub.execute_input":"2023-11-07T19:19:47.702086Z","iopub.status.idle":"2023-11-07T19:19:47.714487Z","shell.execute_reply.started":"2023-11-07T19:19:47.702061Z","shell.execute_reply":"2023-11-07T19:19:47.713527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Clear GPU Memory**","metadata":{"id":"XhK5DGMtP6oY"}},{"cell_type":"code","source":"# # !pip install numba\n# from numba import cuda\n# torch.cuda.empty_cache()","metadata":{"id":"TLZnYMALP5-1","execution":{"iopub.status.busy":"2023-11-07T19:19:47.715592Z","iopub.execute_input":"2023-11-07T19:19:47.715929Z","iopub.status.idle":"2023-11-07T19:19:47.726698Z","shell.execute_reply.started":"2023-11-07T19:19:47.715898Z","shell.execute_reply":"2023-11-07T19:19:47.725859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"img_size = config.data.image_size\nchannels = config.data.num_channels\npredictor = ReverseDiffusionPredictor # param [\"EulerMaruyamaPredictor\", \"AncestralSamplingPredictor\", \"ReverseDiffusionPredictor\", \"None\"] {\"type\": \"raw\"}\ncorrector = LangevinCorrector # param [\"LangevinCorrector\", \"AnnealedLangevinDynamics\", \"None\"] {\"type\": \"raw\"}\nsnr = 0.16 # param {\"type\": \"number\"}\nn_steps =  1 # param {\"type\": \"integer\"}\nprobability_flow = False # param {\"type\": \"boolean\"}","metadata":{"execution":{"iopub.status.busy":"2023-12-17T13:35:23.089587Z","iopub.execute_input":"2023-12-17T13:35:23.089955Z","iopub.status.idle":"2023-12-17T13:35:23.095647Z","shell.execute_reply.started":"2023-12-17T13:35:23.089929Z","shell.execute_reply":"2023-12-17T13:35:23.094590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport torch\nimport numpy as np\n\noutput_data = {}\n\nfor scanner, data_loader in enumerate(test_loaders):\n    x, name = next(iter(data_loader))\n    tik = time.time()    \n    x_in = x.cuda()\n    print(x.shape)\n    init_input = x_in + 0.0 * torch.randn(x_in.shape[0], 3, 128, 128).cuda()\n    shape = x_in.shape\n    sampling_fn = get_pc_sampler(sde, shape, predictor, corrector, \n                                 inverse_scaler, snr, n_steps=n_steps, \n                                 probability_flow=probability_flow,\n                                 continuous=config.training.continuous,\n                                 eps=sampling_eps, device=config.device)\n    x_out, n = sampling_fn(score_model)\n\n    tok = time.time()\n    timer(tik, tok)\n\n    output_data[f'scanner_{scanner}_samples'] = {\n        'x_in': x_in.permute(0, 2, 3, 1).cpu().numpy(),\n        'x_out': x_out.detach().permute(0, 2, 3, 1).cpu().numpy(),\n        'x_name': name\n    }\n\nnp.save('output_data.npy', output_data)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T13:42:53.701300Z","iopub.execute_input":"2023-12-17T13:42:53.702451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot 8*10","metadata":{}},{"cell_type":"code","source":"# !wget https://www.kaggleusercontent.com/kf/148972931/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..M2YddORiOx175JIiO3zFkg.RnhoRNFVH1bRdpk1kVAzKpROMoWmaNc0ncQMGhu4rodckYi_BPrOkGITkzoTxUz9gF7US5fQryazTaYew0VNIn44eqYCIsUrNulJ4adkubEJq7xgNTiolShwjJEoOsgA8GM0M4-5IpULLiysG4G_Wg1xPX1QMeRxnzjLqAaaA5qdWPXHWYMgfvWnONC3SDieZ8xGh3eSYP089dFRvq5AF3u-ZKN3fPalGGIi137PoeyAl4g_HPr8UEzvrdRkNWH5x2OfX67TtKRc68XUbZPYynuit5yPSlkzKkBYCAJzkITuORBqY0waKKSpuxZQAKOpHyGpoqYIZu6ze0wcBq-A92I0f2a6O7lbB8feEN6aUjDpPg3Ue2c6eouewdYq4nJVA0VQnnRfSTD_63ns32fxH0qqXFsoDr2xxeI6NvJEgizQoNmaGmDmtfNvHWeE_FG7mPQhWZovYOQrZwmAROvXz1bB9KUTax-5mu01AAMVUJS6VAu_LpbbC5dOHd_ANDlhXz8fEngYZz3WqmEFYZ9Fvj4sC1J1lrGdgoSXN8oDLcvCGui1TTooimTnUYjx0iYp61v0wUhwVNr_qjNzuQ3PiJyo3Fykyl-PHhMqEJlX5JNCLJbLfbGdbM2dp2NGFWsu9Z2jQt_LWDljJGj2U8IWzbloypA6QEXylF0ehncy0Po.-VAk3Y9G-gb8ZTKwbo9UXA/output_data.npy","metadata":{"execution":{"iopub.status.busy":"2023-11-07T19:24:00.659060Z","iopub.status.idle":"2023-11-07T19:24:00.659424Z","shell.execute_reply.started":"2023-11-07T19:24:00.659256Z","shell.execute_reply":"2023-11-07T19:24:00.659273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\n\noutput_data = np.load('output_data.npy', allow_pickle=True).item()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T19:24:00.660722Z","iopub.status.idle":"2023-11-07T19:24:00.661098Z","shell.execute_reply.started":"2023-11-07T19:24:00.660935Z","shell.execute_reply":"2023-11-07T19:24:00.660952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output_data['scanner_0_samples']['x_in'].shape","metadata":{"execution":{"iopub.status.busy":"2023-11-07T19:24:00.662472Z","iopub.status.idle":"2023-11-07T19:24:00.662841Z","shell.execute_reply.started":"2023-11-07T19:24:00.662644Z","shell.execute_reply":"2023-11-07T19:24:00.662660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.imshow(output_data['scanner_0_samples']['x_out'][0])","metadata":{"execution":{"iopub.status.busy":"2023-11-07T19:24:00.664384Z","iopub.status.idle":"2023-11-07T19:24:00.664692Z","shell.execute_reply.started":"2023-11-07T19:24:00.664541Z","shell.execute_reply":"2023-11-07T19:24:00.664556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(test_batch_size, 2*len(scanners), figsize=(16, 2 * test_batch_size))\n\nfor key, data in tqdm(output_data.items()):\n    scanner = int(key.split('_')[1])\n    \n    for i, sample in enumerate(data['x_in']):\n        ax = axes[scanner] if test_batch_size == 1 else axes[i, scanner]\n        ax.imshow(data['x_in'][i])\n        ax.axis('off')\n        \n    for i, sample in enumerate(data['x_out']):\n        ax = axes[scanner + 4] if test_batch_size == 1 else axes[i, scanner + 4]\n        ax.imshow(data['x_out'][i])\n        ax.axis('off')\n\nplt.tight_layout()\nplt.savefig('output_grid.png')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T19:24:00.666578Z","iopub.status.idle":"2023-11-07T19:24:00.667056Z","shell.execute_reply.started":"2023-11-07T19:24:00.666820Z","shell.execute_reply":"2023-11-07T19:24:00.666843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_torch(data):\n    assert isinstance(data, torch.Tensor)\n    rgb_image = data.permute(1, 2, 0).cpu()\n    plt.imshow(rgb_image)\n    plt.axis('off')\n\ndef plot_single_output(batch_index):\n    plt.subplot(1, 3, 1)\n    plt.title('input')\n    plot_torch(x_in[batch_index])\n    plt.subplot(1, 3, 2)\n    plt.title('noisy')\n    plot_torch(init_input[batch_index])\n    plt.subplot(1, 3, 3)\n    plt.title('output')\n    plot_torch(x_out[batch_index])","metadata":{"execution":{"iopub.status.busy":"2023-11-07T19:24:00.668289Z","iopub.status.idle":"2023-11-07T19:24:00.668757Z","shell.execute_reply.started":"2023-11-07T19:24:00.668502Z","shell.execute_reply":"2023-11-07T19:24:00.668522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_single_output(0)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T19:24:00.670847Z","iopub.status.idle":"2023-11-07T19:24:00.671191Z","shell.execute_reply.started":"2023-11-07T19:24:00.671030Z","shell.execute_reply":"2023-11-07T19:24:00.671046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_single_output(1)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T19:24:00.672359Z","iopub.status.idle":"2023-11-07T19:24:00.672665Z","shell.execute_reply.started":"2023-11-07T19:24:00.672514Z","shell.execute_reply":"2023-11-07T19:24:00.672528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data = np.load('test.npy')","metadata":{"execution":{"iopub.status.busy":"2023-11-07T19:24:00.673972Z","iopub.status.idle":"2023-11-07T19:24:00.674315Z","shell.execute_reply.started":"2023-11-07T19:24:00.674146Z","shell.execute_reply":"2023-11-07T19:24:00.674162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(sum(p.numel() for p in score_model.parameters()))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T12:40:56.523792Z","iopub.execute_input":"2023-12-07T12:40:56.524684Z","iopub.status.idle":"2023-12-07T12:40:56.531864Z","shell.execute_reply.started":"2023-12-07T12:40:56.524649Z","shell.execute_reply":"2023-12-07T12:40:56.530797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}